{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(\"../data/train.csv\")\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000,)\n",
      "(250000, 31)\n",
      "[ 1.  1.  1. ...,  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)\n",
    "print(np.ones(tX.shape[0]).T.shape)\n",
    "onesline = np.ones((tX.shape[0],1))\n",
    "newtx = np.concatenate((onesline, tX), axis=1)\n",
    "print(newtx.shape)\n",
    "print(newtx[:, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All value in y is equal either to 1 or -1.\n"
     ]
    }
   ],
   "source": [
    "# As we can see here, y only takes value -1 or 1:\n",
    "for value in y:\n",
    "    assert(value==1 or value==-1)\n",
    "print(\"All value in y is equal either to 1 or -1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that y is a binary variable. So should we modify y's coding to {0, 1} instead of {-1, 1} if we want the logistic regression methods to work?\n",
    "Note that at first sight, logistic regression seems to be the best solution to fit the data since this method was designed for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We have to handle:\n",
    "- outliers:\n",
    "For the outliers we decided to remove the entire sample when at least one of its feature fields is considered as an outlier. We don't yet know how to determine if a given value is an outlier\n",
    "- unasssigned values (-999):\n",
    "If there more than [given threshold] non-assigned values in a given sample, we remove it. Otherwise we set the missing values to the mean of their corresponding feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to get rid of features that don't give enough information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C = 1/tX.shape[0]*tX.T.dot(tX)\\n    \\neigenvalue, eigenvector = np.linalg.eig(C)\\nSUM = np.sum(eigenvalue)\\n\\nidx = np.argsort(eigenvalue)[::-1]\\neigenvector = eigenvector[:,idx]\\neigenvalue = eigenvalue[idx]\\n\\nF = 0\\nk = 0\\nwhile F < 0.90:\\n    F += eigenvalue[k]/SUM\\n    k = k+1   \\n\\neigenvector=eigenvector[:, :k]\\n\\ntX = np.dot(eigenvector.T, tX.T).T\\n\\nprint(tX.shape)'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in tX.T:\n",
    "    row_cleaned = row[abs(row)<999]\n",
    "    mean = np.mean(row_cleaned)\n",
    "    row[abs(row)==999] = mean\n",
    "    \n",
    "tX = (tX-np.mean(tX,axis=0))/np.std(tX,axis=0)\n",
    "\"\"\"C = 1/tX.shape[0]*tX.T.dot(tX)\n",
    "    \n",
    "eigenvalue, eigenvector = np.linalg.eig(C)\n",
    "SUM = np.sum(eigenvalue)\n",
    "\n",
    "idx = np.argsort(eigenvalue)[::-1]\n",
    "eigenvector = eigenvector[:,idx]\n",
    "eigenvalue = eigenvalue[idx]\n",
    "\n",
    "F = 0\n",
    "k = 0\n",
    "while F < 0.90:\n",
    "    F += eigenvalue[k]/SUM\n",
    "    k = k+1   \n",
    "\n",
    "eigenvector=eigenvector[:, :k]\n",
    "\n",
    "tX = np.dot(eigenvector.T, tX.T).T\n",
    "\n",
    "print(tX.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEACAYAAABGYoqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXm8n1V17//eJ8PJcDKHnAyEBGNBtHrV2yott22oULlq\nCaKX24IoKtpJBJUiDjGh0bZysTi0vf05FVqtLT9729LWqqAGC7dUrVIHwpR5nuc5Ofv+sdYn6zkn\n5wuBnJyckPV5vc7rfIfn2c8e1l7z2t9SayWRSCQSid7QdrI7kEgkEomBixQSiUQikWiJFBKJRCKR\naIkUEolEIpFoiRQSiUQikWiJFBKJRCKRaInBfdFIKWUZsB3oAg7WWl9WShkH/A0wA1gGXFFr3d4X\nz0skEolE/6CvLIkuYHat9SW11pf5ZzcD99ZazwW+Cbyvj56VSCQSiX5CXwmJ0ktbc4A7/fWdwGV9\n9KxEIpFI9BP6SkhU4J5SyndLKdf6Z5211vUAtdZ1wKQ+elYikUgk+gl9EpMALqi1ri2lnAF8vZTy\nKCY4msjzPxKJROIUQ58IiVrrWv+/sZTy98DLgPWllM5a6/pSymRgQ2/3llJSeCQSicQzQK21nOhn\nHLe7qZQyopTS4a9HAr8C/Ai4G7jGL3sT8A+t2qi15l+tzJs376T3YaD85VzkXORcPPlff6EvLIlO\n4O/cIhgMfLHW+vVSyveAu0opbwGWA1f0wbMSiUQi0Y84biFRa10KvLiXz7cAFx1v+4lEIpE4eciK\n6wGE2bNnn+wuDBjkXARyLgI5F/2P0p++rV47UEo92X1IJBKJUw2lFOqpELhOJBKJxLMXKSQSiUQi\n0RIpJBKJRCLREikkEolEItESKSQSiUQi0RIpJBKJRCLREikkEolEItESKSQSiUQi0RIpJBKJRCLR\nEikkEolEItESKSQSiUQi0RIpJBKJRCLREikkEolEItESKSQSiUQi0RJ98hvXJxLLly7ljrlz6Vq9\nmrZp07hmwQJmnH32ye5WIpFInBYY0L8nsXzpUj518cXcsngxI4HdwLxZs7junntSUCQSidMa+XsS\nwB1z5x4REAAjgVsWL+aOuXNPZrcSiUTitMGAFhJdq1cfERDCSKBrzZqT0Z1EIpE47TCghUTbtGns\n7vHZbqBt6tST0Z1EIpE47TCghcQ1CxYwb9asI4JCMYlrFiw4md1KJBKJ0wYDOnANjeymNWtomzo1\ns5sSiUSC/gtcD3ghkUgkEomjkdlNiUQikTjpSCGRSCQSiZZIIZFIJBKJlkghkUgkEomW6DMhUUpp\nK6V8v5Ryt78fV0r5einl0VLK10opY/rqWYlEIpHoH/SlJXE98HDj/c3AvbXWc4FvAu/rw2clEolE\noh/QJ0KilHIm8Crgs42P5wB3+us7gcv64lmJRCKR6D/0lSVxO/C7QLPgobPWuh6g1roOmNRHz0ok\nEolEP+G4hUQp5dXA+lrrQ8CTFXZkxVwikUicYuiLHx26ALi0lPIqYDgwqpTyl8C6UkpnrXV9KWUy\nsKFVA/Pnzz/yevbs2cyePbsPupVIJBLPHixcuJCFCxf2+3P79FiOUsovAe+ptV5aSrkV2Fxr/Wgp\n5b3AuFrrzb3ck8dyJBKJxNPEs+FYjj8ELi6lPAq8wt8nEolE4hRCHvCXSCQSpyCeDZZEIpFIJE5x\npJBIJBKJREukkEgkEolES6SQSCQSiURLpJBIJBKJREukkEgkEolES6SQSCQSiURLpJBIJBKJREuk\nkEgkEolES6SQSCQSiURLpJBIJBKJREukkEgkEolES6SQSCQSiURLpJBIJBKJREukkEgkEolES6SQ\nSCQSiURLpJBIJBKJREukkEgkEolES6SQSCQSiURLpJBIJBKJREukkEgkEolES6SQSCQSiURLpJBI\nJBKJREukkEgkEolES6SQSCQSiURLpJBIJBKJREukkEgkEolES6SQSCQSiURLpJBIJBKJREsct5Ao\npbSXUv69lPKDUsqPSinz/PNxpZSvl1IeLaV8rZQy5vi7m0gkEon+RKm1Hn8jpYyote4ppQwCHgDe\nCbwO2FxrvbWU8l5gXK315l7urX3Rh0QikTidUEqh1lpO9HP6xN1Ua93jL9uBwUAF5gB3+ud3Apf1\nxbMSiUQi0X/oEyFRSmkrpfwAWAfcU2v9LtBZa10PUGtdB0zqi2clEolEov8wuC8aqbV2AS8ppYwG\n/q6U8gLMmuh2Wav758+ff+T17NmzmT17dl90K5FIJJ41WLhwIQsXLuz35/ZJTKJbg6XMBfYA1wKz\na63rSymTgW/VWs/r5fqMSSQSicTTxCkTkyilTFTmUillOHAxsAi4G7jGL3sT8A/H+6xEIpFI9C+O\n25IopbwQC0y3+d/f1Fo/UkoZD9wFTAeWA1fUWrf1cn9aEolEIvE00V+WRJ+7m552B1JIJBKJxNPG\nKeNuSiQSicSzFykkEolEItESKSQSiUQi0RIpJBKJRCLREikkEolEItESKSQSiUQi0RIpJBKJRCLR\nEikkEolEItESKSQSiUQi0RIpJBKJRCLREikkEolEItESKSQSiUQi0RIpJBKJRCLREikkEolEItES\nKSQSiUQi0RIpJBKJRCLREikkEolEItESKSQSiUQi0RIpJBKJRCLREikkEolEItESKSQSiUQi0RIp\nJBKJRCLREikkEolEItESKSQSiUQi0RIpJBKJRCLREikkEolEItESKSQSiUQi0RIpJBKJRCLREsct\nJEopZ5ZSvllK+Ukp5UellHf65+NKKV8vpTxaSvlaKWXM8Xc3kUgkEv2JUms9vgZKmQxMrrU+VErp\nAP4DmAO8Gdhca721lPJeYFyt9eZe7q/H24dEIpE43VBKodZaTvRzjtuSqLWuq7U+5K93AYuAMzFB\ncadfdidw2fE+K5FIJBL9iz6NSZRSZgIvBh4EOmut68EECTCpL5+VSCQSiROPwX3VkLuavgxcX2vd\nVUrp6UNq6VOaP3/+kdezZ89m9uzZfdWtRCKReFZg4cKFLFy4sN+fe9wxCYBSymDgn4B/qbV+wj9b\nBMyuta73uMW3aq3n9XJvxiQSiUTiaeKUiUk4Pg88LAHhuBu4xl+/CfiHPnpWIpFIJPoJfZHddAHw\nbeBHmEupAu8HvgPcBUwHlgNX1Fq39XJ/WhKJRCLxNNFflkSfuJuOqwMpJBKJROJp41RzNyUSiUTi\nWYgUEolEIpFoiRQSiUQikWiJFBKJRCKRaIkUEolEIpFoiRQSiUQikWiJFBKJRCKRaIkUEolEIpFo\niT474C8xsLF86VLumDuXrtWraZs2jWsWLGDG2Wef7G4lEokBjqy4Pg2wfOlSPnXxxdyyeDEjgd3A\nvFmzuO6ee1JQJBKnKLLiOtFnuGPu3CMCAmAkcMvixdwxd+7J7FYikTgFkELiNEDX6tVHBIQwEuha\ns+ZkdCeRSJxCSCFxGqBt2jR29/hsN9A2derJ6E4ikTiFkELiNMA1CxYwb9asI4JCMYlrFiw4md1K\nJBKnADJwfZrgSHbTmjW0TZ2a2U2JxCmO/D2JRCKRSLREZjclEolE4qQjhUQikUgkWiKFRCKRSCRa\nIoVEIpFIJFoihUQikUgkWiKFRCKRSCRaIoVEIpFIJFoihUQikUgkWiKFRCKRSCRaIn90KJE4ycgf\nhEoMZOSxHInEScQD3/42n3v1q/nUrl35g1CJp4U8u+k0QGqQpzeWL13KdS96EV9yASHsBm676irm\nfeELJ6triVMA/SUk+sTdVEr5HPAaYH2t9UX+2Tjgb4AZwDLgilrr9r543rMBvf6k6IMPpgZ5GuGO\nuXN5UQ8BAfmDUImBhb4KXP858Moen90M3FtrPRf4JvC+PnrWswJ98ZOiy5cu5ZY3vIF5F17ILW94\nA8uXLj0hfU2cGHStXs0QyB+ESgxo9IklUWu9v5Qyo8fHc4Bf8td3AgsxwTFgcDLdPcf7k6JpiZz6\naJs2jSuAecAtcGQdr+voYF7+IFRigOBEZjdNqrWuB6i1riulTDqBz3raONlMVj8p2tMX3TZ16jEJ\nr1aWyG1z557WvuyBGufprV/XLFjApx58kLcuXsxtwEHghx0dvPef//mk93mgzuOT4VTs8ymBWmuf\n/GGxhx823m/p8f3mFvfVefPmHfn71re+VfsD86+6qu6CWht/u6DOv+qqfnn+siVL6ntmzTrSh11Q\n3zNrVr3/vvt6/XzZkiXd7v/Q7Nnd+q6/D114Yb/0v7+xbMmSOv+qq+qHZs+u86+66qj50DXHMnf9\njSfr15FxXXhhy3ENpP4OVPRXn4+FDk8UvvWtb3Xjlca++4Z/P9nfiRQSi4BOfz0ZWNTivr6dyaeA\nFvnqMWNOOpPtjUEcq/A62UKuP3GsDGCgzslA7VcrnGr9rbV/+jzQhGd/CYm+rLgu/ifcDVzjr98E\n/EMfPusZQS6mG7/4RWZt335SAobNYPMdc+dyzYIF3PLNbzLvC19gxtlnH3Os4poFC5g3a9aRMSi/\n/poB4Ms+loD60wm6H2uQ/3jjPCcKA7VfrdBf/e3LxIv+6HNfJJscCx749rd53dln88axY3nd2Wfz\nwLe/3aftP130VQrsXwGzgQmllBVYLO4Pgf+/lPIWYDlwRV8863jQXORrODpgOG/WLK47Rib7TPyf\nxxIHebJYRRMzzj6b6+65h9vmzqVrzRrapk7lugHggz2WMT7deNCxMoBjnbv+RrNfy4E7sPjDoqVL\nWb506Ulfs57oj3lcvnQpfzR7Nr+/YsURGnj/v/4r71648BnNR1/0+an29J6GgBBGArsXL35G7fV2\n7Yof/5iDP/whf1Grtb19O7/zilfAN77BBb/4i8c8lj5Ff5grT/bHU7ib+tIH2NOPvwzqfKiXjxpV\nL585s950/vn1hjlz6o2XXnpCfN/HYhIPNJO2J55qPY5ljE/XNXCs1w/UuVO/Hob6Hu/XQOpfT/TH\nPN546aW9rumNl176pP1qRXvNOZ4P9QNQf7Wjo95/333H1J/exvy26dO78YJLp02ruxp843qol0Gd\nM3Roy/4cyxw2r728QR/Nebl85syj7uNUi0k84w48iZA41onujXh6+6w3ZvMw1Dd3dBxZ/Hc9xSZe\ntmRJvXzmzGfk/zzWYPNADGbWemzrcSxj7HnN/b45Xj9oUL185syjNvbT3XADYe560t/99933jOnm\nZED9vXrs2F7X5Hhx9aRJvdLJ1Z2dvV5/LDRw/333HdnLxyrctE6vnTSp29r0xgvmDB1a3+aftxL4\nX/7Sl+rlM2fWi4cMabnWPWmjKTCvbjx/PtQPEYpsT5x2QuJYmfqxaN5vmz69vvOss44pc+hXG0Q1\nv4UU1/P0rA/0QtzHEvQeSAHBZ2Kh9ZWV0Lzmfqhv6rHZ3jR4cK+C4oY5c+olEybUi9vb65UTJtQb\nL710wAjQJnoyq4edzq4aNaol3ZzMrJmeaO6pZVA/CPXKYcNazvcz6XtPpqy1f21DSDTbfSoB+0yU\nt+Y4P9Tjvp68YBlmNUhw9MYr7oF6dSm9tqe/688//yje9Ovt7Ue+v5zeBdCVviea83xaCYnrX/7y\nXjWAm84//ymZcW9m6wefhNn31DSbz2i1sHqemNuNx9J+Q4PU+xvmzOlVeB0rQ3iytp8OY1m2ZMlR\n/XjnWWc95f3HYiWIQT6Z2f9MzOtlS5bUt02ffpR29+YpU57SPXisOFaL9KnaaCoeyxobvpUScsOc\nOS3ToU+G4BCdN/veil6frmtK83nJ+PFHreW1UF9z5pn1+pe/vF4ybVp9U3v7kX68oTGfTQ37+vPP\nP0p56+2a5rN70+B7rk2TF9wP9c3OvK98El7RpOVWa/2aM8886vPmPNwP9Vd6ubfp8VBbp5WQaDWh\nT6YZSLP8H6UczbR6WcCezKznhqgE828S2XW+sB+aPbtePWlSXQZHTM6eTLanpfIwphX36ud8mu6Q\n3rTTqwcN6qbt/Xp7+zFpe70R6i6e3B9cqwnkh+m+Ae/xdWoKwXsam0qC4qLhw+sbf/7n62snTapX\nT5pU3/6KV9QbL720vn7QoF7X6uqxY49ap57C/1jcg8eKp7JIj0WjVj+blmaTtlsx3d4UnYehXjF8\neP2gz/UHvT/H4jp5OkLliFtpzJgjbiUpA1rfS6BeDPW1UH8DE2rN8R6r9t7TQrnWx6V9dvWgQUe0\naK215qyVi+fNHR1H5m9+45om7b1y8OD65S996aj1vXLYsG609J7GMy/z1/dDfWXjmW/R+Hqhxdf3\neN/bWl85YUK3ueqNn7yjl/3QG488rYREb0x9GdRLJkyobx42rFcN623Tp9dXc7TVsAzqq3uZ0FaE\ne/999x0hzrc54WrB7od6TaOtD/omESHd4BvnDZgg6bnZn8p9daxoaqcSYK/psYmejFH2FDBv4Oj5\nrnT3B/dkOF/+0pfqa9rbu7mGHva2mvPzMEebzGIIPZnCLw0dWi9oa+uVQf7yyJFHBM+Nl15arxg5\nsn6I7gK8NyvkYUJoPR0h3Buz68mojkVY3HT++d3WvSdt348x3V8upb62vb3+zkUXHWUx3w/15zla\nAF4L9RWdnUcEbfP5xxK47W1NX9/W1k0Qvb6trf7ORRfVXRhDfL2vVbMfVw0ZcuS5EiittPfmc3vz\n+3/Q6U4KoeZOay1aehvUX22xnxTfWObX9CZMXtlQ1npb3xt8XX4Bsxb0zEt8LtX+bI4WWFJWLmu8\nbxXY7jkH83u558JexvmeHu9POyHRm1QWQ7kek+SvaGurc8aMqZdOm1YvHjOmvgtjUD03sJjRu+i+\nqVtp2TfMmVOvbSxw05roSZRfxoRCUwNQH38JY9za5JdDndPLwlaov9vYQE8F+Vo/0Bjrw41+9CaI\nmozyhjlz6mtGjOi2GXojwl2EP7inVv0w1F8ppdummu99aLZzvffvAz36NR9jNM010Tr9D7oLmqbg\nWeb3PYxtTrUhRthT2DXXpDfte9mSJfWtF11UL2xvr68fMqReOm3aESbamytNDL65kTX/NzjNXOZM\nVc+4fObM+jDUd3K0xrkM6q/TXfHYBfWVrggtg/pWLHh5Gb3viabgeBjqa0aMqDedf/6R5/ZkjlcM\nH15vvPTSIy7dphD5GY4WRO+C+tLRo+uVgwfXC3t836SvS6dNOxIrOJbnivH3th8+dOGFR+Zf19zg\nz27S0k2NuWgKpNdMmHDk2e+k9z3RWxxxGdQr2tu7zWvz3rdD/bXGZ3rfZOiXQ72UUCpf38uc/vbU\nqUfo44Y5c7p9r30tIfUGjhZy90C9qJcx9ZeQGBC/TLcSeDvwQeAu4N+A5wNXAh8DJgAf7Ori9u3b\n6dy+nX8HFgBXAxOB1wJvBHYCL8dOEdzk7W0HPgWM3L+f3Xffzbyf/ITr7rkHsLqJn3zlK/wdcCuW\n8zzC/98GnEP3vOu/wlZmEfBu4Dzv4wLgDOAQcC/wBeDDwG9Cr7nbj65c+ZRzsnzpUha8/e3s/eY3\nOa+riyHAJ4C3+fO2eVtdjfYf8DFPAf5i2TJGLlvGXOC/+Hx8BOgAfhqY623dhZ8ZBEx64QuPzEuz\naOgu4GerVV6qnU5vqzm2pT5H8/29vtsDrAL+kpjb4cBerBT/Umz9RgBLgK/7dfOBP/Drfxp4HHiP\nf36Oj38R8Gms/mAT8DPY+iv3/oaVK3nHL/4is6ZNY/FDDzFh/37+Ud+vXs1vew76ntGjj1qrrh5z\nfBvwVuB2H/sXgJFdXey+916uu+ACJvzsz8KmTdyO0ckfYjT5DuAmjGaGAJ9pPGcTMG7fPq4BpgOb\nvd3f6tGXO3yuNLblwOeAv96zh5EPPnhk79zibV4PPAzM3LuX+XffzW3Ab/g9qg36DkZLzeKwVwEb\nduzgAxiNbOrRDz33r1avZuTq1dzrz/q7HmOatncvb7n7bj4I/IV/9n2fz03Ax72tkcC2xx/neS99\nqdU1+DWDfZ11ppX25qIeY9gNvGXPHt5/1ln85ooVLAVGEft/ua/VKrrvxeXAZ4GNXV2c05jX5n5a\nA7wYK/B6F0Z//w3jOdf4mozBaHYTtvd39jKnt65Zc+RMtRtuv52PfP/7/OHKlbQB3wPWe5/P83s3\nAYcx/rYX2Af8KUfXdfUXBoSQGA38GvC/MIZ+NcbAbgfWYT9K8duYsLiZ2EBnADdgE/xhjKG0+Xeb\ngBUEgd6GEcDwxYtZ8Pa3M3bpUm5ZvJjFfr2Ic4//3wo8SnfCGuKvP+qvF3i7WzCh9jbv5z9hBD6B\nXgr2gAO7dnHLG95A1+rV7BgzhsG1MmLHjiMFN6tWruSTl1zC3r17+RLwXuAy4P/DhOZQgtHj7T6E\nbbyRdGdCbf73WYyxS4C+D2Nif+rXLgJufOAB3vtzP8faJUuOYpb7gWH+/A5v5za6b/rD/no7Jvj1\n3X/6Wom5LQZ2ATOJDfVT2KY71Oj7emLjTvQ+fJQQzh/DGMEMTABdTfcNugmjjY+vWsVtq1Yx2Ofy\nNl/flcDZhw7xoVe9iueefz5zG/cvAh4E3l4Kz6n1iLC4y+fxSoKmVgNda9ceYYodPfrxtxh9DgFe\nRHdG9W6Mdnb7PaLtTrrTXpevo2j5Pwmhi7d9kFjbof6sj/lnjxJCRPf0FPJge+4u/7xiTE/reEcv\nz32gxZg+jDHz8wjFYjK2X/F1+Yg/6+CqVTy4cSM3TZ3KO9asYR5Gayp6fYf34SK////QnQl/fu9e\nPviSl3BTVxe3rlrFOwnG+5fY3ng/oRh9GlOybgKWHzx4hGdA8IGRGHO8FuNJbcBYf/8uuq/xOwkF\nbjHd5+aP/PXKL36R733lK4wbMoThL3whG845hxUPPcTOLVuYXCtX+pxt8ufp9bWY0ncecB1G749g\ne62/MCCExAKMqX4Km+AOjHg6gOdhi7yaYGid2CY+CKzFFuLPMGbRRWgbZxOTfgnwvzFt5Il77+Ve\num/Ga7BFPoQR0xKMcTUJayUwCdP03uD37/FnLmi0NxLTXMA0TzGTHRhzHLt9O1d88Yt82sc1Gfgx\ntvD/94tfpLa18fKuLqq3tRjb7D+DEd4/+pzM8zmai2nxZxOWkNCGaUIfBp7r343ENPlP0l0rvWvv\nXkY++CBz6c6gdmDCeiem1fyDz+tK4Cpv6zBwABNG78IE/ly/Zzr2q1OL/POd3m9pqct9jd5KMISR\nPleLMA30VswyG4Ux0tt9jp/fmPueTO8OQjNb7HPxOeAlwFcwBWIksHv3bt7+wAO8nxAgO3x+5tfK\nd/3ZM32cu/z5nZi2ucKv/ZzP83y6M807MeH2fjjy+xGiy/O8X6P8nu3+/fn+zD8jBOV2ghk92uMZ\nKzEaKJj2Odefp+fsIjRyYRzd1/kBYFDj/bu9DSliPZkg3q/Sy5gkkObTXUGZjwmAKwmLYBNQ9u/n\nB2vWcPO0aYwaPZrljz3G7sPGCkdhdHEm8MIez8ff71i+nOFr1nAecJbfI7q4FfhZ4Oew9enEBMTt\nGC3KYux5EsMuTDm5ztfiILbf1wJ/TdDu4xgPuh1TGKW0fcaftwB4AfDHW7cyErj3G9/gj32eRmDr\n/3ngOdj6qt+yuGcRlsN2n4fJwD30D/ry7KZnjE0Ys9Tid2ILshx4DJusKY3vL8MW7VPANGyCfx+T\nuusxbfGtGOP9LCYg/gJbsIkY09ICH8K0/z2YdvHnmPnchm2i12GM7TAwHttE0pYX+V9Ho73VmNm5\nEVvMTwE3Am/BNtNB4EPYBtmFmat7MYH0d5hW9qKuLtoIhjLGn30tps1swhjPMMx6qt72psY9yzFm\n/SDwAe/veu/vu7x/ms876K5hXuJzIsLc5WN8DiYk5c7Yg1kIm3ysz8OYyF0+7tcBT2DMZjrGZMYC\nv4MxV2mpd/h6/Rm2Ieb55+Mx5joZ24SdmIb8ab93MGFtQCgPt3gbiwnGNRFj/qKFT/t3t/j8rNq3\n7wgN/BBjYm/G6EuWwKOYi+YnPh9X+jy+nGCKIwhrFOCPMdfYXZgQv8DH/1m/fgjBqO71+67G6PBn\nvL//Hfgy8CPC3biBWOePYG6q5/iYR/nz8Oe8FaPnHzb6tRyj5d9ttDPX50jvv4zR6xOEZb+z0QaY\n8rANs14+4WPaT1g1K3zepK0f8D/N11cxQXQz8CfA+NWrObRoEfMOH+YtmID6DEb7P/GxNZ+/3Ndp\nxUMPcW5XF/diNCG6WO59+U9sz7wZUy4+ja3h7/u1cxvtHgAua2tji6/VREw5mo7R40wf27t8bjsI\nmtzrc/V7Pp5PYwrHHzf6cysm7BZg67/V5+kS75voeauv0bUYPX/CnzWK/v1hngEhJD6BMQIt0vnA\nv2Mb4Z0YA9T30szGYwS2CCPUkZgl8QFsIj+Pbc7HvP33YYy5AOf6fTLrfg9b8Oc32qnYxv1bbCMt\nwvyT0pZnYIzyoxhx3IsR/Sxs0z7f+7oO04J+HSOeLdgGeSumXU/GFv0P/NmPe5+7MIZyDUY4OzFi\n3UUQy0Qf09sxIbATsxqu83EVjLlOBy72/nzU+ySNB8IPK8HySYwZvw9jUE8QsYMVGIHuxDbvOsL0\nfo+3KwZxJ+ZKvMvnUC66B7ztzdjmPOjr9ZvevqwvCfwdmHA5ExPqy7zvQzDGqHFchrnQbvS1mOhz\n9Vaft/GYNflCQniMxTTwdox2dvm8fcH7vgCjsysJF8Uh//wuTKjL1TMSE3i3Ypt6UWM9D/p63upj\nesyvvwhjot/zNs/GmMNg/+z1mILyfEwgf9r7+Hyfuw9gysILfHzTfG12YsxMQvuFwP/ErDAJ5tsx\nS/BibJ+NxxSjd2H0swsTVGPozlQ1tnd5++/2OVyJadCPYHHFT2B7cDyxRx5pzMdD3u4Hsb10PUZP\n12K0NcLHtglToF7gcygFZjnGxFdhgvoSbB+JLhQ73IntyxkYbZ+L0dACTJv/ALbfL/JnrwQOdXXR\n7mv1Qe/LRMyD8D0f2zKMiU/3Z63wsW4iFMcnML6h/fWbmBv6AGG5POzvv+p9Ez2v9LmXNbPUx/o2\nbH/0FwaEkFiKLdJcbPK/iG3UUcDXsE3zFkzjkOm6168dhRGWJnYGNtGrMILfhRGNGPN6jBBuIkzd\nj2Mb/zENMzSOAAAgAElEQVRv528xDfEJbPEVqLwEY6hLsIV7EUZkr8M2/+96+1Oxn+TbhBH0DmzD\n3oVpJDv99RnYAjS1ni0YI1+MEcJ0jCHIP3kQYypLMAakQOpwjOn+nn8nc/sujDC/7H19r7e93+db\n7pxFmHBbhG3s/+n9Og8TEGuwmMhBn9up2CZvI/zHYJvke5h7rPra7cSYgQKKi72/HZiGdx9mgX3c\nx/sZTNiN8/v2Yky8+usuf3+Oz8n7vd+KsUjL+w620e7y9VjjfV2K0dF04J+9z4cxIbTLn/EnPqav\nYvQ4ydf7pzDhMdLXapv3VVp6l8/ZazHloPh6/l9MaJ6LMRwFUu/FGJjmbjLGGDZg9PbPPuczfB6X\neT/GYQxzJcYYV/ocvBKj5e8Ar/Z53enz9DeYcL6NsLL+CNsfkzD62elrOtz7pMDpYoKpvhazrjsw\nBvhxX5uxPtYXY5b/Em/vALbOCxrX/geRHHK7j/c5vq6fxPb4YL//D7C9eolfuxdb66t8bv6Lj+Fv\nMbo8x6/9KCZ0p2LW3Rqfu2t9nWRJft6fUzEh+zuYkvTzvlbbG99/zO99lff5HIxm9njfn+fzf5AI\nwA8hlNJh2D5f5N9/z9dyhs+XYh5zMZpc7Wv2uPfhMKYY3kj/YUAIiS3YprsAY2YvwCZjHyYAzsQW\n/DcxRq5g7ESMuMcTDO9d2MaW9t2ObeSDGLMYhS3oCIzJ3UT40z+KCaM/8j6s8z50+rP+1vsyBCMY\nmb7/CLzM25+MCZc7sY3yCYzJz/I+DMI2yCZMEG0lhNwd/qwRmAD5aX/GNIwYV2L+9KE+pokYE60+\nht/wcQ/D/KbyZX8Y29ArCI3wOZhwm+/tvN3na7T37ZP+zAXex6UYUx1EWHW3eV/l+rgDY/CXYa6B\nyd6nf8eY2wZsk83wuX63z+frMYG0FGMir8MYwwi/76M+ZyOxo4Y3+PVDfS3XYesvl9sHMDdbu/f5\nx752430O2oEfEJbAoz7vj/u8TvV21nt/Z3o//8PXU8z9IYzmPuPr8hs+h4u87Z/zOZR1NcPH81lC\nKdrpa/YCbK13YsxabtRz/dq3YcwI72MX5p4b431dign2OzGNvBLKw3exNT7P/67xMfyB90vC6Vpv\nf6P3SYJiO8FUxcB+F9OAH/N5e7GP7Rwfwy5MOXrCr62NPpyJCZQ2jHF3eh9W+dhf5K9HYRb4Zn/2\nhzH6+3OMLovP6WPe3jIf11CMB/wJRod7vU9nY8rRHmwvfQIT8Kt9zUZgStSHsRhGaTzzIMbQd2E8\n51b/v8bbkEv4ALb/2jAhthXjazdgNLLdv/+oz+UfYzQ6yuf4ce/jzf7M5/nY/4BQFpuu4f7AgBAS\nYJrYZzCCaSO0xnXYxF6M+WpXYRM/hGD+XZimcwM2yT/lbd7n10zw9pdiBHW93//72Gb8LYw5jsCk\n/HCMWXVgjFrujrf560UYk7kAI5BhdM8uOQ8j3vX++QFsA/4QM1eHYYzrPGyjKdD4Y4yhfNb7VryN\nKzAzfTxG3JO9zc3YxpBW/i+YFrMVE1BKKVyJEd4Wv15B46/6+K/CGOc+jNgfwTb7Ab+uy/u8vvHs\nM7HNsxEj9JuwjfhVjCF3+r3/7GvR5c/9A29rEOb6uRnLVnkBYZF8CdOefuL3bcEEwzex9Nh2Iq32\nnb52bT7uGzGBORhj9hXT0Cb42okZbfWx/MSfO9nvG47R3Gd9boZhDOlH3uYCX+eP+Pr9nLf9b5hr\n7ieYBfH72Nqv8Hum+Nxf7+s8GROGPyCyqZQCLMY/xu9XjOFM7/+PMcbf5XP7WR/r/8F+uOUvMe10\nH8ZYzsVcREP9OX+EMb9VPpdDff4ewuhc7kcxrGE+J3JhPhfTZg/6/I/AaG2Y37Pcrxvqc/Zpb0dJ\nJVswmtyEMe41ft8+X0tlEW7yMUnZKpgAeQjT8idi+/4VGM1Vn7PvY/tqE7ZH1vn/H3vfrve+Kr5z\nCFOahmOCdQIhKB/3+X8hkf24C9sDW73/H/O+rcYspm0+/zv98096m5/2NpTuOsbXbT3wDZ+HWzFr\n+Ks+X2swWpmKKQHKEuxPDAghIb/0AcId04EtpDSSv8cmaCi2YRX4XI25WDZiBH8YYzAbsYXZQyzs\n4/48+bVf4u18Hlu4T2BEKiba5td939uWu0ia+CcJLeMCbOMOwRb/CYJh/wQj6FdhxHwIEz7jfEyd\n3j+Zlo/48x8hUgrxz9ZghNiOMQD8ujWYMD2IaTgVE0xf9Xk41/smq2kytsHWE4Hddmxj/DTGvB7D\nGNABTJCN8D5e5/+V6TLCx/k1bCP8F59PaV5DMIIf5/35BsYoFIQ+A3MHrcU20Wqf6xE+V7/r/RuE\nMUmAl2KMZ4T3Ywe2ztrcE/2zQb4ek30e9mLamVxhB/z7ijGJTRjTecT7MgJjugeIDToWY1zbMQb0\nL9gm/gyWWSNhtxOjub2+LsWvkSX0txgdrfdx/sTvWYvR4TpfE7k71nv/lKk0BrNQv40xXblDbvZn\nVYwJjsHckmsxV8pvYhrzdoy+l2H0s9r7Pxmjuz0+f5rnyRhjfRjTZodjSsBOTLhtI4TDWRjN7fc5\n6MQE+Dt8bMMw6/WHGJ2NIzK7LsCE406/brc/t83nROnWa3xNv0RYQ2u9rfU+D8Ow/TDU12Etxtgn\nYZbGLv+/xV+3+esVvsYjvK3N3sZwIpHjALZPhvg6VIzmJvn8zfK5n+RtDMNoaCsRw1iJ0cNgX+uz\nsT3yv7wtWbVKQV5P/9ZIABSvej5pKKXUX8cIpGBMeAg2KS/wzzdhG3sXRjjSYCdgC9eBTfZzsUk8\nB/hXbAPKD6kUzu3YYnZiC/KAv95HSOhtfs9/YAu7FyPsUf56N1YLcaO/PoAR3+WY33eI3zce2wDD\nMetmIyaYnoe5BZ7nzxiEZTzdhm2uZRijXooxpEcwItqHMb+9PubzvP1DGGFL42/367Zhm3Sv92Wd\ntzPRr2/3e0dhm6D482R1TG+syTCfm5WYlbTK2xrj77f7Z7+MCdWRPt4X+njW+Vod8PnY6tc00zDX\n+7hXYULgMYxxDfK/KcRmH+zPfdhfy4Uoi2KQv96L0cl0zK00FmOw1ef/Ef9snH/f7vOozSz33VCM\n6b0ei4G0EVr4QZ+jUT63KzHr9JOYNXLAr1vu63CoMSYwZjDU53KXz8c3MatAytBUf45cjRf5vH4A\nEwDTMBo8i6DHid6H3/C5mYkx88MYbY321/u9X+f4Gmzwzyf6vMqN8hz/bhf2s5O/5H1eRVji+7wP\nS3xe12H7aztG4x/xtR7kz1uB0dq+xpwcaMyphNMszIU4ztdziT+309dhcI/7Nee7CZpu8+9ViHom\nto9uwBRVpf/u93uHYQrIOEzx0U8LVW9DlsQOjCY+ju2XnUTWn2hzpn++CaOjXT6XSuPGx7uNiPMd\nJALgu7C99H8xK2qBf1drLZxgDAgh8UpiciuWqbDQP5uFTSyYhvENbMLX+rUiBmkPXRgxb8QYxJl+\nTcUYIX7tDmwTrPH7p2JMYrh/vxYjQD1jN+GO6MC0jzdijGE7Rszy2w7FiGQvtth6/nkYIcol9m/+\nvtPbeQLbOO8lKkBXYhtQguGgtym/9XJCUG71vm8jBEqHX7uZ0KTF4M7FGPFhjHHt8/GNxQhbm300\ntpFn+DysIwK9HdjmGYRpTKMxs14WmepIJLT2+RoonjCIMPfXYtbPH3n/hjfWd4Q/F5+XaX7fRkKr\n3u1/U/yZEhZ7MMGtzT/W51VMqvj3v4cxsjHeT635dIw2D/r7Pb5eW33MG/waBStHNp652edBO3ms\nX4+Psfj1KzAlZwxGT/+IuQH/w5/xIcxKOIyt/xRMi53mfVWfFD8Y7fN9rq/dVOAXvI1R3s5hjAFv\nAC7EFKb93rdRPr/jMIGiYrLB/ozfI+JGO3wsSzEakwX2EX+e1mWCz+Nknxdp5YVIHlAa8ih/1iH/\nPwnbN6p96iAsrlEYnXb5c6di63zYxz6m8RztVWEstifO87keQQgSCZ5zCMuqy9te7+NRzc5w/JwM\nn8OC7Q/1dRdhiRQshvM9748UjS7/votI8liACbGzfS5GYPvjDiyeeNoIiZdjRDaIyBxZgi3SYWzS\npmD+/I9hRDkdW9zDxOYY5O//K+b6mYYJgTP9853YxlXGBhhx7idMOG2uQUTmlIrFJmOa5xSMENsw\n189EoohJxDm48foARih7va1R3s4EjIB3YtrgZ/26V2NBSfnVV/v/cd5XCY79GJGvx4hsKkaM8lVr\nYymQJgG31+dhtLctH/9gf0YXkZUy1q97hKjHGEMIvpdjxN7mn4kZTyXSY4cSQdcOwpLoICxCbWK5\naZRrf6ZfI4YxyK874GvXhm3gsRhDUELBAf9OsYCxfl+Xr0nF4gnf8OeP8Gue8GdUYlPu99cTG+NW\ngH8E4aY47GNSDGuXf9bma7GDsJQP+lwXb18a5FD/7EOY63EI4RN/wJ97wOdyPxFLUl3PFu+n6ky0\nR7Zge2QyYTEe8DWaQjC23d7Xg0RMTWur/am1HOr/lY3V5e3KTTQLUxiEn8b2o5SfoT73+/2ePT7P\nY4jsRrnXJES2++vq/dlOMHP1SVaD1gRCeCk2Vv0+CSQpK0oNb/d7RO8/JgR9h8/LBL/+ORjdPA8T\n3LKYleSxx8dxCFMeh/tc/Kt/LzpRuu8owrqbjLnkhhNxnz/D3LHzOY2ExM9iDP0wRmyjCR/uXsJF\noADKTv8vobCPSDWbRDCRLdjGHINN7j6MOPYQCy6NSS6nXdiCKAite3cQxUDSuHd4XxWkOoARju7V\nJlDWjTbjdm9jqo93svd1OFFYRWPsYuDybbZhxC0Nd4v3YzBReaxceXx80/1Z7YS2qOCgNoSOmFBW\nlIKfh3wM0j6HYMS+2ce02Z+9lXCL7COO6RCGej+3YustIXKYsLpmYSb4QUwzPdOfvcfv0eY/5M8d\nRDB+sHXcQwThpanJqpnS+GwjIXj2+v3N9TqLSEOc4p9VH/tGbL32Y+u3g0hD3uttKCB+kIizKXAp\nS0WCqxJMQIx9lvdzDRYH+gK27nt8jdr9+kIIiz2N5ymmI0Y6xudnIxGDOdgYuxIStvnzFW87g0hH\n3eztjsYY/Ujvo5jmPu/LBsyKeQDTxOUG1L4e5fOxnlAGJYyUNaQ+yP+vOdzv303zsXT5PcOJuMMh\n/9OxIFp7uYravJ3B/n+696+dEEorfaxS+ob5/+FEfETuLhWnytLY52NQIF7Wq2hjsl+7FxMY3yaU\nBggLSHttma9Nu/fpf3OauZt+CZsUEcAwbIK0mAqcTiP86wqIriNiAAfp7k/cT/gZpQUrffAgtoCq\nGt6MEdVZ/v2GRj82YEwAwrc9EiOYmRgxSIsehBGz3FwKxv0URqwH/BnrMaJ5kY9d2TViSKrElHm+\nldBCZZ4qOAlGPDrbZyyhCZ/hbR7ETFZZGjv92Y8QZv8hb2cMobUp40RCSD7zxdjm1wbqavRviI9x\npfetElrnZoKxNbW/NrorBcrq2efzIeEl7V4xCLnPdhHMWunRmwm3xVa/fqe3IY29nYgJQGj26uNh\nokZDY5NlUwjX5QEilrHU+yLGoHjSsEb/p2B01EG44g4RrsUDhIWnmMBev3aJfyfBNZw482oHoRAp\nfgNhJcpNssLnSfUXgwiLRIWk0oa17jMwWh3nn60kYgAKvIoeJvic7/U+TSBcofKxbyUy8aR1K5tN\nlqNqiRRDOYMQYkomUIW7zreCUCCV7LDJx1cwgTIIEyYzCKGpepwp3q/NRJxSCpo4smiqg7A8ZQnu\nJ2Is2zEa3IBZGju8n3L1riOO0mnD9s1uIp6h+ZU7awwW1D7tLInXEDEGMQj5AjcR2s5o/34zYS4O\nJ1xVB7DFUL658voXEZZGJ+F/H0owzMOEL1qSfhmhNYgIDtOdUKSxK6NIQTcJJwVZ8edK0xAxixEo\nTiAhIz/8YIJpKLd8LeFqUbqdivJkzisIOYI4qkNn1IzxuZQbqBLutykY4cqHKg1xN8FEIDZpuz9f\n/ncJdlk5yk9XKmohYkqTfHwr/H/FNt0hfz0JEw7afPLvHyQE+m4i8KlKVvl0JQAgtOzn+mcriXOT\n9hIuBx1LMYlwMbzA+6uxd2H0ISawhRAAWnNpufKDj/Z7td57iDRhZcQVb2uaj3ELYXVNwJjKUH+u\nGPI4os5mnPdbAd29BPMZ5O2pdmiSr8dQwhocS5xNpUKwdu+7Yh163iHCwlfyg7JxHsGY9wFvS2OX\ni3MzkTyB36faG9HVWUTarAThFIy+5S3YTKyxCj7lYtzV+G4npqStIFx97Y3XY4l91Mwo0/6T9T3O\nx7rBv+/0z2XtriHcYOP92fKAaA0mEe7tStSIyUKQsjDK21hLWMXq40uw7LLTypJ4CcEA92ETo+Da\nVCJoOZrQ2sAIQW4SsElXFs52orClvdG2TMNzMSLrIhZFVc9DiEpL/LmqD5BmKY1HRFCwjbGdCMCN\nxoh7MHEs8kjCZJ9E1D2sadwzCCNEZUicQcQSlEWznTgYbj8RMGsj0okVw+jEmIGYsbQ8acLSZmQ1\nQQQBC3Gw2WBsQzyBMf6VRMB5D6aVbSWySkTUzbURE59AaEvNzbqF2GRLMYEi90nzPlkFbYQGOc6f\nIxfXYYKRDSaCgxMx4YP3dVLjHqU3KrCu9pUhs5twDxaMls7y13J3SPHYSiRejCRoQRXnCmSqAExu\nJ9XW7PLnHiJcUcMJgbKBCBxDrKtouplPr10uC3irX6OYw0TvpywzKTLtRCxN2rbmtpNg1IrbKa6w\nyecVIn4m5UxMUxbJOqLiewgRrJZSA6Z0bPR5VTae3FtKInhuY12GYXtkF5F1Nc3blTtqCnFabMXo\nY4Kvj1xE43xcj/j9u4m4WDsRN+ki4pii4U5fm02YkFrn10nwKAYhF60yBEdidL/On69sTmWJfd2v\nKZxmQkJZFjJFOwit/iC2QFuwxRMRLMcmU9k7KqLbSjATZafIRynm2PRTy/W0lTD3NxM+QaXyNbNo\nmv5oBdflRxyGMUjFNGRVaAO1E5XgXcRGlXYmrbeDSG2VlVF9bGt8rBqH8v33E0xf2STK05/pfV2H\nMeFh3o4yp4bQPQtjBLahZO4OpfsGlltjjLcjDXWd3yNt6AlsI8gHDeG/P4gJGuWnDyGySGRu7/f2\nlEYsV5VchRKuqwk3nfzDQ7wdMVulQEsIHiK0zq0+FsV0pmCbXFr1BELJGO5tKV6j95r3Fd6n3RgD\nlptqEKFx6z4I2pF7ZhihJD3h101ojFsWrbKpxNx3YoxFlnfx8W3z9Wvm+iuteLmPWTUBEwmBtBGj\nYwXBxdDkzlSl8Cbv9+DGvA3zPmxsrOFw/5MVMdbHvc5fD/Lvl2D7dh3hZhzk84jPtY7+OBtjrsoo\n2kwoc7sxIb6O2D/jCVeqFEwJp4NEMV9z/6l49xC255U0Iit/j/djra+TUr5lHQwm3KIqvoVINtDe\n0r6SW1juPvz+8Vj6MZxmQuIyonhlLXHSIdhiL6f7JlQwScdjKMsAbPGmEMdWHCCOcJBm1IGZ7hP8\nHqX5yact15ECcGJMw+jOXKZh2vRwQjAok0m+6v2NtiZ4v1X4J81Red2zCKYkVwQ+bgXWJvh8jCNc\nJBv9OvVhnP/JchBj0lj3ND5X4LSTOHBQG0y+6gNETOLHmCtru8/vI4QgVPBxBLbZOon6lA7CZ68C\nSQVO1T+5JUpjflf485VivJ/QkBUvkWYp94viDtJkJRyKz99S/24k4eaE8INP8X7JFTfEnyUmt8b7\nM5zQ7sYQGqrSqFWwJ1eUAprqxxIi86xJyw/5PKmGZBCh6Xf58xS/20Vk70G4X7f79ypc29YYowrU\nmj5vxWIgakyU2bOOqAwe3LhWDE0ZbU3rQXu5C2Nse4gizoPEcewQXoNtRLGkLLlpRMBc3oYN/hwp\nPxrTXsLdo9RpMX9llylNdUfje7n0BhPneynuucP79UJCyLcTdUYQsTElI3T6dZt9/vd5P5v7W4Hx\niYRCIXeeXMfnEG7HMUQ8415OQ0vihYRJWAjCkNQFIxgFpOVumYARoorHlhGajVJdxbx01EAzs0nB\nSQVdDxD1E/K3H8A2wBl+j4rMmgFwBcYU/NyOEXQzT14pdtswjX4XUctQCRebmOsuTDs6EyMg1W18\n358/hci+2OLfqapTVZqVIKaDPrYthFmu4zGkcUnYHiSOWpjiY9pIZKiMx4hWWtYBwpUmd0A7tuFU\n+aznK3VZguNg45ox3p6smf2EC2U/kRb7U0Q2mRjcIUIJUDxJ1oPcTEpQwNtdRqRWyoUjbfEQIbib\nzF/rJoalbBzl/bcRzAhfC7kXN2Ba7R6MMUzwdsFoXpaQXCSdPodKRlAbcmtN9blRSi5Eym6hu+aq\ntZZ7R24rFUduIVwaKvhTIdlEIlV5M+FGkaBfh9G0Ar8awyRvo8PvbcME43h/nlx4Gv9aIqFgtM+R\nChkPEfE91UVJAdvlfVxFxKMOY+tdvN39hOIpT4UC6UoAUXGpAt6yVqSsySLZTihpEsbiLU1FQ96I\nnY01K0RShTIz8blUeqxcq3LNaS8oTjceK8D7MKeRkDgPIzKlaCqAOJ5gXiL2EQRz3IkR+ON+7UTC\nnSI/t0x2FXJJUovhVIy5Po8ghjFEcFEm5RmNfmlhO/xvKUYYz8cWdyXhApIbSemlUwltTNkzKuaq\nhJYoZistTGmuqoRdRRDPUG9TpvV6Ih4DkeK7h4hljPQ2ZhBMfJzPq9xAcoPJdzqR8AFv8Wvlzy0+\nPq2ZYjV7MTN8I1FgpsrofY1rFNtQTYyCy02h104I3SHe3wMYsz+v0W9t1F3+WkqH0mzHEhrjGXRP\nXhjSuA/vn5jjBB+HCgllOYqxNGtDthPFckqyUEBeiRDa3VJkIAKaSreWyxCf242NNhTMlKUjQaMx\n4v1Zjq2z4h5SVlQroWfJIt5L934OIwLeQ4lahe3EkSuiH9W7LPN2R/tczySUNcWhRvp9wzGaFUNW\n+u5QbL9N9/t+gO2xcd7mISLmKLexXESKVe4gLH+55AYTcRJlDBXMrXcukVauwjbtLdHtIJ+77QSf\nkpCWYFbsaimh9G3CrIMNhMdBylGzoHUSIUCaLm4pnS/Bjnfpr8B121NfcuIhaSuXzFYiKDuU7kHF\n7RhhSLTJ1D+ETfhmjIjkyhhCmKfQ/edJx9D9zJcxhC/7DO+Lag/WEtkR+H2biWyi4dhm3EC4FUZ4\nOxIwnYRmPpZwF8nfL7eW3Dtn+DPXEAQyGdv42sTyJSu4utTbVNWpgmsKcipmoj5LM+zAhIZ8u8r4\n2ePXjPc5UxaLTHOl+Ko+QKb0COJ0SxVXqXZF45PPXMFEBe1XY2uts7okMAcRfvnxGKNTyuxyv16+\n42Z2mASc5li1BmOJH5CSK0tpvDrvRwFkuSua5/noHKeNRB2KjltQ/EJMSnn6h73veq1kCKXIijk2\nM3OUjaZ17/T2dWTJGX6vBIS0fM2D+qwg7EQfj+hBAkunBIgmpeRsJLRwpQfLpadqcsUSJ/n8zfDn\n6DgQtTed2KOKC8g6055UEHcMwQ+2YVad3KuKVWnddHzFKKKOYw3dY4faK4OI36qQYiYes6HRRrvP\ntRI8DviYZAmtIQoMtQ9WEzVCcis3Xb3rCEtUtVfV+6c4yD7/r5Tg4v1QAstbsNMe+gsDwpJ4DVEt\nq8Io5XzLXAdbnEcw/2CXf66zYrTZ5auUD1jaSLOAR4uLP3M68dsAyllXeuhojFh/RKR6SrOVJSAN\nUs8U4WwhMjmUJSMrQoFDMV+ZpBOJ6nFlAskCkhYNURy1lDDPFRBUTEBmrbSbUUTgUK6N4USOvIKR\nWzCCVAC8DdP0dRyHXC27iSMZZDEp8D/Z56gTczFMI5ix3Bc6NHAClqasg/M2Eu4VBVLlgpEbQO6D\nbYQpL6G515+7ze/b79eoZmQSUReyh6hQnkgEN1VUuYKoAt5D1JFsIJIMpGkpEULWgyzVoYRLciJx\nhlYncajeOm/njEbbSmGWS02FgxrvIIJZKo9fFpAy+TqJmp+dGK2uJM54UoB7ud83lYjvyVpZTGQS\njSU0bGX/qepY1ttufy83ZxuRlruN7plWKric4O3JZSfNWlACgmoIzvS2J9M9m2qtz+Faoh5F9UkK\nOO8l6rIUB1hCVH/rZAKduyS6loIpCwvCXa251FlT6wk3uBQtuWilsKgIVfEfCa5DRJxCdCTvwnDM\nXfwnPItSYEspl2BnX7UBn6u1frTH9/UXCaa2EZtwHacgjVdMV1WSylDSxO/BNsNMQit+LnH42HLi\nYC2Z1vLNSlKLWe8hji+QH1QbRloCRBB9I3Fq7RZCWIzyMRwmagXkUhOzlb96BZG9AxG4ew5Rzala\nhzMIl45WT4HQDiKjpJ04wmA9oXHJPTGKcBM9z68ZRxyZoBjLcCKTaCdRAS0mpZx/ZeaIUSmONMLn\n9qeI389QrETZKGJwgzCBokCrAo8SCqoXkU9XgeNxWHHaRMKtpFz8QcQRKRA+bGW7DSYEkYT5OsKS\nlVtCWrr6oeppxYcOYu4EpSOrOr6TOLFYOf57iONNVDFcG3OjqmIdByNlYRi2tsOImIYyy2RdaSzK\n3pns/ZOrSZacBGKzbkjKhmqRVMyn4zaksHU05n4fwSDl0pKrtym0RNMT/ZptRCxSdTUKlmsNlCGm\nBI8OumfgKbAvN6OKWqWQHfZ5kounEq5t7ZXNhJLQTqRHq0Jengqth5IDpBBozSB4hDKUtEc053uI\n/SRhO5QoFpRSpjR/KTCKnbRjv239rAlcl1LasESiV2C0/F3g12qtjzSuqf+VMNlUXSymIMkr81PB\nRWUfSEuRL1Emsoqz2jECgkhVUxBa2TEKVqr6UsQwjWCuo/15nZhmpcwIaeWdhM9VgViIYBYEk5lB\nWELaiCLIYUR2l85VUvm+gqgifmmGqiNRYFLxCDEjaSpNBiK/9Vrid5ZHE1qrCrzk5tpC/LylqqEr\ncY+Tb2AAABO6SURBVJSAUhjlBpOQ2oNprdIgZX1N8vEqhVBxFcVvdvvrs7GTXlWXMYPYMLpe2UFi\nttuJFOHnEuu+y/uylGAwk+h+FlJTgIgRSxOVdaXzdpR1JWaonHsdwigtWvUDSnGdQvjtRxFWi+I0\nihUoy6wQv+mtSuFOgiFKgx+FCd3lxJEwi33dJOxkHUtRGuPXN48jkfBWZbMym6SQyXLSkS0zMKar\njL7xHJ2irYyiScQR7do/EriKUyiYLY1cSRGiKSVWDCYUxLO8D7JepOCIVygOpVqrmdj+UlW6mLus\nYqVKKya6GouH7CdStMVLJBCVWfkIcWr1Vrq7pBSYh0haUcGwLMVCHMwpZU1xnVHYCdTQf0LiRMck\nXgY8XmtdXms9CPw1MKfnRZUIVq3CFlWLLd+ltBsV0+n6FRgRy0UxhfBTK8A8ESM4BeZkwqt6WRlE\nSo2c2Gh7CkFYQwmiHUWkz3USTHMa4Use7v1QppMyP1YSRXcyV6f49/I76sgG3aNCv5GEb3qoP2sd\nkZUl/7dy+BUsPYPwnzdrQeT71H+5+iTolOM+EtuQm4lK6smEL1aHpSnbakxjLrf6651+TSfhJtvn\n7UwhjtoQI1MAezpxSN42oiJ9fONzxQt0pIPSRpWGrHTNzUTVqwKOOs5FAX1Zlh10zyzaTWieio/s\nxGhIcRMpGWI4cluKIcpK0hgkHKQtnklYAO1EVb/6pjO8JhAupBHEwYnSkJuuNVndCvJO9O+VVaeY\nmdJ6FVBVjZCq3uWG7CROUlV9kdZAFesTid8aUdag3LIT/LupPtYziMpuuX0h4oSiQ6X2Koa5yttX\nEF2Ko+InE/1Z1e9XvKOLOLdrU2PuxnubezGhIwEvWtxIWFJyQWmscreu9M+XENl5cnWPJOJ1BWP8\nSoNVNpu8IzryY6+3oYSXLfT/70mcaCGhUgJhlX/WDQoIqdJ1PTYph4nAmDaciFM5yROxBWwjMiOk\nbSonexNGnAoASqAMxhZP2VLDMK11LZF5sdKvGUtoRCpsUpB5KuE/VU2HGPhhIttCzEoWg46lkMtq\nNPFDNeOJ0zJlQe0msmmU5ivtbRjxa1+q5xCz7sK01hGE8NSzRhGaqdwjqjvoIE7Z1X8xmkGYEIX4\nyU+IwF8zI0vZUc8hMk7asQ2tVMGldM8/V/xIWpU0cGX8jPExKR1T9QvLCRfJ5MZ4ziAY5mgiS0Ua\n43Si0ErpugqMK9tJwlIB+vUEcxlGHC0/hcii0zhU8yBaVBBXVt0YImNPAn1U4/tJRAaPYinjiOI/\n/ZfLUEJIdCo3maqM5e6Qxb2bOJJCh9bJDy7FTNl9G4isKrnklhJJHrJY1hLJJIOIDDXFA6QQSlDJ\ntSNa2kbEGdcTdCuNXNa3LEdlJHUQacmKscnyrJggXkQkrmzEaHk1sU8fJWo8dL6UXLSyIM8gfh1P\n3getpRSt/UQNjYpydQqCknWWN+YJ4giQTiIhQHQ+ivgJ2f7CgMhuUiBOxWJgRKfsHgUEdV6ONpRy\nldcSWRVLiKMhlJ4GxlBUWaqgmzaa/KNTCVN/PUGE64izYhRAlGYzBCOuFYTfehdxNPV+4uwkMVe5\nz5b4s0X4NPokLV3aqoTCBoxgpQ3KP61Am6wSiIppMURp0TofSgJTcQmdnbQV21gKPMvfK5+0NMF9\nRP2CBMxgIii7lTjIcC9xMOI6Ih9/E2GJTCIqa3diNHC40W9plpO8v6r01sF78ndrg60nmMRmwi8t\nmjpE9xqGSghsWTX7iZTIJ7y/yjwbQlhgO4ifA93t7e3zsYthiRZVGPeEv5bVMtzbUC6+3ED7icwp\nueWkJMmCG06kta5vrN0Kguk8RriLtMZytypbb4TP6Wbv32j/rsP/7yQ0WWn2UnDE5FV1LAt7tL/G\nPx9OKHQKdEuRGkIcCCgFQ5l5qrWQe3g8cXCg4gYH/HvVDJ1BKHzNjL1ziHqSg8Rptsr8UpHgFMKK\nl1UhBq8q80JY40p+GO19kAKnAzPx7xb5czRHcvkqfnSIcF+u9LWQC0oekv7CiY5JnA/Mr7Ve4u9v\nBmozeF1KqTOJs/GHYr6/DUTJ/BlEnvN24pfRpGUuJRijMqSU9aEUWuX4DyUY3ypCA2umIapAT9kF\nClCpyAbigDylz430tsYQ+eOl0e4WIlA8kTgjSccciBkVH8tjhCax1K9b4+0rDVVCQ2mQsrKUvy//\nr4Ld0gh11pHO41EBj/zuqijeRwTMdU4PhOmstg8TxUDKMlLwdq2vl+ZeVo6Ok9hGMAQF7XUWkBiH\nYhw7MTeA8sfHY1qctFnFJVSUuQuLZWj+VHS4xt8PIawB+dwVC5nofdlMHMB2oPFagVpl5UEcuaKi\nsvGYVTiNqMVQnENuKWWzqNp6MxEj0XytIQRoIZisYjCKWaiPcrcpRrafqBVQzEwBex12R2PeFCPS\n/tH7SiQD6NkHfCwziRRnuUInECnEKkrVuOQC20kcoa/YiuJmsnb2E8HpduI4mGZcSmmjzTaUjaag\nt4rnFG9cS+zR0YSVM8ivU0ae9odqmZqnGOjInf3Evhb9KzaiIrhlRN2TUuKVlbXR51QxCQivyBOE\nUrbJ+9WB/R7FsyFwPQhTIF+Brcl3gF+vtS5qXFMvJAJCMuemYExcedIjsc0iBi8ttI3IrhGT3N9o\nT4VN0xqfiVjlQtAZNM2CtXbCByxh1TwKZLPfexamFYwk4icQGorcEWv8vwgOuscX5H7QUSAiRvVv\nGLFpZXbL9aD+KnC9gchEmk6U9p+BMUxp2ysbczGFyBRR3OYgYSbv8zlWhpmqSpVFIsatOhNVz6rt\nx7G1VArveiLAOxGzDOTG20f451WHIl+0MmkUV4BIW9yBaWJ7/f4ziIPsJHh0qJ3SFpvVr0qplUtP\n7jVZd7JOdzfuXUscZ65A8KOYcJLA2Y4JJQkWVfJ2YUJOlbbriJ+MVWbMNozGlfnSha3vBqLmRb7z\nnYRbSm6SicSPOE32Pon+JFilyIixao12EHE5ZYaphkLCRwJBdD+RCMqrKnoSxuiUwSSXXDN9WfE5\n0aXoTcVlSrwY0ngtJqyYodyUCvgO8X4pXVbxMFWBiz7HEenGSrNVCq6KPSf7cx4n6ntUhT4Oc1PP\nIM5Qk0WtvqpIVvUuSrVv8gNZDc/3+9YT2ZUSYlt5Fv58qafAfoJIgf3DHt/XlxOajxiGCuu2EUFj\nfbaM0JxlOh9utNnUfvb7tdJ+pDnsxjJflLMuAdUswNLMKDdd1g5E5oq0O72WZq7nK5Cs1D3ofhKm\nshqaFbj7MYJTBbJqG/YRGrlOelWlZweRuneAqHJWmzqvR4yzi9BcBaXqKRtLhUBK+dtLnNy5hqjG\nVvaYLBQVB0FkPmlNdMKpmFfTgpB2qfRPCUmIoPRowicvbVaFUBKsquqWIFfAXlXwWhNlrY0jfOaT\niTRmxUGGEC5H0YACoM00RqVYika2YnGYVcQBgVsIi1TuOWX2iAaUhgvdaUH1F9I8JxBFW6qOL4TL\nU3Oq4i/N0QiiqlptQmSJyQ3TrDTf3Ghb1uJeIh4jWiuN5+wkfs9aAkBWhVyYsozkzpW7V/GOTYTy\no2pqWV176P4DT3qtjCXtK1lYUrLkJtOJBBIuSiVWooeSS5pp9xLImi/xlEGEMNlMpLlLsezChMcm\nIi4kV7QsJPn+1b72nBIANnuf/4lnUQrsMXWglDqHqC1QNabcANKQnyCOXlBZ/ihiYZRWJyi9TOf7\nbCR+2UyLrWpj5XVLsqt6eZf3Q7n8E4lf41KAbCXGCJRv3tV4BsQJnGMJTR0iBbSD0Ih0ppG0KjHb\nw8R5MzKvFVSf6e0pu0NEtZ4oXpIV0syFV4Bbaasy37cRjHso3X9ZbUNjXmXJqQJXWvcmbPPqrCC5\nWWb6vMhlpnxzHdUhc1oHByruo1iEAoJKj9Y5TIeJXxwbRBzXIK1ezFuCTzQi63Rv43pZk6rqVmaU\n3Jqq4NeGlsUmq3EQcSaRXJAKzsu6kfa/isiGW038TOw279tZBP2q6notkX2mo2Z0hIeEjtJ9lWCg\n85fG+XNUdT2WcG+0Ea4Q0QgYfapCWgF9xS/k0l1PVIHruBsxbWEv4dLT3Db3xL7GfCtwLhemBIvi\nL0q5ViV7U6iK4UKkYusIkol+v4raFMTGx67kECkY24lz4FRjs4b4MTHNEcThlDuJuIeq+nWkjzKw\nIE57lgWubCwJx93+p6xI1fOotqW/U2AHhJA4nyDWpv9TBCcpPYruhSpiAJKq2qhyKYhQFOiRBiii\n2k0ErmWRiFlJY91J/Iyn4hUKrjfP9xEO9njfZPAQp1BKY2mOGSL/WrUOcjHI9JX7S242jfNA4xn0\neCYEo1RAtudzIYRcs15hWOO1oBoWiNRCFRxKe2quj4R9G6FdKhtMz9BrKQtNy0r9V3sa6wGMUZ/Z\nuF7CUBlmEPEPPUN0ItpoxmjEyJWarHhUR+N+acyHCYXgEEEPzXFspHtdjuoeZE2qrmVoY/5k1ULk\n0Ssu10XkzEtb1tyL6cs9CRGrkAUnptNcd4gsqq7Gd1sJJq2ED92rOdtCCHnRfRfd94AgGmx+p31O\nYwzqx4HG6yY9qi3dI3rV/EvJamvcJzdVb/3bSaQAC01hqWu1TnpWk+ZEs5ofubaHEGur7+Q6VT+a\nFoSUyCaNNi3UQ8BCTkNLov2pL0skEokEcDn9e8Df4Ke+JJFIJBIDBf8Hc5H1FwaEkNh3kq2ZRCKR\nONVQygk3IoABUkyXSCQSiYGJFBKJRCKRaIkUEolEIpFoiRQSiUQikWiJFBKJRCKRaIkUEolEIpFo\niRQSiUQikWiJFBKJRCKRaIkUEolEIpFoiRQSiUQikWiJFBKJRCKRaIkUEolEIpFoiRQSiUQikWiJ\nFBKJRCKRaIkUEolEIpFoiRQSiUQikWiJFBKJRCKRaIkUEolEIpFoiRQSiUQikWiJFBKJRCKRaIkU\nEolEIpFoiRQSiUQikWiJFBKJRCKRaIkUEolEIpFoieMSEqWU15dSflxKOVxKeWmP795XSnm8lLKo\nlPIrx9fNRCKRSJwMHK8l8SPgtcB9zQ9LKecBVwDnAf8d+NNSSjnOZz3rsXDhwpPdhQGDnItAzkUg\n56L/cVxCotb6aK31caCnAJgD/HWt9VCtdRnwOPCy43nW6YDcAIGci0DORSDnov9xomIS04CVjfer\n/bNEIpFInEIY/FQXlFLuATqbHwEV+ECt9R9PVMcSiUQicfJRaq3H30gp3wLeU2v9vr+/Gai11o/6\n+68C82qt/97LvcffgUQikTgNUWs94bHep7Qkngaanb0b+GIp5XbMzfRc4Du93dQfg0wkEonEM8Px\npsBeVkpZCZwP/FMp5V8Aaq0PA3cBDwNfAX679oXJkkgkEol+RZ+4mxKJRCLx7MRJrbgupVxSSnmk\nlPJYKeW9J7MvfYlSyrJSyn+WUn5QSvmOfzaulPL1UsqjpZSvlVLGNK7vtfCwlPLSUsoPfX4+3vh8\naCnlr/2efyulnNW/I2yNUsrnSinrSyk/bHzWL2MvpbzJr3+0lPLG/hjvk6HFXMwrpawqpXzf/y5p\nfPdsnoszSynfLKX8pJTyo1LKO/3z0442epmL6/zzgUkbtdaT8ocJqCeAGcAQ4CHgeSerP308tiXA\nuB6ffRS4yV+/F/hDf/184AdYfGimz4ksvH8HftZffwV4pb/+LeBP/fX/xGpSTvq4vT//DXgx8MP+\nHDswDlgMjAHG6vUAnIt5wLt7ufa8Z/lcTAZe7K87gEeB552OtPEkczEgaeNkWhIvAx6vtS6vtR4E\n/horwns2oHC0lTYHuNNf3wlc5q8vpZfCw1LKZGBUrfW7ft1fNO5ptvVl4BV9PoJniFrr/cDWHh+f\nyLH/sr9+JfD1Wuv2Wus24OvAEU3sZKDFXMDRxafQogD1WTQX62qtD/nrXcAi4ExOQ9poMReqIxtw\ntHEyhUTPgrtVPHsK7ipwTynlu6WUa/2zzlrrejAiASb5560KD6dhcyI05+fIPbXWw8C2Usr4EzGQ\nPsKkEzj27T72U6mA8x2llIdKKZ9tuFdOm7kopczELKwHObH7YsDPR2MuVB4w4GgjT4E9Mbig1vpS\n4FXA75RSfgETHE30ZcbAqZZGfDqP/U+B59RaXwysAz7Wh20P+LkopXRgmu31rkWftvuil7kYkLRx\nMoXEaqAZcD3TPzvlUWtd6/83An+PudbWl1I6AdxM3OCXrwamN27XPLT6vNs9pZRBwOha65YTMpi+\nQX+M/ZSgp1rrxurOYeAzxJlmz/q5KKUMxpjiX9Za/8E/Pi1po7e5GKi0cTKFxHeB55ZSZpRShgK/\nhhXhndIopYxwDYFSykjgV7DTcu8GrvHL3gRok9wN/JpnI5yNFx666b29lPKyUkoB3tjjnjf56/8B\nfPPEjuppo3B0ceU1/vpEjf1rwMWllDGllHHAxf7ZyUa3uXBGKFwO/Nhfnw5z8Xng4VrrJxqfna60\ncdRcDFjaOFkRfheYl2CR/ceBm09mX/pwTGdjmVo/wITDzf75eOBeH+/XgbGNe96HZSwsAn6l8fl/\n9TYeBz7R+LwdK1Z8HPPrzjzZ42707a+ANcB+YAXwZiyj4oSPHWM2jwOPAW8coHPxF8APnUb+HvPJ\nnw5zcQFwuLE3vu/7v1/2xUCajyeZiwFJG1lMl0gkEomWyMB1IpFIJFoihUQikUgkWiKFRCKRSCRa\nIoVEIpFIJFoihUQikUgkWiKFRCKRSCRaIoVEIpFIJFoihUQikUgkWuL/AXi9jdEC2+ZHAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23cbc400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(np.shape(tX[:, 0]))\n",
    "plt.plot(tX[:, 3], 'ro')\n",
    "plt.show()\n",
    "print(y[200])\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some helper function:\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (-1/N)*(tx.T).dot(e)\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Compute the cost by MSE\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (1/(2*N))*((e.T).dot(e))\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # concatenate column of ones of size (25000, 1)\n",
    "    result = np.ones((x.shape[0], 1))\n",
    "    for i in range(1, degree+1):\n",
    "       result = np.concatenate((result, x ** i), axis=1)\n",
    "    return result\n",
    "\n",
    "#def build_poly(x, degree):\n",
    "#    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "#    if (x.ndim == 1):\n",
    "#        result = np.empty([degree+1, len(x)])\n",
    "#        for i in range(degree+1):\n",
    "#            result[i] = x ** i\n",
    "#    else:\n",
    "#        result = np.ones(np.shape(x))\n",
    "#        for i in range(1, degree+1):\n",
    "#            result = np.concatenate((result, x ** i), 1)\n",
    "#    return result.T\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"Randomly splits the data given in input into two subsets (test/train).\n",
    "    The ratio determines the size of the training set.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    size = y.shape[0]\n",
    "    # randomly permutes array of intergers from 0 to size-1\n",
    "    indexes = np.random.permutation(size)\n",
    "    tr_size = int(np.floor(ratio * size))\n",
    "    # get (randomly generated) indexes of training/testing set\n",
    "    tr_indexes = indexes[0:tr_size]\n",
    "    te_indexes = indexes[tr_size:]\n",
    "    # split x (resp. y) into two subarrays x_tr, x_te (resp. y_tr, y_te)\n",
    "    x_tr = x[tr_indexes]\n",
    "    y_tr = y[tr_indexes]\n",
    "    x_te = x[te_indexes]\n",
    "    y_te = y[te_indexes]\n",
    "    return [x_tr, y_tr, x_te, y_te]\n",
    "\n",
    "def sigmoid_scal(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    if(t > 0):\n",
    "        return np.exp(t)/(1+np.exp(t))\n",
    "    else:\n",
    "        return 1/(1+np.exp(-t))\n",
    "    \n",
    "sigmoid = np.vectorize(sigmoid_scal)\n",
    "    \n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    #print(len(tx[0]))\n",
    "    loss = 0\n",
    "    for index, row in enumerate(tx):\n",
    "        loss += np.log(1+np.exp(row.dot(w)))-y[index]*(row).dot(w)\n",
    "    return loss\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"   \n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    w -= gamma*grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        #update w\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using stochastic gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    batch_size = 800 #try changing batch size\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        print(\"iteration\", n_iter)\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            #compute new loss and w\n",
    "            loss = compute_loss(y, tx, w) # add one loss per minibatch (compute mean)\n",
    "            w = w - gamma * gradient\n",
    "            # store loss and w in arrays\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "                 \n",
    "    return w, loss\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"performs linear regression by calculating \n",
    "    the least squares solution using normal equations.\n",
    "    returns loss and optimal wieghts.\"\"\"\n",
    "    opt_w = np.linalg.inv(tx.T.dot(tx)).dot(tx.T).dot(y)\n",
    "    #computes the loss using MSE\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "    \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression. This calculates the MSE while taking in \n",
    "    accout a regularizer that is determined by lambda. This has for effect to\n",
    "    penalize/avoid large weights in order to avoid overfitting.\"\"\"\n",
    "    # tx is the polynomial basis\n",
    "    opt_w = (np.linalg.inv(tx.T.dot(tx)+lambda_*2*len(y)*np.identity(tx.shape[1])).dot(tx.T)).dot(y)\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "\n",
    "def logistic_regression(y, x, max_iter, gamma):\n",
    "    # init parameters\n",
    "    #max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    #gamma = 0.001\n",
    "    losses = []\n",
    "    # reshape the array to please numpy\n",
    "    y.shape = (len(y), 1)\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        # 1. using gradient descent\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # 2. using stochastic gradient descent\n",
    "        # ---------- TODO ----------\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "\n",
    "    \n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    #TODO\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0. ...,  1.  0.  0.]\n",
      "ok\n",
      "(250000, 30)\n",
      "Current iteration=0, the loss=[ 6931.4718056]\n",
      "Current iteration=100, the loss=[ 5230.17131272]\n",
      "Current iteration=200, the loss=[ 5123.39239913]\n",
      "Current iteration=300, the loss=[ 5076.26016949]\n",
      "Current iteration=400, the loss=[ 5046.80451083]\n",
      "Current iteration=500, the loss=[ 5026.21586513]\n",
      "Current iteration=600, the loss=[ 5011.01904413]\n",
      "Current iteration=700, the loss=[ 4999.39163294]\n",
      "Current iteration=800, the loss=[ 4990.25310109]\n",
      "Current iteration=900, the loss=[ 4982.91921865]\n",
      "The loss=[ 4976.93636223]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression test\n",
    "max_iter = 1000\n",
    "gamma = 0.00001\n",
    "y2 = (y+1)/2\n",
    "print(y2)\n",
    "for elem in y2:\n",
    "    assert(elem == 0 or elem == 1)\n",
    "print(\"ok\")\n",
    "x3 = tX[0:10000]\n",
    "y3 = y2[0:10000]\n",
    "print(tX.shape)\n",
    "logistic_regression(y3, x3, max_iter, gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"For each degree, constructs the polynomial basis function expansion of the data\n",
    "       and stores the corresponding RMSE in an array. At the end we chose the degree that\n",
    "       generated the smallest RMSE. Of course we cannot test all degrees so this is not\n",
    "       optimal but it helps us having a good idea of the optimal degree value.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    # for each degree we store the corresponding RMSE in this array\n",
    "    rmse_array = np.array([])\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # form the data to do polynomial regression:\n",
    "        polynomial_basis = build_poly(tX, degree)\n",
    "        \n",
    "        # least square and calculate rmse:\n",
    "        mse, weight = least_squares(y, polynomial_basis)\n",
    "        rmse = np.sqrt(2*mse)\n",
    "        rmse_array = np.append(rmse_array, rmse)\n",
    "        print(\"RMSE for degree\", degree, \":\", rmse)\n",
    "    \n",
    "    # plot the RMSE in function of the degree\n",
    "    plt.plot(degrees, rmse_array)\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "    #compute the best degree\n",
    "    best_degree = degrees[np.argmin(rmse_array)]\n",
    "    print(\"The best degree among those we tested is\", best_degree, \".\")\n",
    "\n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 2 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, ratio, seed):\n",
    "    \"\"\"Calculate polyomial basis tX from x with given degree,\n",
    "    splits the data according to given ratio and then run\n",
    "    ridge regression on tX, y with different lambda values.\n",
    "    At the end we plot the RMSEs of training/testing set in\n",
    "    function of lambda in order to determine the best lambda value\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    degrees = [1, 2]\n",
    "    \n",
    "    # split the data, and return train and test data:\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    #calculate test/train RMSE for each lambda and store them in lists\n",
    "    rmse_list_tr = np.empty([len(degrees), len(lambdas)])\n",
    "    rmse_list_te = np.empty([len(degrees), len(lambdas)])\n",
    "    optimal_weights = np.empty([np.shape(tX)[1]])\n",
    "    best_RMSE = 10e10\n",
    "    for i, degree in enumerate(degrees):\n",
    "        # for each lambda, store the best RMSE and the degree that generated it\n",
    "        for j, lambd in enumerate(lambdas):\n",
    "            # compute polynomial basis from given degree\n",
    "            poly_basis_tr = build_poly(x_tr, degree)\n",
    "            poly_basis_te = build_poly(x_te, degree)\n",
    "            # compute training and testing (R)MSE for current lambda/degree\n",
    "            w_tr, mse_tr = ridge_regression(y_tr, poly_basis_tr,lambd)\n",
    "            mse_te = compute_loss(y_te, poly_basis_te, w_tr)\n",
    "            rmse_tr = np.sqrt(2*mse_tr)\n",
    "            rmse_te = np.sqrt(2*mse_te)\n",
    "            #print(\"Training RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_tr, \"\\n\")\n",
    "            #print(\"Testing RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_te, \"\\n\")\n",
    "            # Store RMSEs in arrays\n",
    "            rmse_list_tr[i][j] = rmse_tr\n",
    "            rmse_list_te[i][j] = rmse_te\n",
    "            # we do this to get optimal weights\n",
    "            if(best_RMSE > rmse_tr):\n",
    "                best_RMSE = rmse_tr\n",
    "                optimal_weights = w_tr\n",
    "    \n",
    "    # get best degree, lambda, RMSE\n",
    "    degree_index_tr, lambd_index_tr = np.where(rmse_list_tr == rmse_list_tr.min())\n",
    "    degree_index_tr, lambd_index_tr = (degree_index_tr[0],lambd_index_tr[0])\n",
    "    best_rmse_tr = rmse_list_tr[degree_index_tr][lambd_index_tr]\n",
    "    print(\"Best training RMSE is\", best_rmse_tr, \"with degree\", degrees[degree_index_tr], \"and lambda=\", lambdas[lambd_index_tr], \"\\n\")\n",
    "    return optimal_weights, best_RMSE\n",
    "    \n",
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "#w, loss = ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the weights with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial values\n",
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "poly_basis = build_poly(tX, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.339686809915\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y, tX)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.317887919219\n",
      "(250000,)\n",
      "(250000, 61)\n",
      "(61,)\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y, poly_basis)\n",
    "print(loss)\n",
    "print(y.shape)\n",
    "print(poly_basis.shape)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.433339221781\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.386047330834\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.319366952019\n"
     ]
    }
   ],
   "source": [
    "# to find optimal degree and lambda, check \"Use RR to determine optimal\n",
    "# lambda and degree\" section\n",
    "degree = 2\n",
    "poly_basis = build_poly(tX, 2)\n",
    "lambda_ = 1e-05\n",
    "w, loss = ridge_regression(y, poly_basis, lambda_)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression with data splitting and chosing best lambda/deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training RMSE is 0.799059367416 with degree 2 and lambda= 1e-05 \n",
      "\n",
      "0.799059367416\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "base_poly_x = build_poly(tX_test, 2)\n",
    "poly_basis = build_poly(tX, 2)\n",
    "w, loss = least_squares(y, poly_basis)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/submissionData/least_squares_poly_basis.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, base_poly_x)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
