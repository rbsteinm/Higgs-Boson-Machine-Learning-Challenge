{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helper function:\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (-1/N)*(tx.T).dot(e)\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Compute the cost by MSE\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (1/(2*N))*((e.T).dot(e))\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # concatenate column of ones of size (25000, 1)\n",
    "    result = np.ones((x.shape[0], 1))\n",
    "    for i in range(1, degree+1):\n",
    "       result = np.concatenate((result, x ** i), axis=1)\n",
    "    return result\n",
    "\n",
    "#def build_poly(x, degree):\n",
    "#    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "#    if (x.ndim == 1):\n",
    "#        result = np.empty([degree+1, len(x)])\n",
    "#        for i in range(degree+1):\n",
    "#            result[i] = x ** i\n",
    "#    else:\n",
    "#        result = np.ones(np.shape(x))\n",
    "#        for i in range(1, degree+1):\n",
    "#            result = np.concatenate((result, x ** i), 1)\n",
    "#    return result.T\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"Randomly splits the data given in input into two subsets (test/train).\n",
    "    The ratio determines the size of the training set.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    size = y.shape[0]\n",
    "    # randomly permutes array of intergers from 0 to size-1\n",
    "    indexes = np.random.permutation(size)\n",
    "    tr_size = int(np.floor(ratio * size))\n",
    "    # get (randomly generated) indexes of training/testing set\n",
    "    tr_indexes = indexes[0:tr_size]\n",
    "    te_indexes = indexes[tr_size:]\n",
    "    # split x (resp. y) into two subarrays x_tr, x_te (resp. y_tr, y_te)\n",
    "    x_tr = x[tr_indexes]\n",
    "    y_tr = y[tr_indexes]\n",
    "    x_te = x[te_indexes]\n",
    "    y_te = y[te_indexes]\n",
    "    return [x_tr, y_tr, x_te, y_te]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, gamma, max_iters):\n",
    "    \"\"\"performs linear regression using gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        #update w\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return losses, ws\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, gamma,max_iters):\n",
    "    \"\"\"performs linear regression using stochastic gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    initial_w = np.zeros(len(tX[0])) #try changing initial w\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    batch_size = 700 #try changing batch size\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        print(\"iteration\")\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            #compute new loss and w\n",
    "            loss = compute_loss(y, tx, w) # add one loss per minibatch (compute mean)\n",
    "            w = w - gamma * gradient\n",
    "            # store loss and w in arrays\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "                 \n",
    "    return losses, ws\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"performs linear regression by calculating \n",
    "    the least squares solution using normal equations.\n",
    "    returns loss and optimal wieghts.\"\"\"\n",
    "    opt_w = np.linalg.inv(tx.T.dot(tx)).dot(tx.T).dot(y)\n",
    "    #computes the loss using MSE\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return mse, opt_w\n",
    "    # least squares: TODO\n",
    "    # returns loss (MSE), and optimal weights\n",
    "    # ***************************************************\n",
    "    \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    #TODO\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    #TODO\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def reg_logistic_regression(y, tx, lambda_, gamma, max_iters):\n",
    "    #TODO\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000, 61)\n",
      "0.339686809915 0.317887919219\n"
     ]
    }
   ],
   "source": [
    "#l, w = least_squares_GD(y, tX, 1e-7, 10)\n",
    "poly_basis = build_poly(tX, 2)\n",
    "print(np.shape(tX))\n",
    "print(np.shape(poly_basis))\n",
    "l, w = least_squares(y, tX)\n",
    "l2, w = least_squares(y, poly_basis)\n",
    "print(l, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for degree 1 : 0.823948540296\n",
      "RMSE for degree 2 : 0.797355528255\n",
      "RMSE for degree 3 : 25.9747213842\n",
      "RMSE for degree 4 : 40.2177231713\n",
      "RMSE for degree 5 : 63.6673060443\n",
      "The best degree among those we tested is 2 .\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x278ea278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"For each degree, constructs the polynomial basis function expansion of the data\n",
    "       and stores the corresponding RMSE in an array. At the end we chose the degree that\n",
    "       generated the smallest RMSE. Of course we cannot test all degrees so this is not\n",
    "       optimal but it helps us having a good idea of the optimal degree value.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    # for each degree we store the corresponding RMSE in this array\n",
    "    rmse_array = np.array([])\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # form the data to do polynomial regression:\n",
    "        polynomial_basis = build_poly(tX, degree)\n",
    "        \n",
    "        # least square and calculate rmse:\n",
    "        mse, weight = least_squares(y, polynomial_basis)\n",
    "        rmse = np.sqrt(2*mse)\n",
    "        rmse_array = np.append(rmse_array, rmse)\n",
    "        print(\"RMSE for degree\", degree, \":\", rmse)\n",
    "    \n",
    "    # plot the RMSE in function of the degree\n",
    "    plt.plot(degrees, rmse_array)\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "    #compute the best degree\n",
    "    best_degree = degrees[np.argmin(rmse_array)]\n",
    "    print(\"The best degree among those we tested is\", best_degree, \".\")\n",
    "\n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 2 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression. This calculates the MSE while taking in \n",
    "    accout a regularizer that is determined by lambda. This has for effect to\n",
    "    penalize/avoid large weights in order to avoid overfitting.\"\"\"\n",
    "    # tx is the polynomial basis\n",
    "    opt_w = (np.linalg.inv(tx.T.dot(tx)+lamb*2*len(y)*np.identity(tx.shape[1])).dot(tx.T)).dot(y)\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return mse, opt_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE for lambda = 1e-05 and degree 1 : 0.823611740398 \n",
      "\n",
      "Testing RMSE for lambda = 1e-05 and degree 1 : 0.825347845312 \n",
      "\n",
      "Training RMSE for lambda = 0.000177827941004 and degree 1 : 0.823613653542 \n",
      "\n",
      "Testing RMSE for lambda = 0.000177827941004 and degree 1 : 0.825342184211 \n",
      "\n",
      "Training RMSE for lambda = 0.00316227766017 and degree 1 : 0.823747270793 \n",
      "\n",
      "Testing RMSE for lambda = 0.00316227766017 and degree 1 : 0.825442225594 \n",
      "\n",
      "Training RMSE for lambda = 0.056234132519 and degree 1 : 0.828185241835 \n",
      "\n",
      "Testing RMSE for lambda = 0.056234132519 and degree 1 : 0.82995456787 \n",
      "\n",
      "Training RMSE for lambda = 1.0 and degree 1 : 0.840534106634 \n",
      "\n",
      "Testing RMSE for lambda = 1.0 and degree 1 : 0.842116886198 \n",
      "\n",
      "Training RMSE for lambda = 1e-05 and degree 2 : 0.799059367416 \n",
      "\n",
      "Testing RMSE for lambda = 1e-05 and degree 2 : 0.799987584501 \n",
      "\n",
      "Training RMSE for lambda = 0.000177827941004 and degree 2 : 0.799073698896 \n",
      "\n",
      "Testing RMSE for lambda = 0.000177827941004 and degree 2 : 0.799990454625 \n",
      "\n",
      "Training RMSE for lambda = 0.00316227766017 and degree 2 : 0.800574870949 \n",
      "\n",
      "Testing RMSE for lambda = 0.00316227766017 and degree 2 : 0.801359959613 \n",
      "\n",
      "Training RMSE for lambda = 0.056234132519 and degree 2 : 0.808607511595 \n",
      "\n",
      "Testing RMSE for lambda = 0.056234132519 and degree 2 : 0.809099612687 \n",
      "\n",
      "Training RMSE for lambda = 1.0 and degree 2 : 0.816776650377 \n",
      "\n",
      "Testing RMSE for lambda = 1.0 and degree 2 : 0.817231496994 \n",
      "\n",
      "Best training RMSE is 0.799059367416 with degree 2 and lambda= 1e-05 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression_demo(x, y, ratio, seed):\n",
    "    \"\"\"Calculate polyomial basis tX from x with given degree,\n",
    "    splits the data according to given ratio and then run\n",
    "    ridge regression on tX, y with different lambda values.\n",
    "    At the end we plot the RMSEs of training/testing set in\n",
    "    function of lambda in order to determine the best lambda value\"\"\"\n",
    "    # define parameter\n",
    "    #lambdas = np.logspace(-5, 0, 15)\n",
    "    #lambdas = np.logspace(-5, 0, 15)\n",
    "    lambdas = np.logspace(-5, 0, 5)\n",
    "    degrees = [1, 2]\n",
    "    \n",
    "    # split the data, and return train and test data:\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    #calculate test/train RMSE for each lambda and store them in lists\n",
    "    rmse_list_tr = np.empty([len(degrees), len(lambdas)])\n",
    "    rmse_list_te = np.empty([len(degrees), len(lambdas)])\n",
    "    best_degrees = []\n",
    "    for i, degree in enumerate(degrees):\n",
    "        # for each lambda, store the best RMSE and the degree that generated it\n",
    "        for j, lambd in enumerate(lambdas):\n",
    "            # compute polynomial basis from given degree\n",
    "            poly_basis_tr = build_poly(x_tr, degree)\n",
    "            poly_basis_te = build_poly(x_te, degree)\n",
    "            # compute training and testing (R)MSE for current lambda/degree\n",
    "            mse_tr, w_tr = ridge_regression(y_tr, poly_basis_tr,lambd)\n",
    "            mse_te = compute_loss(y_te, poly_basis_te, w_tr)\n",
    "            rmse_tr = np.sqrt(2*mse_tr)\n",
    "            rmse_te = np.sqrt(2*mse_te)\n",
    "            print(\"Training RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_tr, \"\\n\")\n",
    "            print(\"Testing RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_te, \"\\n\")\n",
    "            # Store RMSEs in arrays\n",
    "            rmse_list_tr[i][j] = rmse_tr\n",
    "            rmse_list_te[i][j] = rmse_te\n",
    "        \n",
    "        #plot_train_test(rmse_list_tr[i][:], rmse_list_te[i][:], lambdas, degree)\n",
    "    degree_index_tr, lambd_index_tr = np.where(rmse_list_tr == rmse_list_tr.min())\n",
    "    degree_index_tr, lambd_index_tr = (degree_index_tr[0],lambd_index_tr[0])\n",
    "    best_rmse_tr = rmse_list_tr[degree_index_tr][lambd_index_tr]\n",
    "    \n",
    "    print(\"Best training RMSE is\", best_rmse_tr, \"with degree\", degrees[degree_index_tr], \"and lambda=\", lambdas[lambd_index_tr], \"\\n\")\n",
    "\n",
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
