{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Importation complete\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "print(\"Importation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    DATA_TRAIN_PATH = '../data/train.csv'\n",
    "    y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "    print(\"training data is loaded\")\n",
    "    return y, tX, ids\n",
    "y, tX, ids = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All value in y is equal either to 1 or -1.\n"
     ]
    }
   ],
   "source": [
    "# As we can see here, y only takes value -1 or 1:\n",
    "for value in y:\n",
    "    assert(value==1 or value==-1)\n",
    "print(\"All value in y is equal either to 1 or -1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that y is a binary variable. So should we modify y's domain to {0, 1} instead of {-1, 1} if we want the logistic regression methods to work?\n",
    "Note that at first sight, logistic regression seems to be the best solution to fit the data since this method was designed for binary classification.\n",
    "- We implemented two methods minus_one_to_zero() and zero_to_minus_one() in the helper methods section that translate y from one domain to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We have to handle:\n",
    "- outliers:\n",
    "A value is considered as an outlier if it does not fit in a range defined from quartiles. Outliers are replaced by the mean value of the observations.\n",
    "- unasssigned values (-999, 999):\n",
    "We proceed the same way\n",
    "- We also standardize the data using the given standardize method. Note that this adds a row of ones in front of the data tX, whose dimension change as we can see:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "tX = data_cleaning(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to get rid of features that don't give enough information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of features in tX: 31\n",
      "New number of features in tX: 25\n",
      "PCA completed\n"
     ]
    }
   ],
   "source": [
    "#tX = PCA(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errs = []\n",
    "degrees = [4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "for degree in degrees:\n",
    "    err = cross_validation_error(tX, y, degree, 1e-11, ridge_regression, k_fold=4)\n",
    "    errs.append(err)\n",
    "    print(\"fitting for degree\",degree,\": \",err,\"%\")\n",
    "plt.plot(degrees, errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tX_subset = tX_tr[0:20000]\n",
    "y_subset = y_tr[0:20000]\n",
    "w_initial = np.zeros(tX_subset.shape[1])\n",
    "w, loss = reg_logistic_regression(y_subset,tX_subset, 0, w_initial, 50, 1e-2)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree for least squares method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 3 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Also the result is biased because the data is not split into training/testing subsets. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda\n",
    "This is a demo where we use ridge regression to deduce the loss function and the error percentage of the testing data for each (degree, lambda) pair. We iterate over different degree/lambda values to find the best ones. Recall that lambda is a coefficient penalizing the size of regression coefficients. Ridge regression introduces bias but reduces the variance of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a few tests with ridge regression, it seems like we should not go above degree 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cross-Validation to have better error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demo of the cross-validation function. We basically run cross-validation for different lamdda/degree combinations to determine which one gives the smallest error, exactly as we did above with ridge_regression_demo.\n",
    "The difference is that above it was a simple 2-fold split of the data, but now we do a k-fold (here k=4) which gives us a less biased error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-1827a3f72877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    520\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m                 \u001b[1;31m# we specify the name of the method we want to use with cross-validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m                 \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m                 \u001b[0mloss_tr_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                 \u001b[0mloss_te_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree, method_to_use)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0my_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgroup_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mx_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgroup_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[0my_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgroup_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "degrees = [5]\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-12, -9, 6)\n",
    "cross_validation_demo(tX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ridge regression: best degree seems to be 5\n",
    "lambda_ between 1e-12 ans 1e0\n",
    "best lambda is 1.58489319246e-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the weights with different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y, tX, ids = load_data()\n",
    "#tX = data_cleaning(tX)\n",
    "tX_tr, y_tr, tX_te, y_te = split_data(tX, y, 3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitting: 77.3616 %\n",
      "AMS: 698.749183579\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_tr, tX_tr)\n",
    "print(\"Data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting: 79.58239999999999 %\n",
      "AMS: 730.512848485\n"
     ]
    }
   ],
   "source": [
    "poly_basis_tr = build_poly(tX_tr, 2)\n",
    "poly_basis_te = build_poly(tX_te, 2)\n",
    "w, loss = least_squares(y_tr, poly_basis_tr)\n",
    "# build a polynomial basis of the same size as training set for the testing set\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting 72.52799999999999 %\n",
      "AMS: 625.342987397\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 20\n",
    "w, loss = least_squares_GD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting 72.5264 %\n",
      "AMS: 625.317583103\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "w, loss = least_squares_SGD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting: 79.81439999999999 %\n",
      "AMS: 733.768544131\n"
     ]
    }
   ],
   "source": [
    "# to find optimal degree and lambda, check \"Use RR to determine optimal\n",
    "# lambda and degree\" section\n",
    "degree = 5\n",
    "poly_basis_tr = build_poly(tX_tr, degree)\n",
    "lambda_ = 1e-11\n",
    "poly_basis_te = build_poly(tX_te, degree)\n",
    "w, loss = ridge_regression(y_tr, poly_basis_tr, lambda_)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=129965.09635472338\n",
      "Current iteration=1, the loss=100863.09323185757\n",
      "Current iteration=2, the loss=94521.44296147379\n",
      "Current iteration=3, the loss=91895.31485633172\n",
      "Current iteration=4, the loss=90614.02620289382\n",
      "Current iteration=5, the loss=89949.2744407631\n",
      "Current iteration=6, the loss=89566.84211172379\n",
      "Current iteration=7, the loss=89329.42488536764\n",
      "Current iteration=8, the loss=89172.26421085933\n",
      "Current iteration=9, the loss=89063.382442311\n",
      "Current iteration=10, the loss=88985.36215163303\n",
      "Current iteration=11, the loss=88927.96562278115\n",
      "Current iteration=12, the loss=88884.8168015312\n",
      "Current iteration=13, the loss=88851.77593771191\n",
      "Current iteration=14, the loss=88826.06959855967\n",
      "Current iteration=15, the loss=88805.79117295034\n",
      "Current iteration=16, the loss=88789.60027293592\n",
      "Current iteration=17, the loss=88776.53547130805\n",
      "Current iteration=18, the loss=88765.89449382323\n",
      "Current iteration=19, the loss=88757.1558078994\n",
      "Current iteration=20, the loss=88749.92625138172\n",
      "Current iteration=21, the loss=88743.90539071252\n",
      "Current iteration=22, the loss=88738.86083380906\n",
      "Current iteration=23, the loss=88734.61084423003\n",
      "Current iteration=24, the loss=88731.01190341434\n",
      "Current iteration=25, the loss=88727.94968003679\n",
      "Current iteration=26, the loss=88725.33238125528\n",
      "Current iteration=27, the loss=88723.08579362884\n",
      "Current iteration=28, the loss=88721.14953939544\n",
      "Current iteration=29, the loss=88719.47421867996\n",
      "Current iteration=30, the loss=88718.01920564282\n",
      "Current iteration=31, the loss=88716.75093307413\n",
      "Current iteration=32, the loss=88715.64154585688\n",
      "Current iteration=33, the loss=88714.66783593029\n",
      "Current iteration=34, the loss=88713.81039407142\n",
      "Current iteration=35, the loss=88713.05293015281\n",
      "The loss=88712.38172535249\n",
      "data fitting: 77.34400000000001 %\n",
      "AMS: 698.492940348\n"
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-5\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "y_red = y01\n",
    "x_red = tX_tr\n",
    "\n",
    "w_initial = np.zeros(tX_tr.shape[1])\n",
    "w,loss = logistic_regression(y_red, x_red, w_initial, max_iter, gamma)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=693.1249363341697\n",
      "The loss=693.1026952652824\n",
      "degree 1 -> fitting: 73.56320000000001\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=693.035534374253\n",
      "The loss=692.9241656477159\n",
      "degree 2 -> fitting: 67.4624\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=691.2371115088397\n",
      "Current iteration=2, the loss=690.569089351456\n",
      "Current iteration=3, the loss=689.9891483731863\n",
      "Current iteration=4, the loss=689.4507329775669\n",
      "The loss=688.9387539970315\n",
      "degree 3 -> fitting: 71.6928\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=697.6593250076747\n",
      "Current iteration=2, the loss=692.3235602901568\n",
      "Current iteration=3, the loss=688.1966916570093\n",
      "Current iteration=4, the loss=684.8651450312649\n",
      "Current iteration=5, the loss=682.227700078745\n",
      "Current iteration=6, the loss=680.192520348591\n",
      "Current iteration=7, the loss=678.4965446704783\n",
      "Current iteration=8, the loss=676.9561644992847\n",
      "Current iteration=9, the loss=675.5254444848107\n",
      "Current iteration=10, the loss=674.1860919666902\n",
      "Current iteration=11, the loss=672.9242963410213\n",
      "Current iteration=12, the loss=671.7287714790349\n",
      "Current iteration=13, the loss=670.5902503761172\n",
      "Current iteration=14, the loss=669.501127254125\n",
      "Current iteration=15, the loss=668.4551590369383\n",
      "Current iteration=16, the loss=667.4472124304931\n",
      "Current iteration=17, the loss=666.4730522309253\n",
      "Current iteration=18, the loss=665.5291671331743\n",
      "Current iteration=19, the loss=664.6126284998863\n",
      "Current iteration=20, the loss=663.7209770706855\n",
      "Current iteration=21, the loss=662.8521326858839\n",
      "Current iteration=22, the loss=662.0043225822083\n",
      "The loss=661.1760244724278\n",
      "degree 4 -> fitting: 69.1392\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=2447.984800118443\n",
      "Current iteration=2, the loss=2313.286191417634\n",
      "Current iteration=3, the loss=2180.6574694707647\n",
      "Current iteration=4, the loss=2050.4125926318766\n",
      "Current iteration=5, the loss=1926.0430097727537\n",
      "Current iteration=6, the loss=1807.4018538569292\n",
      "Current iteration=7, the loss=1683.9778293613092\n",
      "Current iteration=8, the loss=1684.5132399090007\n",
      "Current iteration=9, the loss=1673.5297484305424\n",
      "Current iteration=10, the loss=1580.3573185319071\n",
      "Current iteration=11, the loss=1632.7170437339857\n",
      "Current iteration=12, the loss=1528.6091572881649\n",
      "Current iteration=13, the loss=1574.6941802864355\n",
      "Current iteration=14, the loss=1538.3293997117976\n",
      "Current iteration=15, the loss=1497.5010020789769\n",
      "Current iteration=16, the loss=1538.5434503004524\n",
      "Current iteration=17, the loss=1437.357282374602\n",
      "Current iteration=18, the loss=1498.9274063569874\n",
      "Current iteration=19, the loss=1459.625586006577\n",
      "Current iteration=20, the loss=1410.3824584806268\n",
      "Current iteration=21, the loss=1472.972842409584\n",
      "Current iteration=22, the loss=1365.3023673847067\n",
      "Current iteration=23, the loss=1415.013386979363\n",
      "Current iteration=24, the loss=1385.7818018281257\n",
      "Current iteration=25, the loss=1336.2192103210461\n",
      "Current iteration=26, the loss=1400.8623849992848\n",
      "Current iteration=27, the loss=1293.2651250570932\n",
      "Current iteration=28, the loss=1340.8522690281113\n",
      "Current iteration=29, the loss=1312.3731018571243\n",
      "Current iteration=30, the loss=1263.0595055218428\n",
      "Current iteration=31, the loss=1332.1244810411908\n",
      "Current iteration=32, the loss=1224.198912108194\n",
      "Current iteration=33, the loss=1267.9250709156404\n",
      "Current iteration=34, the loss=1241.9926852867866\n",
      "Current iteration=35, the loss=1199.2520117056072\n",
      "Current iteration=36, the loss=1247.6697327470733\n",
      "Current iteration=37, the loss=1147.1042744434376\n",
      "Current iteration=38, the loss=1206.4811601802917\n",
      "Current iteration=39, the loss=1201.6188270681741\n",
      "Current iteration=40, the loss=1115.5530948222922\n",
      "Current iteration=41, the loss=1185.9198378880758\n",
      "Current iteration=42, the loss=1193.1165996204386\n",
      "Current iteration=43, the loss=1100.013018310119\n",
      "Current iteration=44, the loss=1166.3888534760397\n",
      "Current iteration=45, the loss=1169.4878297260657\n",
      "Current iteration=46, the loss=1097.7223499511492\n",
      "Current iteration=47, the loss=1156.0720574117836\n",
      "Current iteration=48, the loss=1150.3353142857375\n",
      "Current iteration=49, the loss=1064.682540758289\n",
      "Current iteration=50, the loss=1150.2107177044581\n",
      "Current iteration=51, the loss=1141.4792048055062\n",
      "Current iteration=52, the loss=1045.7964280205658\n",
      "Current iteration=53, the loss=1109.3822737928372\n",
      "Current iteration=54, the loss=1139.2657369626336\n",
      "Current iteration=55, the loss=1041.077694197329\n",
      "Current iteration=56, the loss=1074.1618627660332\n",
      "Current iteration=57, the loss=1113.9880550080686\n",
      "Current iteration=58, the loss=1040.3497172846694\n",
      "Current iteration=59, the loss=1058.762742089557\n",
      "Current iteration=60, the loss=1098.9734464907235\n",
      "Current iteration=61, the loss=1011.1816195434046\n",
      "Current iteration=62, the loss=1041.5163433712785\n",
      "Current iteration=63, the loss=1096.7590411879057\n",
      "Current iteration=64, the loss=998.273717513013\n",
      "Current iteration=65, the loss=1002.2883652464292\n",
      "Current iteration=66, the loss=1095.619890208485\n",
      "Current iteration=67, the loss=995.5641536942105\n",
      "Current iteration=68, the loss=979.0625953300478\n",
      "Current iteration=69, the loss=1064.2280675394056\n",
      "Current iteration=70, the loss=991.1034156697178\n",
      "Current iteration=71, the loss=966.9534369158964\n",
      "Current iteration=72, the loss=1046.786712971332\n",
      "Current iteration=73, the loss=961.6100305281652\n",
      "Current iteration=74, the loss=959.4981930850915\n",
      "Current iteration=75, the loss=1037.727652440148\n",
      "Current iteration=76, the loss=945.2582858318655\n",
      "Current iteration=77, the loss=927.9781992371383\n",
      "Current iteration=78, the loss=1027.3898574990656\n",
      "Current iteration=79, the loss=935.0173066833784\n",
      "Current iteration=80, the loss=911.388583629655\n",
      "Current iteration=81, the loss=1002.1980608483431\n",
      "Current iteration=82, the loss=916.4460371771843\n",
      "Current iteration=83, the loss=897.1835628327942\n",
      "Current iteration=84, the loss=984.1520451874585\n",
      "Current iteration=85, the loss=923.4326101817696\n",
      "Current iteration=86, the loss=892.151404235833\n",
      "Current iteration=87, the loss=964.9314298400151\n",
      "Current iteration=88, the loss=890.2624086777639\n",
      "Current iteration=89, the loss=886.0817246244789\n",
      "Current iteration=90, the loss=955.4366289463863\n",
      "Current iteration=91, the loss=874.1874201876124\n",
      "Current iteration=92, the loss=854.7342431371027\n",
      "Current iteration=93, the loss=939.2841074250168\n",
      "Current iteration=94, the loss=862.329682488034\n",
      "Current iteration=95, the loss=843.1020205548546\n",
      "Current iteration=96, the loss=917.517029842433\n",
      "Current iteration=97, the loss=847.9532096656046\n",
      "Current iteration=98, the loss=825.1718469030926\n",
      "Current iteration=99, the loss=826.5052160054776\n",
      "The loss=846.4471771082232\n",
      "degree 5 -> fitting: 71.37599999999999\n",
      "Current iteration=0, the loss=693.1471805599322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py:191: RuntimeWarning: overflow encountered in exp\n",
      "  loss += np.log(1+np.exp(row.dot(w)))-y[index]*(row).dot(w)\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-4313d7b9d129>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mpoly_basis_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_basis_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoly_basis_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mpoly_basis_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfitting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoly_basis_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iter, gamma)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Current iteration={i}, the loss={l}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-7\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "best_fitting = 0\n",
    "y_red = y01[0:1000]\n",
    "x_red = tX_tr[0:1000]\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "best_degree = degrees[0]\n",
    "for degree in degrees:\n",
    "    poly_basis_tr = build_poly(x_red, degree)\n",
    "    initial_w = np.zeros(poly_basis_tr.shape[1])\n",
    "    w,loss = logistic_regression(y_red, poly_basis_tr, initial_w, max_iter, gamma)\n",
    "    poly_basis_te = build_poly(tX_te, degree)\n",
    "    fitting = error(y_te,predict_labels(w,poly_basis_te))\n",
    "    print(\"degree\",degree,\"-> fitting:\",fitting)\n",
    "    if(best_fitting < fitting):\n",
    "        best_fitting = fitting\n",
    "        best_degree = degree\n",
    "print(\"best fitting:\",best_fitting,\"obtained with degree\",best_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=20794.415416791326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-41f99437055a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtX_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewton_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data fitting:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AMS:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompute_AMS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mnewton_logistic_regression\u001b[1;34m(y, tx, max_iter, gamma)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[1;34m(y, tx, w, gamma)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \"\"\"\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_LGH\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mget_LGH\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_hessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcalculate_hessian\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-4\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "y_red = y01[0:30000]\n",
    "x_red = tX_tr[0:30000]\n",
    "\n",
    "w,loss = newton_logistic_regression(y_red, x_red, max_iter, gamma)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"done\")\n",
    "tX_test = data_cleaning(tX_test)\n",
    "tX_test = PCA(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tX = data_cleaning(tX)\n",
    "tX = PCA(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test ridge regression with poly basis of degree 5\n",
    "# This is the optimal solution with cleaning and no PCA\n",
    "lambda_ = 1e-11\n",
    "degree = 5\n",
    "#tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test least squares\n",
    "degree = 2\n",
    "tX_test,_,_=standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = least_squares(y, poly_basis_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test ridge regression with poly_basis of degree 2\n",
    "lambda_ = 0.00138949549437\n",
    "degree = 2\n",
    "tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test ridge regression no data cleaning\n",
    "lambda_ = 0.000268269579528 \n",
    "w, loss = ridge_regression(y, tX, lambda_)\n",
    "#print(error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(w.shape)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submissionData/RR_deg5_withPCA.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "#o = np.ones((tX_test.shape[0],1))\n",
    "\n",
    "y_pred = predict_labels(w, poly_basis_te)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred.shape)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
