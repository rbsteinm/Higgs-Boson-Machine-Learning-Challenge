{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Importation complete\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "print(\"Importation complete\")\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "DATA_TEST_PATH = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n"
     ]
    }
   ],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training data is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All value in y is equal either to 1 or -1.\n"
     ]
    }
   ],
   "source": [
    "# As we can see here, y only takes value -1 or 1:\n",
    "for value in y:\n",
    "    assert(value==1 or value==-1)\n",
    "print(\"All value in y is equal either to 1 or -1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that y is a binary variable. So should we modify y's domain to {0, 1} instead of {-1, 1} if we want the logistic regression methods to work?\n",
    "Note that at first sight, logistic regression seems to be the best solution to fit the data since this method was designed for binary classification.\n",
    "- We implemented two methods minus_one_to_zero() and zero_to_minus_one() in the helper methods section that translate y from one domain to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We have to handle:\n",
    "- outliers:\n",
    "A value is considered as an outlier if it does not fit in a range defined from quartiles. Outliers are replaced by the mean value of the observations.\n",
    "- unasssigned values (-999, 999):\n",
    "We proceed the same way\n",
    "- We also standardize the data using the given standardize method. Note that this adds a row of ones in front of the data tX, whose dimension change as we can see:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "tX = data_cleaning(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to get rid of features that don't give enough information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running some tests we figured out that PCA actually worsens our results. This seems legit since it discards some features. Note that it can still be useful for methods that can't handle calculus if the matrices are too big (ex: logistic regression with polynomial basis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of features in tX: 31\n",
      "New number of features in tX: 25\n",
      "PCA completed\n"
     ]
    }
   ],
   "source": [
    "#tX = PCA(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tX_subset = tX_tr[0:20000]\n",
    "y_subset = y_tr[0:20000]\n",
    "w_initial = np.zeros(tX_subset.shape[1])\n",
    "w, loss = reg_logistic_regression(y_subset,tX_subset, 0, w_initial, 50, 1e-2)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree for least squares method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-72f17602a742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolynomial_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mpolynomial_regression\u001b[1;34m()\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;31m# form the data to do polynomial regression:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mpolynomial_basis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;31m# least square and calculate rmse:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX' is not defined"
     ]
    }
   ],
   "source": [
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 3 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Also the result is biased because the data is not split into training/testing subsets. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda\n",
    "This is a demo where we use ridge regression to deduce the loss function and the error percentage of the testing data for each (degree, lambda) pair. We iterate over different degree/lambda values to find the best ones. Recall that lambda is a coefficient penalizing the size of regression coefficients. Ridge regression introduces bias but reduces the variance of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "test = [[1,2],[3,4]]\n",
    "print((np.argmax(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a few tests with ridge regression, it seems like we should not go above degree 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cross-Validation to find best degree and lambda values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demo of the cross-validation function. We basically run cross-validation for different lamdda/degree combinations to determine which one gives the smallest error, exactly as we did above with ridge_regression_demo.\n",
    "The difference is that above it was a simple 2-fold split of the data, but now we do a k-fold (here k=4) which gives us a less biased error.\n",
    "<br>\n",
    "The idea is to first run this demo with a large number of degree and a large domain for lambda but few lambdas to save computation time. This gives us a good idea of the values we're looking for\n",
    "<br>\n",
    "After that we can refine the search, take only one or two degrees, a smaller domain for lambda but more lambdas in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting for degree 7 and lambda 1e-12 : 75.0836\n",
      "fitting for degree 7 and lambda 1.59985871961e-12 : 78.8852\n",
      "fitting for degree 7 and lambda 2.5595479227e-12 : 80.5888\n",
      "fitting for degree 7 and lambda 4.09491506238e-12 : 81.2376\n",
      "fitting for degree 7 and lambda 6.5512855686e-12 : 81.4376\n",
      "fitting for degree 7 and lambda 1.04811313415e-11 : 81.37480000000001\n",
      "fitting for degree 7 and lambda 1.67683293681e-11 : 81.40039999999999\n",
      "fitting for degree 7 and lambda 2.68269579528e-11 : 81.42\n",
      "fitting for degree 7 and lambda 4.29193426013e-11 : 81.422\n",
      "fitting for degree 7 and lambda 6.86648845004e-11 : 81.4308\n",
      "fitting for degree 7 and lambda 1.09854114199e-10 : 81.4268\n",
      "fitting for degree 7 and lambda 1.75751062485e-10 : 81.428\n",
      "fitting for degree 7 and lambda 2.81176869797e-10 : 81.43079999999999\n",
      "fitting for degree 7 and lambda 4.49843266897e-10 : 81.43199999999999\n",
      "fitting for degree 7 and lambda 7.19685673001e-10 : 81.4324\n",
      "fitting for degree 7 and lambda 1.15139539933e-09 : 81.43360000000001\n",
      "fitting for degree 7 and lambda 1.84206996933e-09 : 81.43279999999999\n",
      "fitting for degree 7 and lambda 2.94705170255e-09 : 81.4336\n",
      "fitting for degree 7 and lambda 4.71486636346e-09 : 81.434\n",
      "fitting for degree 7 and lambda 7.54312006335e-09 : 81.43359999999998\n",
      "fitting for degree 7 and lambda 1.20679264064e-08 : 81.4332\n",
      "fitting for degree 7 and lambda 1.93069772888e-08 : 81.43359999999998\n",
      "fitting for degree 7 and lambda 3.08884359648e-08 : 81.43279999999999\n",
      "fitting for degree 7 and lambda 4.94171336132e-08 : 81.43279999999999\n",
      "fitting for degree 7 and lambda 7.90604321091e-08 : 81.43279999999999\n",
      "fitting for degree 7 and lambda 1.26485521686e-07 : 81.4324\n",
      "fitting for degree 7 and lambda 2.02358964773e-07 : 81.43279999999999\n",
      "fitting for degree 7 and lambda 3.23745754282e-07 : 81.4324\n",
      "fitting for degree 7 and lambda 5.17947467923e-07 : 81.43199999999999\n",
      "fitting for degree 7 and lambda 8.28642772855e-07 : 81.43199999999999\n",
      "fitting for degree 7 and lambda 1.32571136559e-06 : 81.43119999999999\n",
      "fitting for degree 7 and lambda 2.12095088792e-06 : 81.4308\n",
      "fitting for degree 7 and lambda 3.3932217719e-06 : 81.42760000000001\n",
      "fitting for degree 7 and lambda 5.42867543932e-06 : 81.428\n",
      "fitting for degree 7 and lambda 8.68511373751e-06 : 81.4272\n",
      "fitting for degree 7 and lambda 1.38949549437e-05 : 81.432\n",
      "fitting for degree 7 and lambda 2.22299648253e-05 : 81.4308\n",
      "fitting for degree 7 and lambda 3.55648030622e-05 : 81.4276\n",
      "fitting for degree 7 and lambda 5.68986602902e-05 : 81.4332\n",
      "fitting for degree 7 and lambda 9.10298177992e-05 : 81.4324\n",
      "fitting for degree 7 and lambda 0.00014563484775 : 81.4264\n",
      "fitting for degree 7 and lambda 0.000232995181052 : 81.4264\n",
      "fitting for degree 7 and lambda 0.000372759372031 : 81.42800000000001\n",
      "fitting for degree 7 and lambda 0.000596362331659 : 81.4212\n",
      "fitting for degree 7 and lambda 0.00095409547635 : 81.4256\n",
      "fitting for degree 7 and lambda 0.00152641796718 : 81.40960000000001\n",
      "fitting for degree 7 and lambda 0.00244205309455 : 81.39479999999999\n",
      "fitting for degree 7 and lambda 0.00390693993705 : 81.37519999999999\n",
      "fitting for degree 7 and lambda 0.00625055192527 : 81.35079999999999\n",
      "fitting for degree 7 and lambda 0.01 : 81.3068\n",
      "max fitting for lambda = 6.5512855686e-12 degree= 7 -> 81.4376\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEdCAYAAADn46tbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x33c76cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "degrees = [7]\n",
    "k_fold = 4\n",
    "#lambdas = np.logspace(-12, -9, 6)\n",
    "lambdas = np.logspace(-12, -2, 50)\n",
    "cross_validation_demo(tX, y, k_fold, degrees, lambdas, ridge_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this experiment is that the best combinaison we obtain is a fitting of 81.4376 for degree 7 and lambda = 6.5512855686e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the weights with different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n",
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "y, tX, ids = load_data()\n",
    "tX = data_cleaning(tX)\n",
    "tX_tr, y_tr, tX_te, y_te = split_data(tX, y, 3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitting: 77.3616 %\n",
      "AMS: 698.749183579\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_tr, tX_tr)\n",
    "print(\"Data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting: 79.58239999999999 %\n",
      "AMS: 730.512848485\n"
     ]
    }
   ],
   "source": [
    "poly_basis_tr = build_poly(tX_tr, 2)\n",
    "poly_basis_te = build_poly(tX_te, 2)\n",
    "w, loss = least_squares(y_tr, poly_basis_tr)\n",
    "# build a polynomial basis of the same size as training set for the testing set\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting 72.52799999999999 %\n",
      "AMS: 625.342987397\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 20\n",
    "w, loss = least_squares_GD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting 72.5264 %\n",
      "AMS: 625.317583103\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "w, loss = least_squares_SGD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting: 79.81439999999999 %\n",
      "AMS: 733.768544131\n"
     ]
    }
   ],
   "source": [
    "# to find optimal degree and lambda, check \"Use RR to determine optimal\n",
    "# lambda and degree\" section\n",
    "degree = 5\n",
    "poly_basis_tr = build_poly(tX_tr, degree)\n",
    "lambda_ = 1e-11\n",
    "poly_basis_te = build_poly(tX_te, degree)\n",
    "w, loss = ridge_regression(y_tr, poly_basis_tr, lambda_)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=103761.64118282413\n",
      "Current iteration=1, the loss=93885.7977192248\n",
      "Current iteration=2, the loss=91448.45844444552\n",
      "Current iteration=3, the loss=90306.15766872081\n",
      "Current iteration=4, the loss=89706.23392456224\n",
      "Current iteration=5, the loss=89370.71811736\n",
      "Current iteration=6, the loss=89168.42404876703\n",
      "Current iteration=7, the loss=89039.15272002181\n",
      "Current iteration=8, the loss=88952.76723435456\n",
      "Current iteration=9, the loss=88893.03184028868\n",
      "Current iteration=10, the loss=88850.56630385721\n",
      "Current iteration=11, the loss=88819.65658250837\n",
      "Current iteration=12, the loss=88796.68691519476\n",
      "Current iteration=13, the loss=88779.30019154739\n",
      "Current iteration=14, the loss=88765.9214132933\n",
      "Current iteration=15, the loss=88755.47486367195\n",
      "Current iteration=16, the loss=88747.21096279437\n",
      "Current iteration=17, the loss=88740.59756890018\n",
      "Current iteration=18, the loss=88735.25027619062\n",
      "Current iteration=19, the loss=88730.8868565768\n",
      "Current iteration=20, the loss=88727.29695152055\n",
      "Current iteration=21, the loss=88724.32157007401\n",
      "Current iteration=22, the loss=88721.83899762765\n",
      "Current iteration=23, the loss=88719.75496146544\n",
      "Current iteration=24, the loss=88717.99566523425\n",
      "Current iteration=25, the loss=88716.50278491285\n",
      "Current iteration=26, the loss=88715.2298247319\n",
      "Current iteration=27, the loss=88714.13942877464\n",
      "Current iteration=28, the loss=88713.20137316473\n",
      "Current iteration=29, the loss=88712.39104916631\n",
      "Current iteration=30, the loss=88711.6883047422\n",
      "Current iteration=31, the loss=88711.07655101405\n",
      "The loss=88710.54206675642\n",
      "data fitting: 77.3488 %\n",
      "AMS: 698.562832177\n"
     ]
    }
   ],
   "source": [
    "max_iter = 50\n",
    "gamma = 1e-5\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "y_red = y01\n",
    "x_red = tX_tr\n",
    "w_initial, _ = least_squares(y, tX)\n",
    "#w_initial = np.ones(tX_tr.shape[1])\n",
    "w,loss = logistic_regression(y_red, x_red, w_initial, max_iter, gamma)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=693.1249363341697\n",
      "The loss=693.1026952652824\n",
      "degree 1 -> fitting: 73.56320000000001\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=693.035534374253\n",
      "The loss=692.9241656477159\n",
      "degree 2 -> fitting: 67.4624\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=691.2371115088397\n",
      "Current iteration=2, the loss=690.569089351456\n",
      "Current iteration=3, the loss=689.9891483731863\n",
      "Current iteration=4, the loss=689.4507329775669\n",
      "The loss=688.9387539970315\n",
      "degree 3 -> fitting: 71.6928\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=697.6593250076747\n",
      "Current iteration=2, the loss=692.3235602901568\n",
      "Current iteration=3, the loss=688.1966916570093\n",
      "Current iteration=4, the loss=684.8651450312649\n",
      "Current iteration=5, the loss=682.227700078745\n",
      "Current iteration=6, the loss=680.192520348591\n",
      "Current iteration=7, the loss=678.4965446704783\n",
      "Current iteration=8, the loss=676.9561644992847\n",
      "Current iteration=9, the loss=675.5254444848107\n",
      "Current iteration=10, the loss=674.1860919666902\n",
      "Current iteration=11, the loss=672.9242963410213\n",
      "Current iteration=12, the loss=671.7287714790349\n",
      "Current iteration=13, the loss=670.5902503761172\n",
      "Current iteration=14, the loss=669.501127254125\n",
      "Current iteration=15, the loss=668.4551590369383\n",
      "Current iteration=16, the loss=667.4472124304931\n",
      "Current iteration=17, the loss=666.4730522309253\n",
      "Current iteration=18, the loss=665.5291671331743\n",
      "Current iteration=19, the loss=664.6126284998863\n",
      "Current iteration=20, the loss=663.7209770706855\n",
      "Current iteration=21, the loss=662.8521326858839\n",
      "Current iteration=22, the loss=662.0043225822083\n",
      "The loss=661.1760244724278\n",
      "degree 4 -> fitting: 69.1392\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=1, the loss=2447.984800118443\n",
      "Current iteration=2, the loss=2313.286191417634\n",
      "Current iteration=3, the loss=2180.6574694707647\n",
      "Current iteration=4, the loss=2050.4125926318766\n",
      "Current iteration=5, the loss=1926.0430097727537\n",
      "Current iteration=6, the loss=1807.4018538569292\n",
      "Current iteration=7, the loss=1683.9778293613092\n",
      "Current iteration=8, the loss=1684.5132399090007\n",
      "Current iteration=9, the loss=1673.5297484305424\n",
      "Current iteration=10, the loss=1580.3573185319071\n",
      "Current iteration=11, the loss=1632.7170437339857\n",
      "Current iteration=12, the loss=1528.6091572881649\n",
      "Current iteration=13, the loss=1574.6941802864355\n",
      "Current iteration=14, the loss=1538.3293997117976\n",
      "Current iteration=15, the loss=1497.5010020789769\n",
      "Current iteration=16, the loss=1538.5434503004524\n",
      "Current iteration=17, the loss=1437.357282374602\n",
      "Current iteration=18, the loss=1498.9274063569874\n",
      "Current iteration=19, the loss=1459.625586006577\n",
      "Current iteration=20, the loss=1410.3824584806268\n",
      "Current iteration=21, the loss=1472.972842409584\n",
      "Current iteration=22, the loss=1365.3023673847067\n",
      "Current iteration=23, the loss=1415.013386979363\n",
      "Current iteration=24, the loss=1385.7818018281257\n",
      "Current iteration=25, the loss=1336.2192103210461\n",
      "Current iteration=26, the loss=1400.8623849992848\n",
      "Current iteration=27, the loss=1293.2651250570932\n",
      "Current iteration=28, the loss=1340.8522690281113\n",
      "Current iteration=29, the loss=1312.3731018571243\n",
      "Current iteration=30, the loss=1263.0595055218428\n",
      "Current iteration=31, the loss=1332.1244810411908\n",
      "Current iteration=32, the loss=1224.198912108194\n",
      "Current iteration=33, the loss=1267.9250709156404\n",
      "Current iteration=34, the loss=1241.9926852867866\n",
      "Current iteration=35, the loss=1199.2520117056072\n",
      "Current iteration=36, the loss=1247.6697327470733\n",
      "Current iteration=37, the loss=1147.1042744434376\n",
      "Current iteration=38, the loss=1206.4811601802917\n",
      "Current iteration=39, the loss=1201.6188270681741\n",
      "Current iteration=40, the loss=1115.5530948222922\n",
      "Current iteration=41, the loss=1185.9198378880758\n",
      "Current iteration=42, the loss=1193.1165996204386\n",
      "Current iteration=43, the loss=1100.013018310119\n",
      "Current iteration=44, the loss=1166.3888534760397\n",
      "Current iteration=45, the loss=1169.4878297260657\n",
      "Current iteration=46, the loss=1097.7223499511492\n",
      "Current iteration=47, the loss=1156.0720574117836\n",
      "Current iteration=48, the loss=1150.3353142857375\n",
      "Current iteration=49, the loss=1064.682540758289\n",
      "Current iteration=50, the loss=1150.2107177044581\n",
      "Current iteration=51, the loss=1141.4792048055062\n",
      "Current iteration=52, the loss=1045.7964280205658\n",
      "Current iteration=53, the loss=1109.3822737928372\n",
      "Current iteration=54, the loss=1139.2657369626336\n",
      "Current iteration=55, the loss=1041.077694197329\n",
      "Current iteration=56, the loss=1074.1618627660332\n",
      "Current iteration=57, the loss=1113.9880550080686\n",
      "Current iteration=58, the loss=1040.3497172846694\n",
      "Current iteration=59, the loss=1058.762742089557\n",
      "Current iteration=60, the loss=1098.9734464907235\n",
      "Current iteration=61, the loss=1011.1816195434046\n",
      "Current iteration=62, the loss=1041.5163433712785\n",
      "Current iteration=63, the loss=1096.7590411879057\n",
      "Current iteration=64, the loss=998.273717513013\n",
      "Current iteration=65, the loss=1002.2883652464292\n",
      "Current iteration=66, the loss=1095.619890208485\n",
      "Current iteration=67, the loss=995.5641536942105\n",
      "Current iteration=68, the loss=979.0625953300478\n",
      "Current iteration=69, the loss=1064.2280675394056\n",
      "Current iteration=70, the loss=991.1034156697178\n",
      "Current iteration=71, the loss=966.9534369158964\n",
      "Current iteration=72, the loss=1046.786712971332\n",
      "Current iteration=73, the loss=961.6100305281652\n",
      "Current iteration=74, the loss=959.4981930850915\n",
      "Current iteration=75, the loss=1037.727652440148\n",
      "Current iteration=76, the loss=945.2582858318655\n",
      "Current iteration=77, the loss=927.9781992371383\n",
      "Current iteration=78, the loss=1027.3898574990656\n",
      "Current iteration=79, the loss=935.0173066833784\n",
      "Current iteration=80, the loss=911.388583629655\n",
      "Current iteration=81, the loss=1002.1980608483431\n",
      "Current iteration=82, the loss=916.4460371771843\n",
      "Current iteration=83, the loss=897.1835628327942\n",
      "Current iteration=84, the loss=984.1520451874585\n",
      "Current iteration=85, the loss=923.4326101817696\n",
      "Current iteration=86, the loss=892.151404235833\n",
      "Current iteration=87, the loss=964.9314298400151\n",
      "Current iteration=88, the loss=890.2624086777639\n",
      "Current iteration=89, the loss=886.0817246244789\n",
      "Current iteration=90, the loss=955.4366289463863\n",
      "Current iteration=91, the loss=874.1874201876124\n",
      "Current iteration=92, the loss=854.7342431371027\n",
      "Current iteration=93, the loss=939.2841074250168\n",
      "Current iteration=94, the loss=862.329682488034\n",
      "Current iteration=95, the loss=843.1020205548546\n",
      "Current iteration=96, the loss=917.517029842433\n",
      "Current iteration=97, the loss=847.9532096656046\n",
      "Current iteration=98, the loss=825.1718469030926\n",
      "Current iteration=99, the loss=826.5052160054776\n",
      "The loss=846.4471771082232\n",
      "degree 5 -> fitting: 71.37599999999999\n",
      "Current iteration=0, the loss=693.1471805599322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py:191: RuntimeWarning: overflow encountered in exp\n",
      "  loss += np.log(1+np.exp(row.dot(w)))-y[index]*(row).dot(w)\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-4313d7b9d129>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mpoly_basis_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_basis_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoly_basis_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mpoly_basis_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfitting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoly_basis_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iter, gamma)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Current iteration={i}, the loss={l}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-7\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "best_fitting = 0\n",
    "y_red = y01[0:1000]\n",
    "x_red = tX_tr[0:1000]\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "best_degree = degrees[0]\n",
    "for degree in degrees:\n",
    "    poly_basis_tr = build_poly(x_red, degree)\n",
    "    initial_w = np.zeros(poly_basis_tr.shape[1])\n",
    "    w,loss = logistic_regression(y_red, poly_basis_tr, initial_w, max_iter, gamma)\n",
    "    poly_basis_te = build_poly(tX_te, degree)\n",
    "    fitting = error(y_te,predict_labels(w,poly_basis_te))\n",
    "    print(\"degree\",degree,\"-> fitting:\",fitting)\n",
    "    if(best_fitting < fitting):\n",
    "        best_fitting = fitting\n",
    "        best_degree = degree\n",
    "print(\"best fitting:\",best_fitting,\"obtained with degree\",best_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=10397.207708401134\n",
      "The loss=10395.573724355803\n",
      "data fitting: 77.28960000000001 %\n"
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-4\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "y_red = y01[0:15000]\n",
    "x_red = tX_tr[0:15000]\n",
    "initial_w = np.zeros(x_red.shape[1])\n",
    "\n",
    "w,loss = newton_logistic_regression(y_red, x_red, max_iter, gamma, initial_w)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "#print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "shape of tX before standardizing: (568238, 30)\n",
      "shape of tX before standardizing: (568238, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"testing data is loaded\")\n",
    "tX_test = data_cleaning(tX_test)\n",
    "#tX_test = PCA(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n",
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "y, tX, ids = load_data()\n",
    "tX = data_cleaning(tX)\n",
    "#tX = PCA(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test ridge regression with poly basis of degree 5\n",
    "lambda_ = 1e-11\n",
    "degree = 5\n",
    "#tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression with poly basis of degree 7\n",
    "# This is the optimal solution with cleaning and no PCA\n",
    "lambda_ = 6.5512855686e-12\n",
    "degree = 7\n",
    "#tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test least squares\n",
    "degree = 2\n",
    "tX_test,_,_=standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = least_squares(y, poly_basis_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test ridge regression with poly_basis of degree 2\n",
    "lambda_ = 0.00138949549437\n",
    "degree = 2\n",
    "tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test ridge regression no data cleaning\n",
    "lambda_ = 0.000268269579528 \n",
    "w, loss = ridge_regression(y, tX, lambda_)\n",
    "#print(error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(w.shape)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/submissionData/RR_deg7.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, poly_basis_te)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n",
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)\n",
    "print(tX_test.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
