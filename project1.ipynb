{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(\"../data/train.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All value in y is equal either to 1 or -1.\n"
     ]
    }
   ],
   "source": [
    "# As we can see here, y only takes value -1 or 1:\n",
    "for value in y:\n",
    "    assert(value==1 or value==-1)\n",
    "print(\"All value in y is equal either to 1 or -1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that y is a binary variable. So should we modify y's coding to {0, 1} instead of {-1, 1} if we want the logistic regression methods to work?\n",
    "Note that at first sight, logistic regression seems to be the best solution to fit the data since this method was designed for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We have to handle:\n",
    "- outliers:\n",
    "A value is considered as an outlier if it does not fit in a range defined from quartiles. Outliers are replaced by the mean value of the observations.\n",
    "- unasssigned values (-999, 999):\n",
    "We proceed the same way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "done\n",
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)\n",
    "for col in tX.T:\n",
    "    q1 = np.percentile(col,25)\n",
    "    q3 = np.percentile(col,75)\n",
    "    interq = q3-q1\n",
    "    \n",
    "    col_cleaned = col[abs(col)!=999]\n",
    "    col_cleaned = col_cleaned[col_cleaned<=q3+interq]\n",
    "    col_cleaned = col_cleaned[col_cleaned>=q1-interq]\n",
    "    #print(col_cleaned.shape)\n",
    "    mean = np.mean(col_cleaned)\n",
    "    \n",
    "    col[abs(col)==999] = mean\n",
    "    col[col>q3+interq] = mean\n",
    "    col[col<q1-interq] = mean\n",
    "    \n",
    "tX,_,_ = standardize(tX)\n",
    "print(\"done\")\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "[  1.12800597e+00   2.01810042e+00  -1.84383694e-12 ...,  -1.79898422e-01\n",
      "  -5.96123654e-01  -1.84383694e-12]\n",
      "check\n"
     ]
    }
   ],
   "source": [
    "#print(tX[:, 1:tX.shape[1]].shape)\n",
    "#tx2 = tX[:, 1:tX.shape[1]]\n",
    "#print((tx2[:, 0]))\n",
    "#print((tX[:, 0]))\n",
    "#if(np.all(tX[:, 0] == 1.0)):\n",
    "#print(\"check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to get rid of features that don't give enough information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#C = 1/tX.shape[0]*tX.T.dot(tX)\n",
    "#  \n",
    "#eigenvalue, eigenvector = np.linalg.eig(C)\n",
    "#SUM = np.sum(eigenvalue)\n",
    "#\n",
    "#idx = np.argsort(eigenvalue)[::-1]\n",
    "#eigenvector = eigenvector[:,idx]\n",
    "#eigenvalue = eigenvalue[idx]\n",
    "#\n",
    "#F = 0\n",
    "#k = 0\n",
    "#while F < 0.90:\n",
    "#    F += eigenvalue[k]/SUM\n",
    "#    k = k+1   \n",
    "#\n",
    "#eigenvector=eigenvector[:, :k]\n",
    "#\n",
    "#tX = np.dot(eigenvector.T, tX.T).T\n",
    "#\n",
    "#print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some helper function:\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (-1/N)*(tx.T).dot(e)\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Compute the cost by MSE\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (1/(2*N))*((e.T).dot(e))\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # concatenate column of ones of size (25000, 1)\n",
    "    # if necessary get rid of the column of ones that's already there from standardization\n",
    "    if(np.all(x[:, 0] == 1.0)):\n",
    "        x = x[:, 1:x.shape[1]]\n",
    "    result = np.ones((x.shape[0], 1))\n",
    "    for i in range(1, degree+1):\n",
    "       result = np.concatenate((result, x ** i), axis=1)\n",
    "    return result\n",
    "\n",
    "def split_data(x, y, ratio, seed=0):\n",
    "    \"\"\"Randomly splits the data given in input into two subsets (test/train).\n",
    "    The ratio determines the size of the training set.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    size = y.shape[0]\n",
    "    # randomly permutes array of intergers from 0 to size-1\n",
    "    indexes = np.random.permutation(size)\n",
    "    tr_size = int(np.floor(ratio * size))\n",
    "    # get (randomly generated) indexes of training/testing set\n",
    "    tr_indexes = indexes[0:tr_size]\n",
    "    te_indexes = indexes[tr_size:]\n",
    "    # split x (resp. y) into two subarrays x_tr, x_te (resp. y_tr, y_te)\n",
    "    x_tr = x[tr_indexes]\n",
    "    y_tr = y[tr_indexes]\n",
    "    x_te = x[te_indexes]\n",
    "    y_te = y[te_indexes]\n",
    "    return [x_tr, y_tr, x_te, y_te]\n",
    "\n",
    "def sigmoid_scal(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    if(t < 0):\n",
    "        return np.exp(t)/(1+np.exp(t))\n",
    "    else:\n",
    "        return 1/(1+np.exp(-t))\n",
    "    \n",
    "sigmoid = np.vectorize(sigmoid_scal)\n",
    "    \n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = 0\n",
    "    for index, row in enumerate(tx):\n",
    "        loss += np.log(1+np.exp(row.dot(w)))-y[index]*(row).dot(w)\n",
    "    return loss\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"   \n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    w -= gamma*grad\n",
    "    return w, loss\n",
    "\n",
    "def error(y,y_est):\n",
    "    \"\"\"Computes the percentage of right values in the \n",
    "    regression result compared to our test data\"\"\"\n",
    "    diff = np.count_nonzero(y-y_est)/len(y)\n",
    "    return (1-diff)*100\n",
    "\n",
    "def minus_one_to_zero(y):\n",
    "    \"\"\"The domain of the y vector changes from {-1,1} to {0,1}.\n",
    "    This is useful for logistic regression.\"\"\"\n",
    "    assert(np.any(y==-1))\n",
    "    return (y+1)/2\n",
    "\n",
    "def zero_to_minus_one(y):\n",
    "    \"\"\"The domain of the y vector changes from {0,1} to {-1,1}.\n",
    "    This is useful for logistic regression.\"\"\"\n",
    "    assert(np.any(y==0))\n",
    "    return 2*y-1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX, y, tX_te, y_te = split_data(tX, y, 2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166666, 31)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        #update w\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using stochastic gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    batch_size = 800 #try changing batch size\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        print(\"iteration\", n_iter)\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            #compute new loss and w\n",
    "            loss = compute_loss(y, tx, w) # add one loss per minibatch (compute mean)\n",
    "            w = w - gamma * gradient\n",
    "            # store loss and w in arrays\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "                 \n",
    "    return w, loss\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"performs linear regression by calculating \n",
    "    the least squares solution using normal equations.\n",
    "    returns loss and optimal wieghts.\"\"\"\n",
    "    opt_w = np.linalg.inv(tx.T.dot(tx)).dot(tx.T).dot(y)\n",
    "    #computes the loss using MSE\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "    \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression. This calculates the MSE while taking in \n",
    "    accout a regularizer that is determined by lambda. This has for effect to\n",
    "    penalize/avoid large weights in order to avoid overfitting.\"\"\"\n",
    "    # tx is the polynomial basis\n",
    "    opt_w = (np.linalg.inv(tx.T.dot(tx)+lambda_*2*len(y)*np.identity(tx.shape[1])).dot(tx.T)).dot(y)\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "\n",
    "def logistic_regression(y, tx, max_iter, gamma):\n",
    "    threshold = 1e-350\n",
    "    losses = np.array([0,1])\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        w, loss = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        losses[1] = losses[0]\n",
    "        losses[0] = loss\n",
    "        if iter % 50 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # Condition d'arrÃªt: \n",
    "        if abs(losses[0]-losses[1]) < abs(losses[1]*threshold):\n",
    "            break\n",
    "            \n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, loss\n",
    "\n",
    "    \n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    #TODO\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=6931.471805600547\n",
      "Current iteration=10, the loss=4863.227221282203\n",
      "Current iteration=20, the loss=4808.499350856127\n",
      "Current iteration=30, the loss=4798.698514533813\n",
      "Current iteration=40, the loss=4795.933520174678\n",
      "Current iteration=50, the loss=4794.950444113021\n",
      "Current iteration=60, the loss=4794.544244830425\n",
      "Current iteration=70, the loss=4794.354253765837\n",
      "Current iteration=80, the loss=4794.255090032426\n",
      "Current iteration=90, the loss=4794.1984039744875\n",
      "Current iteration=100, the loss=4794.163723068511\n",
      "Current iteration=110, the loss=4794.141503430428\n",
      "Current iteration=120, the loss=4794.126844105239\n",
      "Current iteration=130, the loss=4794.116997951601\n",
      "Current iteration=140, the loss=4794.1103133212855\n",
      "Current iteration=150, the loss=4794.10574599067\n",
      "Current iteration=160, the loss=4794.10261339662\n",
      "Current iteration=170, the loss=4794.100459907386\n",
      "Current iteration=180, the loss=4794.09897743118\n",
      "Current iteration=190, the loss=4794.097956006669\n",
      "Current iteration=200, the loss=4794.097251868374\n",
      "Current iteration=210, the loss=4794.096766292448\n",
      "Current iteration=220, the loss=4794.096431364897\n",
      "Current iteration=230, the loss=4794.096200314847\n",
      "Current iteration=240, the loss=4794.096040909994\n",
      "Current iteration=250, the loss=4794.095930927373\n",
      "Current iteration=260, the loss=4794.095855040816\n",
      "Current iteration=270, the loss=4794.095802678633\n",
      "Current iteration=280, the loss=4794.09576654765\n",
      "Current iteration=290, the loss=4794.095741616199\n",
      "The loss=4794.095724412553\n",
      "77.39098087215302\n"
     ]
    }
   ],
   "source": [
    "### logistic regression test\n",
    "max_iter = 300\n",
    "gamma = 10e-5\n",
    "y01 = minus_one_to_zero(y)\n",
    "\n",
    "y_red = y01[0:10000]\n",
    "x_red = tX[0:10000]\n",
    "\n",
    "w,loss = logistic_regression(y_red, x_red, max_iter, gamma)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for degree 1 : 0.795089290362\n",
      "RMSE for degree 2 : 0.772429591799\n",
      "RMSE for degree 3 : 0.749385237938\n",
      "RMSE for degree 4 : 0.887131255846\n",
      "RMSE for degree 5 : 8.04364351084\n",
      "RMSE for degree 6 : 1.48727512735\n",
      "RMSE for degree 7 : 1.47115258664\n",
      "RMSE for degree 8 : 1.1105923246\n",
      "RMSE for degree 9 : 5.77422577771\n",
      "RMSE for degree 10 : 46.7964419249\n",
      "The best degree among those we tested is 3 .\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x31188ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"For each degree, constructs the polynomial basis function expansion of the data\n",
    "       and stores the corresponding RMSE in an array. At the end we chose the degree that\n",
    "       generated the smallest RMSE. Of course we cannot test all degrees so this is not\n",
    "       optimal but it helps us having a good idea of the optimal degree value.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "    # for each degree we store the corresponding RMSE in this array\n",
    "    rmse_array = np.array([])\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # form the data to do polynomial regression:\n",
    "        polynomial_basis = build_poly(tX, degree)\n",
    "        \n",
    "        # least square and calculate rmse:\n",
    "        weight, mse = least_squares(y, polynomial_basis)\n",
    "        rmse = np.sqrt(2*mse)\n",
    "        rmse_array = np.append(rmse_array, rmse)\n",
    "        print(\"RMSE for degree\", degree, \":\", rmse)\n",
    "    \n",
    "    # plot the RMSE in function of the degree\n",
    "    plt.plot(degrees, rmse_array)\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "    #compute the best degree\n",
    "    best_degree = degrees[np.argmin(rmse_array)]\n",
    "    print(\"The best degree among those we tested is\", best_degree, \".\")\n",
    "\n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 3 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Also the result is biased because the data is not split into training/testing subsets. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda\n",
    "This is a demo where we use ridge regression to deduce the loss function and the error percentage of the testing data for each (degree, lambda) pair. We iterate over different degree/lambda values to find the best ones. Recall that lambda is a coefficient penalizing the size of regression coefficients. Ridge regression introduces bias but reduces the variance of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training RMSE is 0.744307856954 with degree 10 and lambda= 0.000268269579528 \n",
      "\n",
      "Best fitting is 81.2213122131 % with degree 10 and lambda= 0.000117876863479 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2009be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ridge_regression_demo(x, y, ratio, seed):\n",
    "    \"\"\"Calculate polyomial basis tX from x with given degree,\n",
    "    splits the data according to given ratio and then run\n",
    "    ridge regression on tX, y with different lambda values.\n",
    "    At the end we plot the RMSEs of training/testing set in\n",
    "    function of lambda in order to determine the best lambda value\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    degrees = [10]\n",
    "    \n",
    "    # split the data, and return train and test data:\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    #calculate test/train RMSE for each lambda and store them in lists\n",
    "    rmse_list_tr = np.empty([len(degrees), len(lambdas)])\n",
    "    rmse_list_te = np.empty([len(degrees), len(lambdas)])\n",
    "    errors = np.empty([len(degrees), len(lambdas)])\n",
    "    optimal_weights_err = np.empty([np.shape(tX)[1]])\n",
    "    optimal_weights_rmse_te = np.empty([np.shape(tX)[1]])\n",
    "    best_err = 0\n",
    "    best_RMSE = 10e10\n",
    "    for i, degree in enumerate(degrees):\n",
    "        # for each lambda, store the best RMSE and the degree that generated it\n",
    "        for j, lambd in enumerate(lambdas):\n",
    "            # compute polynomial basis from given degree\n",
    "            poly_basis_tr = build_poly(x_tr, degree)\n",
    "            poly_basis_te = build_poly(x_te, degree)\n",
    "            # compute training and testing (R)MSE for current lambda/degree\n",
    "            w_tr, mse_tr = ridge_regression(y_tr, poly_basis_tr,lambd)\n",
    "            mse_te = compute_loss(y_te, poly_basis_te, w_tr)\n",
    "            rmse_tr = np.sqrt(2*mse_tr)\n",
    "            rmse_te = np.sqrt(2*mse_te)\n",
    "            err = error(y_te,predict_labels(w_tr,poly_basis_te))\n",
    "            #print(\"Training RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_tr, \"\\n\")\n",
    "            #print(\"Testing RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_te, \"\\n\")\n",
    "            #print(\"error for lambda = \", lambd, \"and degree\", degree, \":\", err)\n",
    "            # Store RMSEs in arrays\n",
    "            rmse_list_tr[i][j] = rmse_tr\n",
    "            rmse_list_te[i][j] = rmse_te\n",
    "            errors[i][j] = err\n",
    "            # we do this to get optimal weights according to the error\n",
    "            if(best_err < err):\n",
    "                best_err = err\n",
    "                optimal_weights_err = w_tr\n",
    "            # get the optimal weights according to the testing rmse\n",
    "            if(best_RMSE > rmse_te):\n",
    "                best_RMSE = rmse_te\n",
    "                optimal_weights_rmse_te = w_tr\n",
    "        # plot figures\n",
    "        plt.figure(i)\n",
    "        plot_train_test(rmse_list_tr[i, :], rmse_list_te[i, :], lambdas, degree)\n",
    "        # TODO we need to compute the error for each (d, lambda) value, store all in array and print best for each degree\n",
    "        err = error(y_te,predict_labels(w_tr,poly_basis_te))\n",
    "        plt.title((\"RR degree\", degree, \".Best Error: \", best_err))\n",
    "    \n",
    "    # get best degree, lambda according to the testing RMSE\n",
    "    degree_index_te, lambd_index_te = np.where(rmse_list_te == rmse_list_te.min())\n",
    "    degree_index_te, lambd_index_te = (degree_index_te[0],lambd_index_te[0])\n",
    "    best_rmse_te = rmse_list_te[degree_index_te][lambd_index_te]\n",
    "    print(\"Best training RMSE is\", best_rmse_te, \"with degree\", degrees[degree_index_te], \"and lambda=\", lambdas[lambd_index_te], \"\\n\")\n",
    "    # get best degree and lambda according to the error\n",
    "    degree_index_err, lambd_index_err = np.where(errors == errors.max())\n",
    "    degree_index_err, lambd_index_err = (degree_index_err[0],lambd_index_err[0])\n",
    "    best_error = errors[degree_index_err][lambd_index_err]\n",
    "    print(\"Best fitting is\", best_error, \"% with degree\", degrees[degree_index_err], \"and lambda=\", lambdas[lambd_index_err], \"\\n\")\n",
    "    #return optimal_weights, best_RMSE\n",
    "    return optimal_weights_rmse_te, best_rmse_te\n",
    "    \n",
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a few tests with ridge regression, it seems like we should not go above degree 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the weights with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial values\n",
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "poly_basis = build_poly(tX, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.11498308013536\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y, tX)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.28036575707395\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y, poly_basis)\n",
    "# build a polynomial basis of the same size as training set for the testing set\n",
    "tX_te_poly = build_poly(tX_te, 2)\n",
    "print(error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.42052463580291\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "34.440924472604216\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.53694770441837\n"
     ]
    }
   ],
   "source": [
    "# to find optimal degree and lambda, check \"Use RR to determine optimal\n",
    "# lambda and degree\" section\n",
    "degree = 10\n",
    "poly_basis = build_poly(tX, degree)\n",
    "lambda_ = 1e-04\n",
    "tX_te_poly = build_poly(tX_te, 10)\n",
    "w, loss = ridge_regression(y, poly_basis, lambda_)\n",
    "print(error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "print(min(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression with data splitting and chosing best lambda/deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO, we should remove this section, this code is already ran above under function definition and is only here\n",
    "# for explanation/demonstration purpose\n",
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "#w, loss = ridge_regression_demo(tX, y, split_ratio, seed)\n",
    "#print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 32)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=6931.471805600547\n",
      "Current iteration=10, the loss=4863.227221282203\n",
      "Current iteration=20, the loss=4808.499350856127\n",
      "Current iteration=30, the loss=4798.698514533813\n",
      "Current iteration=40, the loss=4795.933520174678\n",
      "Current iteration=50, the loss=4794.950444113021\n",
      "Current iteration=60, the loss=4794.544244830425\n",
      "Current iteration=70, the loss=4794.354253765837\n",
      "Current iteration=80, the loss=4794.255090032426\n",
      "Current iteration=90, the loss=4794.1984039744875\n",
      "The loss=4794.163723068511\n",
      "77.36218110255118\n"
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "gamma = 10e-5\n",
    "y01 = minus_one_to_zero(y)\n",
    "\n",
    "y_red = y01[0:10000]\n",
    "x_red = tX[0:10000]\n",
    "\n",
    "w,loss = logistic_regression(y_red, x_red, max_iter, gamma)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regressio with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=50, the loss=476.91560916082864\n",
      "Current iteration=100, the loss=457.3971525405103\n",
      "The loss=451.0880234527123\n",
      "fitting % for degree 1 : 76.32058943528452\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=50, the loss=428.39151253226265\n",
      "Current iteration=100, the loss=410.96562142800707\n",
      "The loss=404.4942353342304\n",
      "fitting % for degree 2 : 78.46017231862146\n",
      "Current iteration=0, the loss=693.1471805599322\n",
      "Current iteration=50, the loss=512.001995595834\n",
      "Current iteration=100, the loss=558.3316230096852\n",
      "The loss=410.1678836092355\n",
      "fitting % for degree 3 : 77.40538075695395\n",
      "Current iteration=0, the loss=693.1471805599322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbsteinm\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:58: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-485-c8f2c80fcee3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mx_red_poly_basis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_red_poly_basis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtX_te_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fitting % for degree\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_te_poly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-477-047999a8f1db>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, max_iter, gamma)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Current iteration={i}, the loss={l}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "max_iter = 150\n",
    "gamma = 10e-5\n",
    "y01 = minus_one_to_zero(y)\n",
    "\n",
    "y_red = y01[0:1000]\n",
    "x_red = tX[0:1000]\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "for degree in degrees:\n",
    "    x_red_poly_basis = build_poly(x_red, degree)\n",
    "    w,loss = logistic_regression(y_red, x_red_poly_basis, max_iter, gamma)\n",
    "    tX_te_poly = build_poly(tX_te, degree)\n",
    "    print(\"fitting % for degree\",degree,\":\",error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression with poly basis of degree 10\n",
    "lambda_ = 1e-04\n",
    "degree = 10\n",
    "tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301,)\n",
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/submissionData/ridgeRegressionPolyBasis_degree10.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "o = np.ones((tX_test.shape[0],1))\n",
    "\n",
    "y_pred = predict_labels(w, poly_basis_te)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
