{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "def load_data():\n",
    "    DATA_TRAIN_PATH = '../data/train.csv'\n",
    "    y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "    print(\"training data is loaded\")\n",
    "    return y, tX, ids\n",
    "y, tX, ids = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All value in y is equal either to 1 or -1.\n"
     ]
    }
   ],
   "source": [
    "# As we can see here, y only takes value -1 or 1:\n",
    "for value in y:\n",
    "    assert(value==1 or value==-1)\n",
    "print(\"All value in y is equal either to 1 or -1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that y is a binary variable. So should we modify y's domain to {0, 1} instead of {-1, 1} if we want the logistic regression methods to work?\n",
    "Note that at first sight, logistic regression seems to be the best solution to fit the data since this method was designed for binary classification.\n",
    "- We implemented two methods minus_one_to_zero() and zero_to_minus_one() in the helper methods section that translate y from one domain to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We have to handle:\n",
    "- outliers:\n",
    "A value is considered as an outlier if it does not fit in a range defined from quartiles. Outliers are replaced by the mean value of the observations.\n",
    "- unasssigned values (-999, 999):\n",
    "We proceed the same way\n",
    "- We also standardize the data using the given standardize method. Note that this adds a row of ones in front of the data tX, whose dimension change as we can see:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "def data_cleaning(tX):\n",
    "    print(\"shape of tX before standardizing:\",tX.shape)\n",
    "    for col in tX.T:\n",
    "        q1 = np.percentile(col,25)\n",
    "        q3 = np.percentile(col,75)\n",
    "        interq = q3-q1\n",
    "\n",
    "        col_cleaned = col[abs(col)!=999]\n",
    "        col_cleaned = col_cleaned[col_cleaned<=q3+interq]\n",
    "        col_cleaned = col_cleaned[col_cleaned>=q1-interq]\n",
    "        #print(col_cleaned.shape)\n",
    "        mean = np.mean(col_cleaned)\n",
    "\n",
    "        col[abs(col)==999] = mean\n",
    "        col[col>q3+interq] = mean\n",
    "        col[col<q1-interq] = mean\n",
    "\n",
    "    tX,_,_ = standardize(tX)\n",
    "    print(\"shape of tX before standardizing:\",tX.shape)\n",
    "    print(\"data cleaning completed\")\n",
    "    return tX\n",
    "tX = data_cleaning(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to get rid of features that don't give enough information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of features in tX: 31\n",
      "New number of features in tX: 21\n"
     ]
    }
   ],
   "source": [
    "def PCA(tX):\n",
    "    print(\"Previous number of features in tX:\", tX.shape[1])\n",
    "    C = 1/tX.shape[0]*tX.T.dot(tX)\n",
    "\n",
    "    eigenvalue, eigenvector = np.linalg.eig(C)\n",
    "    SUM = np.sum(eigenvalue)\n",
    "\n",
    "    idx = np.argsort(eigenvalue)[::-1]\n",
    "    eigenvector = eigenvector[:,idx]\n",
    "    eigenvalue = eigenvalue[idx]\n",
    "\n",
    "    F = 0\n",
    "    k = 0\n",
    "    while F < 0.90:\n",
    "        F += eigenvalue[k]/SUM\n",
    "        k = k+1   \n",
    "\n",
    "    eigenvector=eigenvector[:, :k]\n",
    "\n",
    "    tX = np.dot(eigenvector.T, tX.T).T\n",
    "    print(\"New number of features in tX:\",tX.shape[1])\n",
    "    print(\"PCA completed\")\n",
    "    return tX\n",
    "tX = PCA(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some helper function:\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (-1/N)*(tx.T).dot(e)\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Compute the cost by MSE\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (1/(2*N))*((e.T).dot(e))\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # temporarily get rid of the column of ones that's already there from standardization\n",
    "    if(np.all(x[:, 0] == 1.0)):\n",
    "        x = x[:, 1:x.shape[1]]\n",
    "    result = np.ones((x.shape[0], 1))\n",
    "    for i in range(1, degree+1):\n",
    "       result = np.concatenate((result, x ** i), axis=1)\n",
    "    return result\n",
    "\n",
    "def split_data(x, y, ratio, seed=0):\n",
    "    \"\"\"Randomly splits the data given in input into two subsets (test/train).\n",
    "    The ratio determines the size of the training set.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    size = y.shape[0]\n",
    "    # randomly permutes array of intergers from 0 to size-1\n",
    "    indexes = np.random.permutation(size)\n",
    "    tr_size = int(np.floor(ratio * size))\n",
    "    # get (randomly generated) indexes of training/testing set\n",
    "    tr_indexes = indexes[0:tr_size]\n",
    "    te_indexes = indexes[tr_size:]\n",
    "    # split x (resp. y) into two subarrays x_tr, x_te (resp. y_tr, y_te)\n",
    "    x_tr = x[tr_indexes]\n",
    "    y_tr = y[tr_indexes]\n",
    "    x_te = x[te_indexes]\n",
    "    y_te = y[te_indexes]\n",
    "    return [x_tr, y_tr, x_te, y_te]\n",
    "\n",
    "def sigmoid_scal(t):\n",
    "    \"\"\"apply sigmoid function on scalar value t.\"\"\"\n",
    "    if(t < 0):\n",
    "        return np.exp(t)/(1+np.exp(t))\n",
    "    else:\n",
    "        return 1/(1+np.exp(-t))\n",
    "\"\"\"This allows us to call the sigmoid function element-wise on a vector\"\"\"\n",
    "sigmoid = np.vectorize(sigmoid_scal)\n",
    "    \n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = 0\n",
    "    for index, row in enumerate(tx):\n",
    "        loss += np.log(1+np.exp(row.dot(w)))-y[index]*(row).dot(w)\n",
    "    return loss\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"   \n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    w -= gamma*grad\n",
    "    return w, loss\n",
    "\n",
    "def error(y,y_est):\n",
    "    \"\"\"Computes the percentage of right values in the \n",
    "    regression result compared to our test data\"\"\"\n",
    "    diff = np.count_nonzero(y-y_est)/len(y)\n",
    "    return (1-diff)*100\n",
    "\n",
    "def compute_AMS(w, true_y, tX_te):\n",
    "    \"\"\"AMS is an estimator of the precision. We want to maximize it.\n",
    "    true_y and y_pred should be in the form {0,1}\"\"\"\n",
    "    y_pred = predict_labels(w, tX_te)\n",
    "    s = sum(true_y*y_pred)\n",
    "    b = sum((true_y==0)*(y_pred==1))\n",
    "    return np.sqrt(2*((s+b)*np.log(1+s/(b+10))-s))\n",
    "\n",
    "def minus_one_to_zero(y):\n",
    "    \"\"\"The domain of the y vector changes from {-1,1} to {0,1}.\n",
    "    This is useful for logistic regression.\"\"\"\n",
    "    assert(np.any(y==-1))\n",
    "    return (y+1)/2\n",
    "\n",
    "def zero_to_minus_one(y):\n",
    "    \"\"\"The domain of the y vector changes from {0,1} to {-1,1}.\n",
    "    This is useful for logistic regression.\"\"\"\n",
    "    assert(np.any(y==0))\n",
    "    return 2*y-1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting into two subsets: training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX, y, tX_te, y_te = split_data(tX, y, 2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166666, 31)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        #update w\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using stochastic gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    batch_size = 800 #try changing batch size\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            #compute new loss and w\n",
    "            loss = compute_loss(y, tx, w) # add one loss per minibatch (compute mean)\n",
    "            w = w - gamma * gradient\n",
    "            # store loss and w in arrays\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "                 \n",
    "    return w, loss\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"performs linear regression by calculating \n",
    "    the least squares solution using normal equations.\n",
    "    returns loss and optimal wieghts.\"\"\"\n",
    "    opt_w = np.linalg.inv(tx.T.dot(tx)).dot(tx.T).dot(y)\n",
    "    #computes the loss using MSE\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "    \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression. This calculates the MSE while taking in \n",
    "    accout a regularizer that is determined by lambda. This has for effect to\n",
    "    penalize/avoid large weights in order to avoid overfitting.\"\"\"\n",
    "    # tx is the polynomial basis\n",
    "    opt_w = (np.linalg.inv(tx.T.dot(tx)+lambda_*2*len(y)*np.identity(tx.shape[1])).dot(tx.T)).dot(y)\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "\n",
    "def logistic_regression(y, tx, max_iter, gamma):\n",
    "    threshold = 1e-10\n",
    "    losses = np.array([0,1])\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        w, loss = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        losses[1] = losses[0]\n",
    "        losses[0] = loss\n",
    "        if iter % 50 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # Condition d'arrÃªt: \n",
    "        if abs(losses[0]-losses[1]) < abs(losses[1]*threshold):\n",
    "            break\n",
    "            \n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, loss\n",
    "\n",
    "    \n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    #TODO\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for degree 1 : 0.795089290362\n",
      "RMSE for degree 2 : 0.772429591799\n",
      "RMSE for degree 3 : 0.749385237938\n",
      "RMSE for degree 4 : 0.887131255846\n",
      "RMSE for degree 5 : 8.04364351084\n",
      "RMSE for degree 6 : 1.48727512735\n",
      "RMSE for degree 7 : 1.47115258664\n",
      "RMSE for degree 8 : 1.1105923246\n",
      "RMSE for degree 9 : 5.77422577771\n",
      "RMSE for degree 10 : 46.7964419249\n",
      "The best degree among those we tested is 3 .\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x31188ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"For each degree, constructs the polynomial basis function expansion of the data\n",
    "       and stores the corresponding RMSE in an array. At the end we chose the degree that\n",
    "       generated the smallest RMSE. Of course we cannot test all degrees so this is not\n",
    "       optimal but it helps us having a good idea of the optimal degree value.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "    # for each degree we store the corresponding RMSE in this array\n",
    "    rmse_array = np.array([])\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # form the data to do polynomial regression:\n",
    "        polynomial_basis = build_poly(tX, degree)\n",
    "        \n",
    "        # least square and calculate rmse:\n",
    "        weight, mse = least_squares(y, polynomial_basis)\n",
    "        rmse = np.sqrt(2*mse)\n",
    "        rmse_array = np.append(rmse_array, rmse)\n",
    "        print(\"RMSE for degree\", degree, \":\", rmse)\n",
    "    \n",
    "    # plot the RMSE in function of the degree\n",
    "    plt.plot(degrees, rmse_array)\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "    #compute the best degree\n",
    "    best_degree = degrees[np.argmin(rmse_array)]\n",
    "    print(\"The best degree among those we tested is\", best_degree, \".\")\n",
    "\n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 3 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Also the result is biased because the data is not split into training/testing subsets. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda\n",
    "This is a demo where we use ridge regression to deduce the loss function and the error percentage of the testing data for each (degree, lambda) pair. We iterate over different degree/lambda values to find the best ones. Recall that lambda is a coefficient penalizing the size of regression coefficients. Ridge regression introduces bias but reduces the variance of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best testing RMSE is 0.825340815986 with degree 1 and lambda= 0.000268269579528 \n",
      "\n",
      "Best fitting is 74.468 % with degree 1 and lambda= 5.17947467923e-05 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEdCAYAAAAvj0GNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x635da2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ridge_regression_demo(x, y, ratio, seed):\n",
    "    \"\"\"Calculate polyomial basis tX from x with given degree,\n",
    "    splits the data according to given ratio and then run\n",
    "    ridge regression on tX, y with different lambda values.\n",
    "    At the end we plot the RMSEs of training/testing set in\n",
    "    function of lambda in order to determine the best lambda value\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    degrees = [1]\n",
    "    \n",
    "    # split the data, and return train and test data:\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    #calculate test/train RMSE for each lambda and store them in lists\n",
    "    rmse_list_tr = np.empty([len(degrees), len(lambdas)])\n",
    "    rmse_list_te = np.empty([len(degrees), len(lambdas)])\n",
    "    errors = np.empty([len(degrees), len(lambdas)])\n",
    "    optimal_weights_err = np.empty([np.shape(tX)[1]])\n",
    "    optimal_weights_rmse_te = np.empty([np.shape(tX)[1]])\n",
    "    best_err = 0\n",
    "    best_RMSE = 10e10\n",
    "    for i, degree in enumerate(degrees):\n",
    "        # for each lambda, store the best RMSE and the degree that generated it\n",
    "        for j, lambd in enumerate(lambdas):\n",
    "            # compute polynomial basis from given degree\n",
    "            poly_basis_tr = build_poly(x_tr, degree)\n",
    "            poly_basis_te = build_poly(x_te, degree)\n",
    "            # compute training and testing (R)MSE for current lambda/degree\n",
    "            w_tr, mse_tr = ridge_regression(y_tr, poly_basis_tr,lambd)\n",
    "            mse_te = compute_loss(y_te, poly_basis_te, w_tr)\n",
    "            rmse_tr = np.sqrt(2*mse_tr)\n",
    "            rmse_te = np.sqrt(2*mse_te)\n",
    "            err = error(y_te,predict_labels(w_tr,poly_basis_te))\n",
    "            #print(\"Training RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_tr, \"\\n\")\n",
    "            #print(\"Testing RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_te, \"\\n\")\n",
    "            #print(\"error for lambda = \", lambd, \"and degree\", degree, \":\", err)\n",
    "            # Store RMSEs in arrays\n",
    "            rmse_list_tr[i][j] = rmse_tr\n",
    "            rmse_list_te[i][j] = rmse_te\n",
    "            errors[i][j] = err\n",
    "            # we do this to get optimal weights according to the error\n",
    "            if(best_err < err):\n",
    "                best_err = err\n",
    "                optimal_weights_err = w_tr\n",
    "            # get the optimal weights according to the testing rmse\n",
    "            if(best_RMSE > rmse_te):\n",
    "                best_RMSE = rmse_te\n",
    "                optimal_weights_rmse_te = w_tr\n",
    "        # plot figures\n",
    "        plt.figure(i)\n",
    "        plot_train_test(rmse_list_tr[i, :], rmse_list_te[i, :], lambdas, degree)\n",
    "        # TODO we need to compute the error for each (d, lambda) value, store all in array and print best for each degree\n",
    "        #err = error(y_te,predict_labels(w_tr,poly_basis_te))\n",
    "        plt.title((\"RR degree\", degree, \".Best Error: \", best_err))\n",
    "    \n",
    "    # get best degree, lambda according to the testing RMSE\n",
    "    degree_index_te, lambd_index_te = np.where(rmse_list_te == rmse_list_te.min())\n",
    "    degree_index_te, lambd_index_te = (degree_index_te[0],lambd_index_te[0])\n",
    "    best_rmse_te = rmse_list_te[degree_index_te][lambd_index_te]\n",
    "    print(\"Best testing RMSE is\", best_rmse_te, \"with degree\", degrees[degree_index_te], \"and lambda=\", lambdas[lambd_index_te], \"\\n\")\n",
    "    # get best degree and lambda according to the error\n",
    "    degree_index_err, lambd_index_err = np.where(errors == errors.max())\n",
    "    degree_index_err, lambd_index_err = (degree_index_err[0],lambd_index_err[0])\n",
    "    best_error = errors[degree_index_err][lambd_index_err]\n",
    "    print(\"Best fitting is\", best_error, \"% with degree\", degrees[degree_index_err], \"and lambda=\", lambdas[lambd_index_err], \"\\n\")\n",
    "    #return optimal_weights, best_RMSE\n",
    "    return optimal_weights_rmse_te, best_rmse_te\n",
    "    \n",
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a few tests with ridge regression, it seems like we should not go above degree 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the weights with different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n",
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "y, tX, ids = load_data()\n",
    "tX = data_cleaning(tX)\n",
    "tX_tr, y_tr, tX_te, y_te = split_data(tX, y, 4/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitting: 77.49418004655962 %\n",
      "AMS: 825.204602496\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_tr, tX_tr)\n",
    "print(\"Data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting: 79.67696258429933 %\n",
      "AMS: 861.731180242\n"
     ]
    }
   ],
   "source": [
    "poly_basis_tr = build_poly(tX_tr, 2)\n",
    "poly_basis_te = build_poly(tX_te, 2)\n",
    "w, loss = least_squares(y_tr, poly_basis_tr)\n",
    "# build a polynomial basis of the same size as training set for the testing set\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting 72.6282189742482 %\n",
      "AMS: 738.753531931\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "w, loss = least_squares_GD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting 72.62461900304797 %\n",
      "AMS: 738.686644983\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "w, loss = least_squares_SGD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting: 81.284 %\n",
      "AMS: 664.090799747\n"
     ]
    }
   ],
   "source": [
    "# to find optimal degree and lambda, check \"Use RR to determine optimal\n",
    "# lambda and degree\" section\n",
    "degree = 5\n",
    "poly_basis_tr = build_poly(tX_tr, degree)\n",
    "lambda_ = 1e-04\n",
    "poly_basis_te = build_poly(tX_te, degree)\n",
    "w, loss = ridge_regression(y_tr, poly_basis_tr, lambda_)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=6931.471805600547\n",
      "The loss=4800.393150917134\n",
      "data fitting: 77.266\n",
      "AMS: 613.905794826\n"
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "gamma = 10e-5\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "y_red = y01[0:10000]\n",
    "x_red = tX_tr[0:10000]\n",
    "\n",
    "w,loss = logistic_regression(y_red, x_red, max_iter, gamma)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regressio with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=3465.7359027999587\n",
      "The loss=3464.7027803266888\n",
      "degree 1 -> AMS: 556.91335426\n",
      "degree 1 -> fitting: 73.004\n",
      "Current iteration=0, the loss=3465.7359027999587\n",
      "Current iteration=50, the loss=3374.5051145036605\n",
      "The loss=3299.3363057928127\n",
      "degree 2 -> AMS: 499.87169188\n",
      "degree 2 -> fitting: 69.064\n",
      "Current iteration=0, the loss=3465.7359027999587\n",
      "Current iteration=50, the loss=3265.3979106009465\n",
      "The loss=3157.556982992481\n",
      "degree 3 -> AMS: 556.412796543\n",
      "degree 3 -> fitting: 72.968\n",
      "Current iteration=0, the loss=3465.7359027999587\n",
      "The loss=3924.302030874706\n",
      "degree 4 -> AMS: 502.43529962\n",
      "degree 4 -> fitting: 69.234\n",
      "best AMS: 556.91335426 obtained with degree 1\n"
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-7\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "max_AMS = 0\n",
    "y_red = y01[0:5000]\n",
    "x_red = tX_tr[0:5000]\n",
    "degrees = [1, 2, 3, 4]\n",
    "best_degree = degrees[0]\n",
    "for degree in degrees:\n",
    "    poly_basis_tr = build_poly(x_red, degree)\n",
    "    w,loss = logistic_regression(y_red, poly_basis_tr, max_iter, gamma)\n",
    "    poly_basis_te = build_poly(tX_te, degree)\n",
    "    ams = compute_AMS(w, y_te, poly_basis_te)\n",
    "    print(\"degree\",degree,\"-> AMS:\", ams)\n",
    "    print(\"degree\",degree,\"-> fitting:\",error(y_te,predict_labels(w,poly_basis_te)))\n",
    "    if(max_AMS < ams):\n",
    "        max_AMS = ams\n",
    "        best_degree = degree\n",
    "print(\"best AMS:\",max_AMS,\"obtained with degree\",best_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression with poly basis of degree 10\n",
    "lambda_ = 1e-04\n",
    "degree = 10\n",
    "tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test least squares\n",
    "degree = 2\n",
    "tX_test,_,_=standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = least_squares(y, poly_basis_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression with poly_basis of degree 2\n",
    "lambda_ = 0.00138949549437\n",
    "degree = 2\n",
    "tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test ridge regression no data cleaning\n",
    "lambda_ = 0.000268269579528 \n",
    "w, loss = ridge_regression(y, tX, lambda_)\n",
    "#print(error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61,)\n",
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/submissionData/RR_no_poly_no_cleaning.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "#o = np.ones((tX_test.shape[0],1))\n",
    "\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 10 18]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([1, 2, 3])\n",
    "test2 = np.array([4, 5, 6])\n",
    "print(test * test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
