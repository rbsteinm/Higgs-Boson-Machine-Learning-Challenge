{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importation complete\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "print(\"Importation complete\")\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "DATA_TEST_PATH = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n"
     ]
    }
   ],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training data is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As we can see here, y only takes value -1 or 1:\n",
    "for value in y:\n",
    "    assert(value==1 or value==-1)\n",
    "print(\"All value in y is equal either to 1 or -1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that y is a binary variable. So should we modify y's domain to {0, 1} instead of {-1, 1} if we want the logistic regression methods to work?\n",
    "Note that at first sight, logistic regression seems to be the best solution to fit the data since this method was designed for binary classification.\n",
    "- We implemented two methods minus_one_to_zero() and zero_to_minus_one() in the helper methods section that translate y from one domain to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We have to handle:\n",
    "- outliers:\n",
    "A value is considered as an outlier if it does not fit in a range defined from quartiles. Outliers are replaced by the mean value of the observations.\n",
    "- unasssigned values (-999, 999):\n",
    "We proceed the same way\n",
    "- We also standardize the data using the given standardize method. Note that this adds a row of ones in front of the data tX, whose dimension change as we can see:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "tX = data_cleaning(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to get rid of features that don't give enough information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running some tests we figured out that PCA actually worsens our results. This seems legit since it discards some features. Note that it can still be useful for methods that can't handle calculus if the matrices are too big (ex: logistic regression with polynomial basis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tX = PCA(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tX_subset = tX_tr[0:20000]\n",
    "y_subset = y_tr[0:20000]\n",
    "w_initial = np.zeros(tX_subset.shape[1])\n",
    "w, loss = reg_logistic_regression(y_subset,tX_subset, 0, w_initial, 50, 1e-2)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree for least squares method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 3 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Also the result is biased because the data is not split into training/testing subsets. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda\n",
    "This is a demo where we use ridge regression to deduce the loss function and the error percentage of the testing data for each (degree, lambda) pair. We iterate over different degree/lambda values to find the best ones. Recall that lambda is a coefficient penalizing the size of regression coefficients. Ridge regression introduces bias but reduces the variance of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = [[1,2],[3,4]]\n",
    "print((np.argmax(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a few tests with ridge regression, it seems like we should not go above degree 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cross-Validation to find best degree and lambda values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demo of the cross-validation function. We basically run cross-validation for different lamdda/degree combinations to determine which one gives the smallest error, exactly as we did above with ridge_regression_demo.\n",
    "The difference is that above it was a simple 2-fold split of the data, but now we do a k-fold (here k=4) which gives us a less biased error.\n",
    "<br>\n",
    "The idea is to first run this demo with a large number of degree and a large domain for lambda but few lambdas to save computation time. This gives us a good idea of the values we're looking for\n",
    "<br>\n",
    "After that we can refine the search, take only one or two degrees, a smaller domain for lambda but more lambdas in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4c08535693c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#lambdas = np.logspace(-12, -9, 6)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m(x, y, k_fold, degrees, lambdas, method_to_use, initial_w)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m                 \u001b[1;31m# we specify the name of the method we want to use with cross-validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m                 \u001b[0mfitting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_to_use\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m                 \u001b[0mfittings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitting\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_fitting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation_error\u001b[1;34m(x, y, degree, lambda_, method_to_use, k_fold)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mfitting_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfitting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mfitting_mean\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mk\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree, method_to_use)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;31m# form data with polynomial degree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[0mpoly_basis_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[0mpoly_basis_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;31m# compute weights, MSE and fitting percentage with given method (eg ridge regression)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m        \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degrees = [7]\n",
    "k_fold = 4\n",
    "#lambdas = np.logspace(-12, -9, 6)\n",
    "lambdas = np.logspace(-12, -2, 50)\n",
    "cross_validation_demo(tX, y, k_fold, degrees, lambdas, ridge_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this experiment is that the best combinaison we obtain is a fitting of 81.4376 for degree 7 and lambda = 6.5512855686e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the weights with different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tX = data_cleaning(tX)\n",
    "tX_tr, y_tr, tX_te, y_te = split_data(tX, y, 3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w, loss = least_squares(y_tr, tX_tr)\n",
    "print(\"Data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_basis_tr = build_poly(tX_tr, 2)\n",
    "poly_basis_te = build_poly(tX_te, 2)\n",
    "w, loss = least_squares(y_tr, poly_basis_tr)\n",
    "# build a polynomial basis of the same size as training set for the testing set\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 20\n",
    "w, loss = least_squares_GD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "w, loss = least_squares_SGD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to find optimal degree and lambda, check \"Use RR to determine optimal\n",
    "# lambda and degree\" section\n",
    "degree = 5\n",
    "poly_basis_tr = build_poly(tX_tr, degree)\n",
    "lambda_ = 1e-11\n",
    "poly_basis_te = build_poly(tX_te, degree)\n",
    "w, loss = ridge_regression(y_tr, poly_basis_tr, lambda_)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=129965.09635472338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py:193: RuntimeWarning: overflow encountered in exp\n",
      "  loss += np.log(1+np.exp(row.dot(w)))-y[index]*(row).dot(w)\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a1e596dc2c42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mw_initial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data fitting:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AMS:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompute_AMS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Google Drive\\EPFL\\MA1\\PCML\\Projetcs\\project1\\scripts\\implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iter, gamma)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Current iteration={i}, the loss={l}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "max_iter = 50\n",
    "gamma = 1e-2\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "w_initial = np.zeros(tX_tr.shape[1])\n",
    "w,loss = logistic_regression(y01, tX_tr, w_initial, max_iter, gamma)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-7\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "best_fitting = 0\n",
    "y_red = y01[0:1000]\n",
    "x_red = tX_tr[0:1000]\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "best_degree = degrees[0]\n",
    "for degree in degrees:\n",
    "    poly_basis_tr = build_poly(x_red, degree)\n",
    "    initial_w = np.zeros(poly_basis_tr.shape[1])\n",
    "    w,loss = logistic_regression(y_red, poly_basis_tr, initial_w, max_iter, gamma)\n",
    "    poly_basis_te = build_poly(tX_te, degree)\n",
    "    fitting = error(y_te,predict_labels(w,poly_basis_te))\n",
    "    print(\"degree\",degree,\"-> fitting:\",fitting)\n",
    "    if(best_fitting < fitting):\n",
    "        best_fitting = fitting\n",
    "        best_degree = degree\n",
    "print(\"best fitting:\",best_fitting,\"obtained with degree\",best_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=13862.943611201723\n",
      "Current iteration=1, the loss=13125.242858621055\n",
      "Current iteration=2, the loss=12401.731465432991\n",
      "Current iteration=3, the loss=11691.191489855997\n",
      "Current iteration=4, the loss=10992.48598859021\n",
      "Current iteration=5, the loss=10304.548280329807\n",
      "Current iteration=6, the loss=9626.372461992625\n",
      "Current iteration=7, the loss=8957.004962289908\n",
      "Current iteration=8, the loss=8295.53695201505\n",
      "Current iteration=9, the loss=7641.097457899936\n",
      "Current iteration=10, the loss=6992.847048545602\n",
      "Current iteration=11, the loss=6349.971977865889\n",
      "Current iteration=12, the loss=5711.678684526323\n",
      "Current iteration=13, the loss=5077.1885556197385\n",
      "Current iteration=14, the loss=4445.732869745337\n",
      "Current iteration=15, the loss=3816.5478390383496\n",
      "Current iteration=16, the loss=3188.8696717243656\n",
      "Current iteration=17, the loss=2561.929576506526\n",
      "Current iteration=18, the loss=1934.9486275015634\n",
      "Current iteration=19, the loss=1307.132403375403\n",
      "Current iteration=20, the loss=677.6653065206971\n",
      "Current iteration=21, the loss=45.704457159806516\n",
      "Current iteration=22, the loss=-589.6269574334632\n",
      "Current iteration=23, the loss=-1229.2470176176623\n",
      "Current iteration=24, the loss=-1874.123253813906\n",
      "Current iteration=25, the loss=-2525.2818723782334\n",
      "Current iteration=26, the loss=-3183.8182614266\n",
      "Current iteration=27, the loss=-3850.9090654780766\n",
      "Current iteration=28, the loss=-4527.826185149095\n",
      "Current iteration=29, the loss=-5215.953145218271\n",
      "Current iteration=30, the loss=-5916.80438760574\n",
      "Current iteration=31, the loss=-6632.048194000378\n",
      "Current iteration=32, the loss=-7363.534138265002\n",
      "Current iteration=33, the loss=-8113.326228483431\n",
      "Current iteration=34, the loss=-8883.743246793325\n",
      "Current iteration=35, the loss=-9677.40826653569\n",
      "Current iteration=36, the loss=-10497.309970620403\n",
      "Current iteration=37, the loss=-11346.879285143243\n",
      "Current iteration=38, the loss=-12230.086085708192\n",
      "Current iteration=39, the loss=-13151.562491377563\n",
      "Current iteration=40, the loss=-14116.761776331427\n",
      "Current iteration=41, the loss=-15132.16557651017\n",
      "Current iteration=42, the loss=-16205.557431514302\n",
      "Current iteration=43, the loss=-17346.388705726265\n",
      "Current iteration=44, the loss=-18566.27506774659\n",
      "Current iteration=45, the loss=-19879.680422806072\n",
      "Current iteration=46, the loss=-21304.87459968712\n",
      "Current iteration=47, the loss=-22865.298267609076\n",
      "Current iteration=48, the loss=-24591.546048819717\n",
      "Current iteration=49, the loss=-26524.30968076871\n",
      "The loss=-28718.851739012185\n",
      "data fitting: 73.93119999999999 %\n"
     ]
    }
   ],
   "source": [
    "tX_subset = tX_tr[0:20000]\n",
    "y_subset = y_tr[0:20000]\n",
    "w_initial = np.zeros(tX_subset.shape[1])\n",
    "w, loss = reg_logistic_regression(y_subset,tX_subset, 0, w_initial, 50, 1e-2)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-4\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "y_red = y01[0:15000]\n",
    "x_red = tX_tr[0:15000]\n",
    "initial_w = np.zeros(x_red.shape[1])\n",
    "\n",
    "w,loss = newton_logistic_regression(y_red, x_red, max_iter, gamma, initial_w)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "#print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"testing data is loaded\")\n",
    "tX_test = data_cleaning(tX_test)\n",
    "#tX_test = PCA(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, tX, ids = load_data()\n",
    "tX = data_cleaning(tX)\n",
    "#tX = PCA(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test ridge regression with poly basis of degree 5\n",
    "lambda_ = 1e-11\n",
    "degree = 5\n",
    "#tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test ridge regression with poly basis of degree 7\n",
    "# This is the optimal solution with cleaning and no PCA\n",
    "lambda_ = 6.5512855686e-12\n",
    "degree = 7\n",
    "#tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test least squares\n",
    "degree = 2\n",
    "tX_test,_,_=standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = least_squares(y, poly_basis_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test ridge regression with poly_basis of degree 2\n",
    "lambda_ = 0.00138949549437\n",
    "degree = 2\n",
    "tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test ridge regression no data cleaning\n",
    "lambda_ = 0.000268269579528 \n",
    "w, loss = ridge_regression(y, tX, lambda_)\n",
    "#print(error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(w.shape)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submissionData/RR_deg7.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, poly_basis_te)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred.shape)\n",
    "print(tX_test.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
