{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "def load_data():\n",
    "    DATA_TRAIN_PATH = '../data/train.csv'\n",
    "    y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "    print(\"training data is loaded\")\n",
    "    return y, tX, ids\n",
    "y, tX, ids = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As we can see here, y only takes value -1 or 1:\n",
    "for value in y:\n",
    "    assert(value==1 or value==-1)\n",
    "print(\"All value in y is equal either to 1 or -1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that y is a binary variable. So should we modify y's domain to {0, 1} instead of {-1, 1} if we want the logistic regression methods to work?\n",
    "Note that at first sight, logistic regression seems to be the best solution to fit the data since this method was designed for binary classification.\n",
    "- We implemented two methods minus_one_to_zero() and zero_to_minus_one() in the helper methods section that translate y from one domain to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We have to handle:\n",
    "- outliers:\n",
    "A value is considered as an outlier if it does not fit in a range defined from quartiles. Outliers are replaced by the mean value of the observations.\n",
    "- unasssigned values (-999, 999):\n",
    "We proceed the same way\n",
    "- We also standardize the data using the given standardize method. Note that this adds a row of ones in front of the data tX, whose dimension change as we can see:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_cleaning(tX):\n",
    "    print(\"shape of tX before standardizing:\",tX.shape)\n",
    "    for col in tX.T:\n",
    "        q1 = np.percentile(col,25)\n",
    "        q3 = np.percentile(col,75)\n",
    "        interq = q3-q1\n",
    "\n",
    "        col_cleaned = col[abs(col)!=999]\n",
    "        col_cleaned = col_cleaned[col_cleaned<=q3+interq]\n",
    "        col_cleaned = col_cleaned[col_cleaned>=q1-interq]\n",
    "        #print(col_cleaned.shape)\n",
    "        mean = np.mean(col_cleaned)\n",
    "\n",
    "        col[abs(col)==999] = mean\n",
    "        col[col>q3+interq] = mean\n",
    "        col[col<q1-interq] = mean\n",
    "\n",
    "    tX,_,_ = standardize(tX)\n",
    "    print(\"shape of tX before standardizing:\",tX.shape)\n",
    "    print(\"data cleaning completed\")\n",
    "    return tX\n",
    "#tX = data_cleaning(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to get rid of features that don't give enough information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PCA(tX):\n",
    "    print(\"Previous number of features in tX:\", tX.shape[1])\n",
    "    threshold = 0.95\n",
    "    C = 1/tX.shape[0]*tX.T.dot(tX)\n",
    "\n",
    "    eigenvalue, eigenvector = np.linalg.eig(C)\n",
    "    SUM = np.sum(eigenvalue)\n",
    "\n",
    "    idx = np.argsort(eigenvalue)[::-1]\n",
    "    eigenvector = eigenvector[:,idx]\n",
    "    eigenvalue = eigenvalue[idx]\n",
    "\n",
    "    F = 0\n",
    "    k = 0\n",
    "    while F < threshold:\n",
    "        F += eigenvalue[k]/SUM\n",
    "        k = k+1   \n",
    "\n",
    "    eigenvector=eigenvector[:, :k]\n",
    "\n",
    "    tX = np.dot(eigenvector.T, tX.T).T\n",
    "    print(\"New number of features in tX:\",tX.shape[1])\n",
    "    print(\"PCA completed\")\n",
    "    return tX\n",
    "#tX = PCA(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some helper function:\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (-1/N)*(tx.T).dot(e)\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Compute the cost by MSE\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (1/(2*N))*((e.T).dot(e))\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # temporarily get rid of the column of ones that's already there from standardization\n",
    "    if(np.all(x[:, 0] == 1.0)):\n",
    "        x = x[:, 1:x.shape[1]]\n",
    "    result = np.ones((x.shape[0], 1))\n",
    "    for i in range(1, degree+1):\n",
    "       result = np.concatenate((result, x ** i), axis=1)\n",
    "    return result\n",
    "\n",
    "def split_data(x, y, ratio, seed=0):\n",
    "    \"\"\"Randomly splits the data given in input into two subsets (test/train).\n",
    "    The ratio determines the size of the training set.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    size = y.shape[0]\n",
    "    # randomly permutes array of intergers from 0 to size-1\n",
    "    indexes = np.random.permutation(size)\n",
    "    tr_size = int(np.floor(ratio * size))\n",
    "    # get (randomly generated) indexes of training/testing set\n",
    "    tr_indexes = indexes[0:tr_size]\n",
    "    te_indexes = indexes[tr_size:]\n",
    "    # split x (resp. y) into two subarrays x_tr, x_te (resp. y_tr, y_te)\n",
    "    x_tr = x[tr_indexes]\n",
    "    y_tr = y[tr_indexes]\n",
    "    x_te = x[te_indexes]\n",
    "    y_te = y[te_indexes]\n",
    "    return [x_tr, y_tr, x_te, y_te]\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree, method_to_use):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    x_tr = np.empty([0,31])\n",
    "    x_te = np.empty([0,31])\n",
    "    y_tr = np.array([])\n",
    "    y_te = np.array([])\n",
    "    \n",
    "    for i in range(len(k_indices)):\n",
    "        subgroup_x = x[k_indices[i]]\n",
    "        subgroup_y = y[k_indices[i]]\n",
    "        if(i != k):\n",
    "            x_tr = np.concatenate((x_tr, subgroup_x))\n",
    "            y_tr = np.append(y_tr, subgroup_y)\n",
    "        else:\n",
    "            x_te = np.concatenate((x_te, subgroup_x))\n",
    "            y_te = np.append(y_te, subgroup_y)\n",
    "\n",
    "    # form data with polynomial degree\n",
    "    poly_basis_tr = build_poly(x_tr, degree)\n",
    "    poly_basis_te = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression\n",
    "    w_tr, mse_tr = method_to_use(y_tr, poly_basis_tr,lambda_)\n",
    "    y_est = predict_labels(w_tr,poly_basis_te)\n",
    "    fitting = error(y_te, y_est)\n",
    "    \n",
    "    # calculate the loss for train and test data\n",
    "    #print(\"x shape:\",poly_basis_te.shape,\"y shape:\",y_te.shape)\n",
    "    mse_te = compute_loss(y_te, poly_basis_te, w_tr)\n",
    "    loss_tr = np.sqrt(2*mse_tr)\n",
    "    loss_te = np.sqrt(2*mse_te)\n",
    "    return loss_tr, loss_te, fitting\n",
    "\n",
    "def cross_validation_error(x, y, degree, lambda_, method_to_use):\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    k = 0\n",
    "    fitting_mean = 0\n",
    "    for k in range(k_fold):\n",
    "        _,_,fitting = cross_validation(y, x, k_indices, k, lambda_, degree, method_to_use)\n",
    "        fitting_mean += fitting\n",
    "        k+=1\n",
    "        print(\"fitting:\",fitting)\n",
    "    fitting_mean /= k_fold\n",
    "    return fitting_mean\n",
    "\n",
    "def sigmoid_scal(t):\n",
    "    \"\"\"apply sigmoid function on scalar value t.\"\"\"\n",
    "    if(t < 0):\n",
    "        return np.exp(t)/(1+np.exp(t))\n",
    "    else:\n",
    "        return 1/(1+np.exp(-t))\n",
    "\"\"\"This allows us to call the sigmoid function element-wise on a vector\"\"\"\n",
    "sigmoid = np.vectorize(sigmoid_scal)\n",
    "    \n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = 0\n",
    "    for index, row in enumerate(tx):\n",
    "        loss += np.log(1+np.exp(row.dot(w)))-y[index]*(row).dot(w)\n",
    "    return loss\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"   \n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    w -= gamma*grad\n",
    "    return w, loss, gamma\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    S = sigmoid(tx.dot(w))*(1-sigmoid(tx.dot(w)))\n",
    "    S = np.reshape(S, [len(S)])\n",
    "    return tx.T.dot(np.diag(S)).dot(tx)\n",
    "\n",
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient, hessian = get_LGH(y, tx, w)\n",
    "    w -= gamma * np.linalg.inv(hessian).dot(gradient)\n",
    "    return w, loss\n",
    "\n",
    "def learning_by_penalized_gradient_descent(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hessian = get_LGH(y, tx, w)\n",
    "    grad += 2*lambda_*w\n",
    "    loss += lambda_*(np.linalg.norm(w)**2)\n",
    "    hessian += np.diag(2*lambda_*np.ones(len(hessian)))\n",
    "    w -= gamma*np.linalg.inv(hessian).dot(grad)\n",
    "    return w, loss\n",
    "\n",
    "def get_LGH(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    hessian = calculate_hessian(y, tx, w)\n",
    "    return loss, gradient, hessian\n",
    "\n",
    "def error(y,y_est):\n",
    "    \"\"\"Computes the percentage of right values in the \n",
    "    regression result compared to our test data\"\"\"\n",
    "    diff = np.count_nonzero(y-y_est)/len(y)\n",
    "    return (1-diff)*100\n",
    "\n",
    "def compute_AMS(w, true_y, tX_te):\n",
    "    \"\"\"AMS is an estimator of the precision. We want to maximize it.\n",
    "    true_y and y_pred should be in the form {0,1}\"\"\"\n",
    "    y_pred = predict_labels(w, tX_te)\n",
    "    s = sum(true_y*y_pred)\n",
    "    b = sum((true_y==0)*(y_pred==1))\n",
    "    return np.sqrt(2*((s+b)*np.log(1+s/(b+10))-s))\n",
    "\n",
    "def minus_one_to_zero(y):\n",
    "    \"\"\"The domain of the y vector changes from {-1,1} to {0,1}.\n",
    "    This is useful for logistic regression.\"\"\"\n",
    "    assert(np.any(y==-1))\n",
    "    return (y+1)/2\n",
    "\n",
    "def zero_to_minus_one(y):\n",
    "    \"\"\"The domain of the y vector changes from {0,1} to {-1,1}.\n",
    "    This is useful for logistic regression.\"\"\"\n",
    "    assert(np.any(y==0))\n",
    "    return 2*y-1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errs = []\n",
    "degrees = [4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "for degree in degrees:\n",
    "    err = cross_validation_error(tX, y, degree, 1e-11, ridge_regression)\n",
    "    errs.append(err)\n",
    "    print(\"error for degree\",degree,\": \",err)\n",
    "plt.plot(degrees, errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.argmax(errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        #update w\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using stochastic gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    batch_size = 800 #try changing batch size\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            #compute new loss and w\n",
    "            loss = compute_loss(y, tx, w) # add one loss per minibatch (compute mean)\n",
    "            w = w - gamma * gradient\n",
    "            # store loss and w in arrays\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "                 \n",
    "    return w, loss\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"performs linear regression by calculating \n",
    "    the least squares solution using normal equations.\n",
    "    returns loss and optimal wieghts.\"\"\"\n",
    "    opt_w = np.linalg.inv(tx.T.dot(tx)).dot(tx.T).dot(y)\n",
    "    #computes the loss using MSE\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "    \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression. This calculates the MSE while taking in \n",
    "    accout a regularizer that is determined by lambda. This has for effect to\n",
    "    penalize/avoid large weights in order to avoid overfitting.\"\"\"\n",
    "    # tx is the polynomial basis\n",
    "    opt_w = (np.linalg.inv(tx.T.dot(tx)+lambda_*2*len(y)*np.identity(tx.shape[1])).dot(tx.T)).dot(y)\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iter, gamma):\n",
    "    threshold = 1e-20\n",
    "    losses = np.array([0,1])\n",
    "    w = initial_w\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        w, loss, gamma = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        losses[1] = losses[0]\n",
    "        losses[0] = loss\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # Condition d'arrêt:\n",
    "        if abs(losses[0]-losses[1]) < abs(losses[1]*threshold):\n",
    "            break\n",
    "            \n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, loss\n",
    "    \n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iter, gamma):\n",
    "    threshold = 1e-350\n",
    "    losses = np.array([0,1])\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        w, loss = learning_by_penalized_gradient_descent(y, tx, w, gamma, lambda_)\n",
    "        losses[1] = losses[0]\n",
    "        losses[0] = loss\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # Condition d'arrêt: \n",
    "        if abs(losses[0]-losses[1]) < abs(losses[1]*threshold):\n",
    "            break\n",
    "\n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, loss\n",
    "    \n",
    "def newton_logistic_regression(y, tx, max_iter, gamma):\n",
    "    threshold = 1e-10\n",
    "    losses = np.array([0,1])\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        w, loss = learning_by_newton_method(y, tx, w, gamma)\n",
    "        losses[1] = losses[0]\n",
    "        losses[0] = loss\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # Condition d'arrêt:\n",
    "        if abs(losses[0]-losses[1]) < abs(losses[1]*threshold):\n",
    "            break\n",
    "            \n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tX_subset = tX_tr[0:20000]\n",
    "y_subset = y_tr[0:20000]\n",
    "w_initial = np.zeros(tX_subset.shape[1])\n",
    "w, loss = reg_logistic_regression(y_subset,tX_subset, 0, w_initial, 50, 1e-2)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"For each degree, constructs the polynomial basis function expansion of the data\n",
    "       and stores the corresponding RMSE in an array. At the end we chose the degree that\n",
    "       generated the smallest RMSE. Of course we cannot test all degrees so this is not\n",
    "       optimal but it helps us having a good idea of the optimal degree value.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "    # for each degree we store the corresponding RMSE in this array\n",
    "    rmse_array = np.array([])\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # form the data to do polynomial regression:\n",
    "        polynomial_basis = build_poly(tX, degree)\n",
    "        \n",
    "        # least square and calculate rmse:\n",
    "        weight, mse = least_squares(y, polynomial_basis)\n",
    "        rmse = np.sqrt(2*mse)\n",
    "        rmse_array = np.append(rmse_array, rmse)\n",
    "        print(\"RMSE for degree\", degree, \":\", rmse)\n",
    "    \n",
    "    # plot the RMSE in function of the degree\n",
    "    plt.plot(degrees, rmse_array)\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "    #compute the best degree\n",
    "    best_degree = degrees[np.argmin(rmse_array)]\n",
    "    print(\"The best degree among those we tested is\", best_degree, \".\")\n",
    "\n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 3 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Also the result is biased because the data is not split into training/testing subsets. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda\n",
    "This is a demo where we use ridge regression to deduce the loss function and the error percentage of the testing data for each (degree, lambda) pair. We iterate over different degree/lambda values to find the best ones. Recall that lambda is a coefficient penalizing the size of regression coefficients. Ridge regression introduces bias but reduces the variance of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, ratio, seed):\n",
    "    \"\"\"Calculate polyomial basis tX from x with given degree,\n",
    "    splits the data according to given ratio and then run\n",
    "    ridge regression on tX, y with different lambda values.\n",
    "    At the end we plot the RMSEs of training/testing set in\n",
    "    function of lambda in order to determine the best lambda value\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    \n",
    "    # split the data, and return train and test data:\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    #calculate test/train RMSE for each lambda and store them in lists\n",
    "    rmse_list_tr = np.empty([len(degrees), len(lambdas)])\n",
    "    rmse_list_te = np.empty([len(degrees), len(lambdas)])\n",
    "    errors = np.empty([len(degrees), len(lambdas)])\n",
    "    optimal_weights_err = np.empty([np.shape(tX)[1]])\n",
    "    optimal_weights_rmse_te = np.empty([np.shape(tX)[1]])\n",
    "    best_err = 0\n",
    "    best_RMSE = 10e10\n",
    "    for i, degree in enumerate(degrees):\n",
    "        # for each lambda, store the best RMSE and the degree that generated it\n",
    "        for j, lambd in enumerate(lambdas):\n",
    "            # compute polynomial basis from given degree\n",
    "            poly_basis_tr = build_poly(x_tr, degree)\n",
    "            poly_basis_te = build_poly(x_te, degree)\n",
    "            # compute training and testing (R)MSE for current lambda/degree\n",
    "            w_tr, mse_tr = ridge_regression(y_tr, poly_basis_tr,lambd)\n",
    "            mse_te = compute_loss(y_te, poly_basis_te, w_tr)\n",
    "            rmse_tr = np.sqrt(2*mse_tr)\n",
    "            rmse_te = np.sqrt(2*mse_te)\n",
    "            err = error(y_te,predict_labels(w_tr,poly_basis_te))\n",
    "            #print(\"Training RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_tr, \"\\n\")\n",
    "            #print(\"Testing RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_te, \"\\n\")\n",
    "            #print(\"error for lambda = \", lambd, \"and degree\", degree, \":\", err)\n",
    "            # Store RMSEs in arrays\n",
    "            rmse_list_tr[i][j] = rmse_tr\n",
    "            rmse_list_te[i][j] = rmse_te\n",
    "            errors[i][j] = err\n",
    "            # we do this to get optimal weights according to the error\n",
    "            if(best_err < err):\n",
    "                best_err = err\n",
    "                optimal_weights_err = w_tr\n",
    "            # get the optimal weights according to the testing rmse\n",
    "            if(best_RMSE > rmse_te):\n",
    "                best_RMSE = rmse_te\n",
    "                optimal_weights_rmse_te = w_tr\n",
    "        # plot figures\n",
    "        plt.figure(i)\n",
    "        plot_train_test(rmse_list_tr[i, :], rmse_list_te[i, :], lambdas, degree)\n",
    "        # TODO we need to compute the error for each (d, lambda) value, store all in array and print best for each degree\n",
    "        #err = error(y_te,predict_labels(w_tr,poly_basis_te))\n",
    "        plt.title((\"RR degree\", degree, \".Best Error: \", best_err))\n",
    "    \n",
    "    # get best degree, lambda according to the testing RMSE\n",
    "    degree_index_te, lambd_index_te = np.where(rmse_list_te == rmse_list_te.min())\n",
    "    degree_index_te, lambd_index_te = (degree_index_te[0],lambd_index_te[0])\n",
    "    best_rmse_te = rmse_list_te[degree_index_te][lambd_index_te]\n",
    "    print(\"Best testing RMSE is\", best_rmse_te, \"with degree\", degrees[degree_index_te], \"and lambda=\", lambdas[lambd_index_te], \"\\n\")\n",
    "    # get best degree and lambda according to the error\n",
    "    degree_index_err, lambd_index_err = np.where(errors == errors.max())\n",
    "    degree_index_err, lambd_index_err = (degree_index_err[0],lambd_index_err[0])\n",
    "    best_error = errors[degree_index_err][lambd_index_err]\n",
    "    print(\"Best fitting is\", best_error, \"% with degree\", degrees[degree_index_err], \"and lambda=\", lambdas[lambd_index_err], \"\\n\")\n",
    "    #return optimal_weights, best_RMSE\n",
    "    return optimal_weights_rmse_te, best_rmse_te\n",
    "    \n",
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a few tests with ridge regression, it seems like we should not go above degree 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cross-Validation to have better error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demo of the cross-validation function. We basically run cross-validation for different lamdda/degree combinations to determine which one gives the smallest error, exactly as we did above with ridge_regression_demo.\n",
    "The difference is that above it was a simple 2-fold split of the data, but now we do a k-fold (here k=4) which gives us a less biased error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_demo(x, y):\n",
    "    seed = 1\n",
    "    degrees = [5]\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-12, -9, 6)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    for i, degree in enumerate(degrees):\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        for lambda_ in lambdas:\n",
    "            loss_tr_sum,loss_te_sum = 0, 0\n",
    "            k = 0\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te,_ = cross_validation(y, x, k_indices, k, lambda_, degree, ridge_regression)\n",
    "                loss_tr_sum += loss_tr\n",
    "                loss_te_sum += loss_te\n",
    "                k+=1\n",
    "            print(\"RMSE for lambda =\",lambda_,\":\",loss_te_sum)\n",
    "            rmse_tr.append(loss_tr_sum)\n",
    "            rmse_te.append(loss_te_sum)\n",
    "        plt.figure(i)\n",
    "        cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "        plt.title((\"cross validation degree\",degree,\"best RMSE_te:\",min(rmse_te)))\n",
    "\n",
    "#cross_validation_demo(tX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation_demo(tX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ridge regression: best degree seems to be 5\n",
    "lambda_ between 1e-12 ans 1e0\n",
    "best lambda is 1.58489319246e-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the weights with different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is loaded\n",
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "y, tX, ids = load_data()\n",
    "tX = data_cleaning(tX)\n",
    "tX_tr, y_tr, tX_te, y_te = split_data(tX, y, 3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w, loss = least_squares(y_tr, tX_tr)\n",
    "print(\"Data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_basis_tr = build_poly(tX_tr, 2)\n",
    "poly_basis_te = build_poly(tX_te, 2)\n",
    "w, loss = least_squares(y_tr, poly_basis_tr)\n",
    "# build a polynomial basis of the same size as training set for the testing set\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "w, loss = least_squares_GD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "w, loss = least_squares_SGD(y_tr, tX_tr, initial_w, max_iters, gamma)\n",
    "print(\"data fitting\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fitting: 81.2336 %\n",
      "AMS: 753.442565185\n"
     ]
    }
   ],
   "source": [
    "# to find optimal degree and lambda, check \"Use RR to determine optimal\n",
    "# lambda and degree\" section\n",
    "degree = 5\n",
    "poly_basis_tr = build_poly(tX_tr, degree)\n",
    "lambda_ = 1e-11\n",
    "poly_basis_te = build_poly(tX_te, degree)\n",
    "w, loss = ridge_regression(y_tr, poly_basis_tr, lambda_)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,poly_basis_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, poly_basis_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-5\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "y_red = y01\n",
    "x_red = tX_tr\n",
    "\n",
    "w_initial = np.zeros(tX_tr.shape[1])\n",
    "w,loss = logistic_regression(y_red, x_red, w_initial, max_iter, gamma)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-7\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "\n",
    "max_AMS = 0\n",
    "y_red = y01[0:5000]\n",
    "x_red = tX_tr[0:5000]\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "best_degree = degrees[0]\n",
    "for degree in degrees:\n",
    "    poly_basis_tr = build_poly(x_red, degree)\n",
    "    w,loss = logistic_regression(y_red, poly_basis_tr, initial_w, max_iter, gamma)\n",
    "    poly_basis_te = build_poly(tX_te, degree)\n",
    "    ams = compute_AMS(w, y_te, poly_basis_te)\n",
    "    print(\"degree\",degree,\"-> AMS:\", ams)\n",
    "    print(\"degree\",degree,\"-> fitting:\",error(y_te,predict_labels(w,poly_basis_te)))\n",
    "    if(max_AMS < ams):\n",
    "        max_AMS = ams\n",
    "        best_degree = degree\n",
    "print(\"best AMS:\",max_AMS,\"obtained with degree\",best_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "gamma = 1e-4\n",
    "y01 = minus_one_to_zero(y_tr)\n",
    "\n",
    "y_red = y01[0:10000]\n",
    "x_red = tX_tr[0:10000]\n",
    "\n",
    "w,loss = newton_logistic_regression(y_red, x_red, max_iter, gamma)\n",
    "print(\"data fitting:\",error(y_te,predict_labels(w,tX_te)),\"%\")\n",
    "print(\"AMS:\",compute_AMS(w, y_te, tX_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "shape of tX before standardizing: (568238, 30)\n",
      "shape of tX before standardizing: (568238, 31)\n",
      "data cleaning completed\n",
      "Previous number of features in tX: 31\n",
      "New number of features in tX: 25\n",
      "PCA completed\n"
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"done\")\n",
    "tX_test = data_cleaning(tX_test)\n",
    "tX_test = PCA(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tX before standardizing: (250000, 30)\n",
      "shape of tX before standardizing: (250000, 31)\n",
      "data cleaning completed\n",
      "Previous number of features in tX: 31\n",
      "New number of features in tX: 25\n",
      "PCA completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tX = data_cleaning(tX)\n",
    "tX = PCA(tX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression with poly basis of degree 5\n",
    "# This is the optimal solution with cleaning and no PCA\n",
    "lambda_ = 1e-11\n",
    "degree = 5\n",
    "#tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test least squares\n",
    "degree = 2\n",
    "tX_test,_,_=standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = least_squares(y, poly_basis_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test ridge regression with poly_basis of degree 2\n",
    "lambda_ = 0.00138949549437\n",
    "degree = 2\n",
    "tX_test,_,_ = standardize(tX_test)\n",
    "poly_basis_te = build_poly(tX_test, degree)\n",
    "poly_basis_tr = build_poly(tX, degree)\n",
    "w, loss = ridge_regression(y, poly_basis_tr, lambda_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test ridge regression no data cleaning\n",
    "lambda_ = 0.000268269579528 \n",
    "w, loss = ridge_regression(y, tX, lambda_)\n",
    "#print(error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(w.shape)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/submissionData/RR_deg5_withPCA.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "#o = np.ones((tX_test.shape[0],1))\n",
    "\n",
    "y_pred = predict_labels(w, poly_basis_te)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n",
      "(568238, 25)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
