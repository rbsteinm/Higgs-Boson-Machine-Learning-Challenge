{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(\"../data/train.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All value in y is equal either to 1 or -1.\n"
     ]
    }
   ],
   "source": [
    "# As we can see here, y only takes value -1 or 1:\n",
    "for value in y:\n",
    "    assert(value==1 or value==-1)\n",
    "print(\"All value in y is equal either to 1 or -1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that y is a binary variable. So should we modify y's coding to {0, 1} instead of {-1, 1} if we want the logistic regression methods to work?\n",
    "Note that at first sight, logistic regression seems to be the best solution to fit the data since this method was designed for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We have to handle:\n",
    "- outliers:\n",
    "A value is considered as an outlier if it does not fit in a range defined from quartiles. Outliers are replaced by the mean value of the observations.\n",
    "- unasssigned values (-999, 999):\n",
    "We proceed the same way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "done\n",
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)\n",
    "for col in tX.T:\n",
    "    q1 = np.percentile(col,25)\n",
    "    q3 = np.percentile(col,75)\n",
    "    interq = q3-q1\n",
    "    \n",
    "    col_cleaned = col[abs(col)!=999]\n",
    "    col_cleaned = col_cleaned[col_cleaned<=q3+interq]\n",
    "    col_cleaned = col_cleaned[col_cleaned>=q1-interq]\n",
    "    #print(col_cleaned.shape)\n",
    "    mean = np.mean(col_cleaned)\n",
    "    \n",
    "    col[abs(col)==999] = mean\n",
    "    col[col>q3+interq] = mean\n",
    "    col[col<q1-interq] = mean\n",
    "    \n",
    "tX,_,_ = standardize(tX)\n",
    "print(\"done\")\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "[  1.12800597e+00   2.01810042e+00  -1.84383694e-12 ...,  -1.79898422e-01\n",
      "  -5.96123654e-01  -1.84383694e-12]\n",
      "check\n"
     ]
    }
   ],
   "source": [
    "print(tX[:, 1:tX.shape[1]].shape)\n",
    "tx2 = tX[:, 1:tX.shape[1]]\n",
    "print((tx2[:, 0]))\n",
    "#print((tX[:, 0]))\n",
    "if(np.all(tX[:, 0] == 1.0)):\n",
    "    print(\"check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to get rid of features that don't give enough information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#C = 1/tX.shape[0]*tX.T.dot(tX)\n",
    "#  \n",
    "#eigenvalue, eigenvector = np.linalg.eig(C)\n",
    "#SUM = np.sum(eigenvalue)\n",
    "#\n",
    "#idx = np.argsort(eigenvalue)[::-1]\n",
    "#eigenvector = eigenvector[:,idx]\n",
    "#eigenvalue = eigenvalue[idx]\n",
    "#\n",
    "#F = 0\n",
    "#k = 0\n",
    "#while F < 0.90:\n",
    "#    F += eigenvalue[k]/SUM\n",
    "#    k = k+1   \n",
    "#\n",
    "#eigenvector=eigenvector[:, :k]\n",
    "#\n",
    "#tX = np.dot(eigenvector.T, tX.T).T\n",
    "#\n",
    "#print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some helper function:\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (-1/N)*(tx.T).dot(e)\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Compute the cost by MSE\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return (1/(2*N))*((e.T).dot(e))\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # concatenate column of ones of size (25000, 1)\n",
    "    # if necessary get rid of the column of ones that's already there from standardization\n",
    "    if(np.all(x[:, 0] == 1.0)):\n",
    "        x = x[:, 1:x.shape[1]]\n",
    "    result = np.ones((x.shape[0], 1))\n",
    "    for i in range(1, degree+1):\n",
    "       result = np.concatenate((result, x ** i), axis=1)\n",
    "    return result\n",
    "\n",
    "def split_data(x, y, ratio, seed=0):\n",
    "    \"\"\"Randomly splits the data given in input into two subsets (test/train).\n",
    "    The ratio determines the size of the training set.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    size = y.shape[0]\n",
    "    # randomly permutes array of intergers from 0 to size-1\n",
    "    indexes = np.random.permutation(size)\n",
    "    tr_size = int(np.floor(ratio * size))\n",
    "    # get (randomly generated) indexes of training/testing set\n",
    "    tr_indexes = indexes[0:tr_size]\n",
    "    te_indexes = indexes[tr_size:]\n",
    "    # split x (resp. y) into two subarrays x_tr, x_te (resp. y_tr, y_te)\n",
    "    x_tr = x[tr_indexes]\n",
    "    y_tr = y[tr_indexes]\n",
    "    x_te = x[te_indexes]\n",
    "    y_te = y[te_indexes]\n",
    "    return [x_tr, y_tr, x_te, y_te]\n",
    "\n",
    "def sigmoid_scal(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    if(t > 0):\n",
    "        return np.exp(t)/(1+np.exp(t))\n",
    "    else:\n",
    "        return 1/(1+np.exp(-t))\n",
    "    \n",
    "sigmoid = np.vectorize(sigmoid_scal)\n",
    "    \n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = 0\n",
    "    for index, row in enumerate(tx):\n",
    "        loss += np.log(1+np.exp(row.dot(w)))-y[index]*(row).dot(w)\n",
    "    return loss\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"   \n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    w -= gamma*grad\n",
    "    return w, loss\n",
    "\n",
    "#def new_step_size(f, x, gamma, beta=1/2):\n",
    "#    alpha = gamma\n",
    "#    while f(x)-f(x+alpha*)\n",
    "    \n",
    "\n",
    "def error(y,y_est):\n",
    "    \"\"\"Computes the percentage of right values in the \n",
    "    regression result compared to our test data\"\"\"\n",
    "    diff = np.count_nonzero(y-y_est)/len(y)\n",
    "    return (1-diff)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX, y, tX_te, y_te = split_data(tX, y, 2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111110, 31)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        #update w\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"performs linear regression using stochastic gradient descent algorithm. \n",
    "    Returns two arrays containing weights and loss values \n",
    "    at each step of the algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    batch_size = 800 #try changing batch size\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        print(\"iteration\", n_iter)\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            #compute new loss and w\n",
    "            loss = compute_loss(y, tx, w) # add one loss per minibatch (compute mean)\n",
    "            w = w - gamma * gradient\n",
    "            # store loss and w in arrays\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "                 \n",
    "    return w, loss\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"performs linear regression by calculating \n",
    "    the least squares solution using normal equations.\n",
    "    returns loss and optimal wieghts.\"\"\"\n",
    "    opt_w = np.linalg.inv(tx.T.dot(tx)).dot(tx.T).dot(y)\n",
    "    #computes the loss using MSE\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "    \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression. This calculates the MSE while taking in \n",
    "    accout a regularizer that is determined by lambda. This has for effect to\n",
    "    penalize/avoid large weights in order to avoid overfitting.\"\"\"\n",
    "    # tx is the polynomial basis\n",
    "    opt_w = (np.linalg.inv(tx.T.dot(tx)+lambda_*2*len(y)*np.identity(tx.shape[1])).dot(tx.T)).dot(y)\n",
    "    mse = compute_loss(y, tx, opt_w)\n",
    "    return opt_w, mse\n",
    "\n",
    "def logistic_regression(y, tx, max_iter, gamma):\n",
    "    threshold = 1e-10\n",
    "    losses = np.array([0,1])\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        w, loss = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        losses[1] = losses[0]\n",
    "        losses[0] = loss\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        if abs(losses[0]-losses[1]) < losses[1]*threshold:\n",
    "            break\n",
    "            \n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, loss\n",
    "\n",
    "    \n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    #TODO\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=6931.471805600547\n",
      "Current iteration=10, the loss=4754.77033580776\n",
      "Current iteration=20, the loss=4696.4068519540715\n",
      "The loss=4685.625252814106\n",
      "77.0717834257326\n"
     ]
    }
   ],
   "source": [
    "### logistic regression test\n",
    "max_iter = 1000\n",
    "gamma = 10e-5\n",
    "y01 = (y+1)/2\n",
    "\n",
    "\n",
    "y_red = y01[0:10000]\n",
    "x_red = tX[0:10000]\n",
    "\n",
    "w,loss = logistic_regression(y_red, x_red, max_iter, gamma)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Polynomial Regression to find the optimal degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbsteinm\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for degree 1 : [        nan  0.64545768         nan  0.42273952         nan  0.3795007\n",
      "  0.00820395         nan         nan         nan         nan         nan\n",
      "  0.45251252  0.36704847  0.38730081         nan         nan         nan\n",
      "  0.04614404  0.0999644          nan         nan  0.15739899         nan\n",
      "  0.19103558  0.09951122  0.05056202         nan  0.08269145  0.03149865\n",
      "  0.37269137]\n",
      "RMSE for degree 2 : [        nan  0.63496136         nan  0.43509821         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "  0.35195894  0.37266127  0.39014326  0.02967158         nan  0.10003188\n",
      "  0.06079484  0.0855573          nan         nan  0.25774899         nan\n",
      "  0.28320305  0.09246861  0.06463537  0.27060311  0.07904409         nan\n",
      "  0.6339042          nan  0.16101386         nan  0.1600057   0.11445989\n",
      "  0.18338812         nan         nan  0.07039186  0.21235473  0.24773943\n",
      "  0.48459803  0.17894471         nan         nan         nan         nan\n",
      "         nan         nan  0.19078175         nan         nan  0.183773\n",
      "         nan  0.38555191  0.06309331         nan  0.29232535  0.04854509\n",
      "         nan]\n",
      "RMSE for degree 3 : [        nan  0.9508984          nan  0.45239686         nan  0.30954418\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "  0.36425256  0.43457603  0.39128008         nan         nan         nan\n",
      "         nan  0.0669203          nan  0.04976229  0.15569858         nan\n",
      "  0.39917081  0.04954507  0.07944606  0.36428304  0.04097547         nan\n",
      "  0.61136018         nan         nan         nan  0.22068101  0.06665181\n",
      "  0.33011308  0.1537625          nan         nan  0.25406292  0.23916321\n",
      "  0.46190581  0.16838721         nan         nan         nan         nan\n",
      "         nan  0.03182597  0.17740077  0.04128676         nan         nan\n",
      "         nan  0.37213068  0.06961675         nan  0.29304037  0.05761177\n",
      "         nan         nan  0.35811562         nan         nan         nan\n",
      "         nan         nan         nan  0.12340761         nan         nan\n",
      "         nan         nan         nan  0.0561535   0.08590327  0.12315131\n",
      "  0.04670893  0.03602962         nan         nan  0.11266091  0.22858914\n",
      "  0.02578987  0.04129228         nan  0.01900996  0.01634602  0.03533943\n",
      "  0.14985985]\n",
      "RMSE for degree 4 : [        nan  0.57194946         nan         nan  0.5579751   0.43432097\n",
      "         nan         nan  0.56894164         nan         nan         nan\n",
      "         nan  0.4787923   0.55948466         nan         nan  0.37083628\n",
      "  0.02504509  0.11107331         nan  0.0338485   0.34875478  0.8391371\n",
      "  0.40368234  0.02773709  0.06410545  0.39037212  0.04504009         nan\n",
      "  0.57670207  0.11546864         nan  0.22533312         nan         nan\n",
      "  0.38645429         nan  0.18762932  0.28810419  0.58145857  0.3443805\n",
      "  0.67838376  0.09606717         nan         nan         nan         nan\n",
      "         nan  0.25422328         nan         nan         nan         nan\n",
      "         nan  0.4338015   0.07376172         nan  0.33688234         nan\n",
      "         nan         nan  0.36815001         nan         nan         nan\n",
      "         nan  0.03737756         nan  0.25597044         nan  0.09831654\n",
      "  0.26909404         nan         nan  0.05606334  0.08180903  0.21311131\n",
      "  0.04840227  0.04061465         nan         nan  0.10524791         nan\n",
      "  0.07061178  0.0456629          nan  0.03912007  0.00935069  0.03527475\n",
      "  0.34003642         nan         nan         nan  0.12194513  0.05725929\n",
      "  0.05405815  0.00861176         nan         nan  0.05166607         nan\n",
      "         nan         nan  0.04156015         nan  0.08854503         nan\n",
      "  0.04922407  0.12143288  0.11262377         nan  0.03105748  0.54222717\n",
      "         nan         nan  0.02477035         nan         nan  0.05327992\n",
      "         nan]\n",
      "RMSE for degree 5 : [        nan  1.02576156         nan  0.53778307  0.69216178  0.47391092\n",
      "         nan         nan  0.14889855         nan         nan  0.08220589\n",
      "         nan  0.44700139  0.55823172         nan         nan         nan\n",
      "  0.4489205          nan         nan  0.09914067         nan         nan\n",
      "  0.46350987  0.06983672  0.08376466  0.34969717         nan         nan\n",
      "  0.29843747         nan         nan         nan         nan  0.19236032\n",
      "  0.33003314         nan  0.22230383  0.20548471  0.4372319   0.2697951\n",
      "  0.37938189  0.22370995  0.04728092         nan         nan  0.22956189\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan  0.43529566         nan         nan  0.31803638         nan\n",
      "         nan         nan  0.46974359  0.04070622         nan         nan\n",
      "         nan  0.02800141         nan  0.26453193         nan         nan\n",
      "  0.81849644         nan         nan  0.08740691  0.18513725  0.1960068\n",
      "  0.12745116  0.03416792         nan         nan         nan  1.1672793\n",
      "  0.12393718         nan         nan  0.05524768  0.04065219         nan\n",
      "  0.40356704         nan  0.21020102         nan  0.08074011         nan\n",
      "  0.07290178         nan         nan         nan         nan         nan\n",
      "  0.15674763         nan  0.05371661         nan  0.09921752         nan\n",
      "  0.04945276  0.12521646  0.15639102         nan         nan  0.10091225\n",
      "         nan         nan  0.02946473         nan         nan  0.05155084\n",
      "         nan  0.13429181         nan         nan  0.04081343  0.03939633\n",
      "         nan  0.0092231   0.09725377  0.10138789  0.11880476  0.07400975\n",
      "         nan  0.05770481         nan         nan         nan  0.08587864\n",
      "         nan  0.00823169         nan  0.06925023  0.05028825         nan\n",
      "  0.0051786   0.01670229  0.01574775  0.00121627         nan  0.01155426\n",
      "  0.03030915]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-329-88c05626dac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The best degree among those we tested is\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_degree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mpolynomial_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-329-88c05626dac0>\u001b[0m in \u001b[0;36mpolynomial_regression\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# plot the RMSE in function of the degree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'degree'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RMSE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3152\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3153\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3154\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3155\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3156\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1809\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1810\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1811\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1812\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1422\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'color'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1426\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbsteinm\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must have same first dimension\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqbH/BTlbs8dE2Xm9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2694eac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"For each degree, constructs the polynomial basis function expansion of the data\n",
    "       and stores the corresponding RMSE in an array. At the end we chose the degree that\n",
    "       generated the smallest RMSE. Of course we cannot test all degrees so this is not\n",
    "       optimal but it helps us having a good idea of the optimal degree value.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    # for each degree we store the corresponding RMSE in this array\n",
    "    rmse_array = np.array([])\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # form the data to do polynomial regression:\n",
    "        polynomial_basis = build_poly(tX, degree)\n",
    "        \n",
    "        # least square and calculate rmse:\n",
    "        mse, weight = least_squares(y, polynomial_basis)\n",
    "        rmse = np.sqrt(2*mse)\n",
    "        rmse_array = np.append(rmse_array, rmse)\n",
    "        print(\"RMSE for degree\", degree, \":\", rmse)\n",
    "    \n",
    "    # plot the RMSE in function of the degree\n",
    "    plt.plot(degrees, rmse_array)\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "    #compute the best degree\n",
    "    best_degree = degrees[np.argmin(rmse_array)]\n",
    "    print(\"The best degree among those we tested is\", best_degree, \".\")\n",
    "\n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Looking at the results, it seems like 2 is the optimal degree. However, we might be overfitting the data because there is no regularization step in polynomial_regression. Thus we'll use the Ridge regression, which uses a regularizer that depends on a parameter lambda.\n",
    "We'll compute the RMSE for different lambda and degree values in order to determine the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ridge Regression to determine optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training RMSE is 0.734614685283 with degree 7 and lambda= 0.000268269579528 \n",
      "\n",
      "Best error is 81.9638196382 with degree 7 and lambda= 1e-05 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTJn8P1sugWsWzbVLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x373a49b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ridge_regression_demo(x, y, ratio, seed):\n",
    "    \"\"\"Calculate polyomial basis tX from x with given degree,\n",
    "    splits the data according to given ratio and then run\n",
    "    ridge regression on tX, y with different lambda values.\n",
    "    At the end we plot the RMSEs of training/testing set in\n",
    "    function of lambda in order to determine the best lambda value\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    degrees = [7]\n",
    "    \n",
    "    # split the data, and return train and test data:\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    #calculate test/train RMSE for each lambda and store them in lists\n",
    "    rmse_list_tr = np.empty([len(degrees), len(lambdas)])\n",
    "    rmse_list_te = np.empty([len(degrees), len(lambdas)])\n",
    "    errors = np.empty([len(degrees), len(lambdas)])\n",
    "    optimal_weights_err = np.empty([np.shape(tX)[1]])\n",
    "    optimal_weights_rmse_te = np.empty([np.shape(tX)[1]])\n",
    "    best_err = 10e10\n",
    "    best_RMSE = 10e10\n",
    "    for i, degree in enumerate(degrees):\n",
    "        # for each lambda, store the best RMSE and the degree that generated it\n",
    "        for j, lambd in enumerate(lambdas):\n",
    "            # compute polynomial basis from given degree\n",
    "            poly_basis_tr = build_poly(x_tr, degree)\n",
    "            poly_basis_te = build_poly(x_te, degree)\n",
    "            # compute training and testing (R)MSE for current lambda/degree\n",
    "            w_tr, mse_tr = ridge_regression(y_tr, poly_basis_tr,lambd)\n",
    "            mse_te = compute_loss(y_te, poly_basis_te, w_tr)\n",
    "            rmse_tr = np.sqrt(2*mse_tr)\n",
    "            rmse_te = np.sqrt(2*mse_te)\n",
    "            err = error(y_te,predict_labels(w_tr,poly_basis_te))\n",
    "            #print(\"Training RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_tr, \"\\n\")\n",
    "            #print(\"Testing RMSE for lambda =\", lambd, \"and degree\", degree, \":\", rmse_te, \"\\n\")\n",
    "            #print(\"error for lambda = \", lambd, \"and degree\", degree, \":\", err)\n",
    "            # Store RMSEs in arrays\n",
    "            rmse_list_tr[i][j] = rmse_tr\n",
    "            rmse_list_te[i][j] = rmse_te\n",
    "            errors[i][j] = err\n",
    "            # we do this to get optimal weights according to the error\n",
    "            if(best_err < err):\n",
    "                best_err = err\n",
    "                optimal_weights_err = w_tr\n",
    "            # get the optimal weights according to the testing rmse\n",
    "            if(best_RMSE > rmse_te):\n",
    "                best_RMSE = rmse_te\n",
    "                optimal_weights_rmse_te = w_tr\n",
    "        # plot figures\n",
    "        plt.figure(i)\n",
    "        plot_train_test(rmse_list_tr[i, :], rmse_list_te[i, :], lambdas, degree)\n",
    "        # TODO we need to compute the error for each (d, lambda) value, store all in array and print best for each degree\n",
    "        err = error(y_te,predict_labels(w_tr,poly_basis_te))\n",
    "        plt.title((\"RR degree\", i+1, \".Best Error: \", err))\n",
    "    \n",
    "    # get best degree, lambda, RMSE\n",
    "    degree_index_te, lambd_index_te = np.where(rmse_list_te == rmse_list_te.min())\n",
    "    degree_index_te, lambd_index_te = (degree_index_te[0],lambd_index_te[0])\n",
    "    best_rmse_te = rmse_list_te[degree_index_te][lambd_index_te]\n",
    "    print(\"Best training RMSE is\", best_rmse_te, \"with degree\", degrees[degree_index_te], \"and lambda=\", lambdas[lambd_index_te], \"\\n\")\n",
    "    # get best degree and lambda according to the error\n",
    "    degree_index_err, lambd_index_err = np.where(errors == errors.max())\n",
    "    degree_index_err, lambd_index_err = (degree_index_err[0],lambd_index_err[0])\n",
    "    best_err = errors[degree_index_err][lambd_index_err]\n",
    "    print(\"Best error is\", best_err, \"with degree\", degrees[degree_index_err], \"and lambda=\", lambdas[lambd_index_err], \"\\n\")\n",
    "    #return optimal_weights, best_RMSE\n",
    "    return optimal_weights_rmse_te, best_rmse_te\n",
    "    \n",
    "seed = 2\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the weights with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial values\n",
    "gamma = 1e-7\n",
    "initial_w = 0*np.ones(len(tX[0])) #try changing initial w\n",
    "max_iters = 10\n",
    "poly_basis = build_poly(tX, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.11498308013536\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y, tX)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.28036575707395\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y, poly_basis)\n",
    "# build a polynomial basis of the same size as training set for the testing set\n",
    "tX_te_poly = build_poly(tX_te, 2)\n",
    "print(error(y_te,predict_labels(w,tX_te_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.42052463580291\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "34.440924472604216\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (83334,30) and (61,) not aligned: 30 (dim 1) != 61 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-ddda3e3320ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-05\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoly_basis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/maxime/Bureau/EPFL/PCML/pcml-project1/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[1;34m(weights, data)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (83334,30) and (61,) not aligned: 30 (dim 1) != 61 (dim 0)"
     ]
    }
   ],
   "source": [
    "# to find optimal degree and lambda, check \"Use RR to determine optimal\n",
    "# lambda and degree\" section\n",
    "degree = 2\n",
    "poly_basis = build_poly(tX, 2)\n",
    "lambda_ = 1e-05\n",
    "w, loss = ridge_regression(y, poly_basis, lambda_)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression with data splitting and chosing best lambda/deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training RMSE is 0.770148656077 with degree 2 and lambda= 1e-05 \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (83334,30) and (61,) not aligned: 30 (dim 1) != 61 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-c8c801a3512d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msplit_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_regression_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/maxime/Bureau/EPFL/PCML/pcml-project1/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[1;34m(weights, data)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (83334,30) and (61,) not aligned: 30 (dim 1) != 61 (dim 0)"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "split_ratio = 0.8\n",
    "w, loss = ridge_regression_demo(tX, y, split_ratio, seed)\n",
    "print(error(y_te,predict_labels(w,tX_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 32)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "base_poly_x = build_poly(tX_test, 2)\n",
    "poly_basis = build_poly(tX, 2)\n",
    "w, loss = least_squares(y, poly_basis)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/submissionData/logisticRegression1.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "o = np.ones((tX_test.shape[0],1))\n",
    "tX_test,_,_ = standardize(tX_test)\n",
    "\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
